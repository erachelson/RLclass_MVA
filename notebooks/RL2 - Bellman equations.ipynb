{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b6229bf",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://erachelson/RLclass_MVA/\">https://erachelson.github.io/RLclass_MVA/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72b7186",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Chapter 2: Characterizing value functions: the Bellman equations</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d146450",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Learning outcomes**  \n",
    "By the end of this chapter, you should be able to:\n",
    "- explain what a state-action value function is,\n",
    "- link optimal policies and value functions,\n",
    "- write the Bellman equations (evaluation and optimality) and define a value optimization problem,\n",
    "- write and execute a value iteration algorithm to find an optimal value function for a finite state space MDP.\n",
    "\n",
    "Additionally, after doing the homework, you should be able to:\n",
    "- solve the evaluation equation by both matrix inversion and dynamic programming,\n",
    "- write value iteration as the repeated application of a greediness operator and the evaluation operator,\n",
    "- explain, write and execute a modified policy iteration algorithm to find an optimal value function for a finite state space MDP,\n",
    "- explain and discuss the principle of asynchronous dynamic programming for MDPs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f07ce15-f0fd-4c2f-a233-c0418c648f3f",
   "metadata": {},
   "source": [
    "# Foreword <a class=\"tocSkip\">\n",
    "\n",
    "In this chapter, we will use reward models interchangeably under the form $r(s,a)$ or $r(s,a,s')$. Similarly, we will manipulate memoryless, stationary policies, either deterministic or stochastic.\n",
    "\n",
    "Following the notation $X^Y$ for the set of functions from set $Y$ to set $X$, we shall write $\\mathbb{R}^S$ the set of functions from $S$ to $\\mathbb{R}$. For ease of notation, we shall write $\\mathbb{R}^{SA} = \\mathbb{R}^{S\\times A}$.\n",
    "\n",
    "Note that when $S$ is discrete, $\\mathbb{R}^S$ is isomorphic to $\\mathbb{R}^{|S|}$, that is we can represent functions as vectors of values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74408b35",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Intuitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d279b9d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Consider the maze below, where an agent can move North, South, East or West. The resulting transition is deterministic and a reward of $+1$ is gained when exiting the maze (which terminates the game). Otherwise all rewards are zero. Bumping into a wall leaves the agent in place, with a reward of zero.\n",
    "\n",
    "<center><img src=\"img/grid_raw.png\" width=\"200px\"></img></center>\n",
    "\n",
    "Let's consider the policy $\\pi$ that always moves East.\n",
    "\n",
    "<center><img src=\"img/grid_policy.png\" width=\"200px\"></img></center>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "Without writing any equation, what is the value of the top-right cell under this policy?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8798d071",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "$V^\\pi((3,3)) = 1$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39556880",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let's take $\\gamma=0.9$.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "Without writing any equation, what is the value of the top-middle cell under this policy? What is the value of the bottom-right cell?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab6ac6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "The value of $(2,3)$ is the expected discounted sum of what one gets from applying $\\pi$ from $(2,3)$. Since the $\\pi$ is deterministic and the transitions are deterministic too, $\\pi(2,3)$ always take us to state $(3,3)$. So $V^\\pi((2,3)) = 0 + \\gamma \\times V^\\pi((3,3)) = 0.9$.\n",
    "    \n",
    "The value of $(3,1)$ is the expected infinite sum of discounted rewards from $(3,1)$. Since the agent keeps bumping into the wall when applying $\\pi$, it never exits the maze and this is an infinite sum of zero terms. Hence $V^\\pi((2,3)) = 0$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b184b4c9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's draw the value function.\n",
    "\n",
    "<center><img src=\"img/grid_vpi.png\" width=\"200px\"></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c530970",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose you are currently in cell $(1,2)$ (left column, middle cell) and would like to choose what action to take. Suppose also that you know the value function above. You need to put a scalar value on all four actions to decide which one you prefer. To evaluate each action, let's estimate what we can get by 1) getting an instantaneous reward after applying the action and then 2) using $\\gamma \\times V^\\pi(s)$ to estimate what can obtain in the long run after this first action. Define $Q^\\pi((x,y),a)$ as the utility we estimate for each action $a$ in $(x,y)$.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "What is $Q^\\pi((1,2),a)$ for action $a$ in $\\{N,S,E,W\\}$? What seems to be the most interesting first action to take, if we follow $\\pi$ after?  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff66093a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "$Q^\\pi((1,2),N) = 0 + \\gamma \\cdot \\gamma^2 = 0.729$  \n",
    "$Q^\\pi((1,2),S) = 0 + \\gamma \\cdot 0 = 0$  \n",
    "$Q^\\pi((1,2),E) = 0 + \\gamma \\cdot 0 = 0$  \n",
    "$Q^\\pi((1,2),W) = 0$  \n",
    "The best action seems to be $N$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94867bb3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An optimal policy is quite easy to guess. Let's draw the optimal value function (the value function of any optimal policy).\n",
    "\n",
    "<center><img src=\"img/grid_vopt.png\" width=\"200px\"></img></center>\n",
    "\n",
    "Define $Q^*((x,y),a)$ as the utility we estimate for each action $a$ in $(x,y)$ if it is followed by an optimal policy.\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**   \n",
    "What is $Q^*((1,2),a)$ for action $a$ in $\\{N,S,E,W\\}$? What seems to be the most interesting first action to take, if we act optimally after? Rank the actions by utility.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80935e7f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "$Q^*$ is what we gain immediately, plus $\\gamma$ times what we expect to receive from applying an optimal policy in the state we reach by applying $a$.  \n",
    "$Q^*((1,2),N) = 0 + \\gamma\\times\\gamma^2=\\gamma^3$  \n",
    "$Q^*((1,2),S) = 0 + \\gamma\\times\\gamma^4=\\gamma^5$  \n",
    "$Q^*((1,2),E) = 0 + \\gamma\\times\\gamma^4=\\gamma^5$  \n",
    "$Q^*((1,2),W) = 0 + \\gamma\\times0=0$  \n",
    "The best action seems to be $N$, followed by $W$, after that $S$ and $E$ are tied.  \n",
    "We can note the value of $Q^*$ for the action $N$ in $(1,2)$ is equal to the optimal value function $V^*$ in $(1,2)$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce277c6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "What property has the policy that always picks greedily the $Q^*$ maximizing action in each state?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb4428d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "It is an optimal policy.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f6732",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now suppose $(1,2)$ is a special slippery cell. Going North has a $0.7$ probability of actually reaching $(1,3)$, but also a $0.2$ probability of staying in $(1,2)$ and a $0.1$ probability of ending in $(2,2)$. Note that this changes the problem and the optimal expected return function $V^*$.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "Given this new problem, can you write $Q^*((1,2),N)$ as a function of $V^*(1,3)$, $V^*(1,2)$ and $V^*(2,2)$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c07a14",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "When we take action $N$ in $(1,2)$, there are 3 possible outcomes:\n",
    "- with probability $0.7$, reach $(1,3)$ and get reward $0$,\n",
    "- with probability $0.2$, reach $(1,2)$ and get reward $0$,\n",
    "- with probability $0.1$, reach $(2,2)$ and get reward $0$.\n",
    "\n",
    "So what we can expect to get from applying $N$ in $(1,2)$ is:  \n",
    "\\begin{align*}\n",
    "    Q^*((1,2), N) &= 0.7 \\times (0+\\gamma V^*(1,3)) + 0.2\\times(0+\\gamma V^*(1,2)) + 0.1\\times(0+\\gamma V^*(2,2))\\\\\n",
    "    &= \\gamma \\left(0.7\\times V^*(1,3) + 0.2\\times V^*(1,2)+ 0.1\\times V^*(2,2)\\right)\n",
    "\\end{align*}\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0dac08",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now you can remark that if we knew the action $\\pi^*((1,2))$ taken by an optimal policy in $(1,2)$, then $Q^*((1,2), \\pi^*(1,2))$ would actually be precisely the optimal long-term return $V^*$ (since it would be the expected return of a policy that acts optimally at every time step, including the first one).\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "Suppose an oracle tells us that $\\pi^*((1,2))=N$. Using the previous exercice, write $V^*(1,2)$ as a function of $V^*(1,3)$, $V^*(1,2)$ and $V^*(2,2)$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd97a01",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "We have $V^*((1,2)) = Q^*((1,2),N)$, so\n",
    "$$V^*((1,2)) = \\gamma \\left(0.7\\times V^*(1,3) + 0.2\\times V^*(1,2)+ 0.1\\times V^*(2,2)\\right)$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f77581",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have introduced the key concepts upon which this secton is built: $V$ and $Q$ functions, and the relation between $V(s)$ and $V(s')$ when $s'$ can be reached from $s$ in one action. The next steps are now to write all this formally, prove strong properties and derive algorithms for computing value functions and policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8c9924",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# The evaluation equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598154da-3dfb-48d5-bd2d-7b20bcb08ed6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Drawing inspiration from the exercises above, we can define state-action value functions, also called Q-functions, which are a central object in RL. A Q-function is a function $S\\times A \\rightarrow \\mathbb{R}$.\n",
    "\n",
    "The state-action value function of policy $\\pi$ is noted $Q^\\pi$ and is the expected return of playing action $a$ in $s$, then playing policy $\\pi$ in any subsequent state.\n",
    "\n",
    "<div class=\"alert alert-success\"><b>State-action value function</b><br>\n",
    "$$Q^\\pi(s,a) = \\mathbb{E}\\left( \\sum\\limits_{t=0}^\\infty \\gamma^t r\\left(S_t, A_t, S_{t+1}\\right) \\bigg| S_0 = s, A_0=a, \\pi \\right)$$\n",
    "</div>\n",
    "\n",
    "To be precise and reuse the full notations from the MDP definition:\n",
    "\\begin{align*}\n",
    "Q^\\pi(s,a) &=\\mathbb{E}\\left[ \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s, A_0=a,\\\\ A_t \\sim \\pi(S_t)\\textrm{ for }t>0,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1})\\end{array} \\right],\\\\\n",
    " &= \\mathbb{E}_{s'} \\left[ r(s,a,s') + \\gamma V^\\pi(s') \\right], \\\\\n",
    " &= r(s,a) + \\gamma \\mathbb{E}_{s'} \\left[ V^\\pi(s') \\right]\n",
    "\\end{align*}\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"img/Qfunctions.png\" style=\"height: 200px;\"></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c535020-8ba9-455d-896d-33b386c21739",
   "metadata": {},
   "source": [
    "You might remember how we defined the *return* random variable $G^\\pi(s)$ for a policy $\\pi$, in each state $s$, as the $\\gamma$-discounted sum of rewards:\n",
    "$$G^\\pi(s) = \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s,\\\\ A_t \\sim \\pi(S_t),\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1}).\\end{array}$$\n",
    "\n",
    "This lead to $V^\\pi(s) = \\mathbb{E}[G^\\pi(s)]$.\n",
    "\n",
    "We can similarly define the *state-action return* random variable $G^\\pi(s,a)$ for a policy $\\pi$, in each state-action pair $(s,a)$, as the $\\gamma$-discounted sum of rewards:\n",
    "$$G^\\pi(s,a) = \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s, A_0=a\\\\ A_t \\sim \\pi(S_t) \\textrm{ for }t>0,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1}).\\end{array}$$\n",
    "\n",
    "An then we have $Q^\\pi(s,a) = \\mathbb{E}[G^\\pi(s,a)]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6086765-fdf5-4f6c-9399-4a7d6fcb17cf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that this definition of Q-functions uses the $\\gamma$-discounted criterion, but that it can be straightforwardly extended to any other criterion.  \n",
    "Let $C((R_t)_{t\\in \\mathbb{N}})$ be a criterion defined on the sequence of reward random variables.  \n",
    "Then the return random variable $G^\\pi(s,a)$ is the random variable $C((R_t)_{t\\in \\mathbb{N}})$ given that $S_0 = s$, $A_0=a$, $A_t \\sim \\pi(S_t)$ for $t>0$, $S_{t+1}\\sim p(\\cdot|S_t,A_t)$, and $R_t = r(S_t,A_t,S_{t+1})$.  \n",
    "And the corresponding Q-function for this criterion is simply \n",
    "$$Q^\\pi(s,a) = \\mathbb{E}\\left[ C((R_t)_{t\\in \\mathbb{N}}) \\quad \\Bigg| \\quadÂ \\begin{array}{l}S_0 = s, A_0=a,\\\\ A_t \\sim \\pi(S_t)\\textrm{ for }t>0,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1})\\end{array} \\right|.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8d3e4f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's remark that $V^\\pi(s) = Q^\\pi(s,\\pi(s))$. Let's replace $a$ by $\\pi(s)$ above and we obtain an important equation to characterize $V^\\pi$.\n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"img/V-DP.png\" style=\"height: 200px;\"></img></center>\n",
    "$$V^\\pi(s) = r(s,\\pi(s)) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,\\pi(s))} \\left[ V^\\pi(s') \\right]$$\n",
    "\n",
    "This equation uses $V^\\pi(s')$ in all $s'$ reachable from $s$ to define $V^\\pi(s)$.  \n",
    "Since this equation is true in all $s$, this provides as many equations as we have states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc559b0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-success\"><b>Theorem: evaluation equation</b><br>\n",
    "$V^\\pi$ obeys the linear system of equations:\n",
    "$$\n",
    "V^\\pi\\left(s\\right) = r(s,\\pi(s)) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,\\pi(s))} \\left[ V^\\pi(s') \\right]\\\\\n",
    "$$\n",
    "Similarly:\n",
    "$$\n",
    "Q^\\pi\\left(s,a\\right) = r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ Q^\\pi(s',\\pi(s')) \\right]\n",
    "$$\n",
    "</div>\n",
    "\n",
    "This leads to the introduction of the **Bellman evaluation operator**:\n",
    "<div class=\"alert alert-success\"><b>Evaluation operator $T^\\pi$</b><br>\n",
    "$T^\\pi$ is an operator on value functions, that transforms a function $V:S\\rightarrow \\mathbb{R}$ into:\n",
    "\\begin{align*}\n",
    "T^\\pi V\\left(s\\right) &= r(s,\\pi(s)) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,\\pi(s))} \\left[ V(s') \\right]\\\\\n",
    " &= r\\left(s,\\pi\\left(s\\right)\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,\\pi\\left(s\\right)\\right) V\\left(s'\\right)\n",
    "\\end{align*}\n",
    "    \n",
    "Similarly we can introduce an evaluation operator (with the same name $T^\\pi$) over state-action value functions. <br> \n",
    "$T^\\pi$ is an operator on state-action value functions, that transforms a function $Q:S\\times A\\rightarrow \\mathbb{R}$ into:\n",
    "\\begin{align*}\n",
    "T^\\pi Q\\left(s,a\\right) &= r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ Q(s',\\pi(s')) \\right]\\\\\n",
    " &= r\\left(s,a\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,a\\right) Q\\left(s', \\pi\\left(s'\\right)\\right)\n",
    "\\end{align*}\n",
    "</div>\n",
    "\n",
    "Note that, fundamentally, we have written 4 times the same thing in the block above.  \n",
    "So finding $V^\\pi$ (resp. $Q^\\pi$) boils down to solving the evaluation equation $V= T^\\pi V$ (resp. $Q = T^\\pi Q$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b2d64b-aee7-4047-9c64-b7d66ec78171",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Let's write these equations a few more times. We have written them for a memoryless deterministic policy and a reward model of the form $r(s,a)$. Can you write $T^\\pi V$ and $T^\\pi Q$ for a memoryless stochastic policy and a reward model of the form $r(s,a,s')$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab89c337-57a2-4a30-b61c-27de2d3d712f",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "$$T^\\pi V\\left(s\\right) = \\mathbb{E}_{\\substack{a \\sim \\pi(a|s) \\\\ s'\\sim p(s'|s,a)}} \\left[ r(s,a,s') + \\gamma V(s') \\right]$$\n",
    "\n",
    "$$T^\\pi Q\\left(s\\right) = \\mathbb{E}_{\\substack{s'\\sim p(s'|s,a) \\\\ a' \\sim \\pi(a'|s')}} \\left[ r(s,a,s') + \\gamma Q(s',a') \\right]$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46fb24-9879-4cd5-a751-6c2f679c3d74",
   "metadata": {},
   "source": [
    "Let's write the evaluation operator (yet) another time. This time, we want to focus on random variables and name them.\n",
    "\n",
    "Let us define a *bootstrapped return* $G^\\pi_m(s,a,Q)$, for $m\\geq 1$, as the random variable:\n",
    "$$G^\\pi_m(s,a,Q) = \\sum\\limits_{t = 0}^{m-1} \\gamma^t R_t + \\gamma^m Q(S_m, A_m) \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s, A_0=a\\\\ A_t \\sim \\pi(S_t)\\textrm{ for }t>0,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1}).\\end{array}$$\n",
    "\n",
    "Note that this term of *bootstrap* has nothing to do with the statistical procedure of *[bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))* and is fully related to the idea of appending a \"bootstrap\" value at the end of a sequence of sampled rewards. It is a historical term whose use will be crucial in the next chapter. \n",
    "\n",
    "In particular:\n",
    "$$G^\\pi_1(s,a,Q) = R_0 + \\gamma Q(S_1, A_1) \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s, A_0=a\\\\ A_1 \\sim \\pi(S_1),\\\\ S_{1}\\sim p(\\cdot|S_0,A_0),\\\\ R_0 = r(S_0,A_0,S_{1}).\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a845e0b3-7d9a-4508-b78a-b0c6e6fac63b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "What is the link between $(T^\\pi Q)(s,a)$ and $G^\\pi_1(s,a,Q)$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7a3148-667f-42be-b065-2e240489e723",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "$$(T^\\pi Q)(s,a) = \\mathbb{E} \\left[ G^\\pi_1(s,a,Q) \\right]$$\n",
    "\n",
    "This last exercise was not for pure amusement: it will come in handy later on.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31dd548",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have gone far from our inspiration problems (the patient prescription, FrozenLake). Let's make all this very concrete:\n",
    "- A policy $\\pi$ is an agent's behaviour\n",
    "- In every state $s$, one can expect to gain $V^\\pi(s)$ in the long run by applying $\\pi$\n",
    "- $V^\\pi(s)$ is the sum of the reward on the first step $r(s,\\pi(s))$ and the expected long-term return from the next state $\\gamma \\mathbb{E}_{s'} \\left[V^\\pi(s')\\right]$ \n",
    "- The function $V^\\pi$ actually obeys the linear system of equations above that simply link the value of a state with the values of its successors in an episode.\n",
    "\n",
    "We can stop for a minute on the $T^\\pi$ evaluation operator (that maps a function $S\\rightarrow\\mathbb{R}$ to another function $S\\rightarrow\\mathbb{R}$) and the search for $V^\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af06d78",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-success\"><b>Properties of $T^\\pi$</b><br>\n",
    "<ol>\n",
    "<li> $T^\\pi$ is an affine operator, it defines a linear system of equations.<br>\n",
    "<li> $T^\\pi$ is a contraction mapping<br>\n",
    "    Specifically, with $\\gamma<1$, $T^\\pi$ is a $\\| \\cdot \\|_\\infty$-contraction mapping over the $\\mathbb{R}^S$ (resp. $\\mathbb{R}^{S A}$) Banach space.<br>\n",
    "$\\Rightarrow$ With $\\gamma<1$, $V^\\pi$ (resp. $Q^\\pi$) is the unique solution to the (linear) fixed point equation:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V=T^\\pi V$ (resp. $Q=T^\\pi Q$).\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db792538-af2b-492a-a35c-2b6a1afa68b4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Dynamic programming for the evaluation equation\n",
    "\n",
    "Let's use this second property to compute $Q^\\pi$ for the policy that always moves right on FrozenLake.\n",
    "\n",
    "Suppose we start with $Q_0(s,a) = 0$ for all $(s,a)$.\n",
    "\n",
    "Recall that, in FrozenLake, rewards are provided under the $r(s,a,s')$ form.\n",
    "\n",
    "Applying $T^\\pi$ once results in:\n",
    "$$Q_1(s,a) = \\sum_{s'} p(s'|s,a) \\left[ r(s,a,s') + \\gamma Q_0(s',\\pi(s')) \\right]$$\n",
    "\n",
    "In plain words, $Q_1$ is the one-step expected return under policy $\\pi$.\n",
    "\n",
    "Applying $T^\\pi$ twice results in:\n",
    "$$Q_2(s,a) = \\sum_{s'} p(s'|s,a) \\left[ r(s,a,s') + \\gamma Q_1(s',\\pi(s')) \\right]$$\n",
    "\n",
    "This is the two-step expected return.\n",
    "\n",
    "And so on.\n",
    "\n",
    "If we apply $T^\\pi$ enough times, $Q_n$ should become closer to $Q^\\pi$, whatever the chosen value for $Q_0$.\n",
    "\n",
    "In more formal words, because $T^\\pi$ is a contraction mapping, the sequence $Q_{n+1} = T^\\pi Q_n$ converges to $T^\\pi$'s fixed point.\n",
    "\n",
    "Because the procedure of repeatedly applying $T^\\pi$ breaks the problem of evaluating $\\mathbb{E} [\\sum_t \\gamma^t R_t]$ into sub-problems of increasing horizon, it is a **[dynamic programming](https://www.jstor.org/stable/j.ctv1nxcw0f)** procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ac891f-342f-4d1c-92fc-121605912e57",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise (together)</b><br>\n",
    "Let's compute the sequence $Q_{n+1} = T^\\pi Q_n$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764f7ed",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "# use render_mode=\"human\" to open the game window\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "\n",
    "pi = fl.RIGHT*np.ones((env.observation_space.n), dtype=np.uint8)\n",
    "nb_iter = 20\n",
    "gamma = 0.9\n",
    "\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "Qpi_sequence = [Q]\n",
    "for i in range(nb_iter):\n",
    "    Qnew = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for x in range(env.observation_space.n):\n",
    "        for a in range(env.action_space.n):\n",
    "            outcomes = env.unwrapped.P[x][a]\n",
    "            for o in outcomes:\n",
    "                p = o[0]\n",
    "                y = o[1]\n",
    "                r = o[2]\n",
    "                Qnew[x,a] += p * (r + gamma * Q[y,pi[y]])\n",
    "    Q = Qnew\n",
    "    Qpi_sequence.append(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b64904",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise (together)</b><br>\n",
    "Let's plot the sequence of $\\| Q_n - Q_{n-1} \\|_\\infty$ to verify the convergence of the sequence. Comment on the decrease rate and the contraction mapping property.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b6801",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "residuals = []\n",
    "for i in range(1, len(Qpi_sequence)):\n",
    "    residuals.append(np.max(np.abs(Qpi_sequence[i]-Qpi_sequence[i-1])))\n",
    "\n",
    "plt.plot(residuals)\n",
    "plt.figure()\n",
    "plt.semilogy(residuals);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4098e642",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# The optimality equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a5012",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can unfold the same kind of reasoning on the value of an optimal policy. We write:\n",
    "$$V^{\\pi^*} = V^*, \\quad Q^{\\pi^*} = Q^*$$\n",
    "\n",
    "<div class=\"alert alert-success\"><b>Theorem: Optimal greedy policy</b><br>\n",
    "Any policy $\\pi$ defined by $\\pi(s) \\in \\arg\\max\\limits_{a\\in A} Q^*(s,a)$ is an optimal policy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5a9f9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And $Q^*$ obeys the same type of recurrence relation:\n",
    "\n",
    "<div class=\"alert alert-success\"><b>Theorem: Bellman optimality equation</b><br>\n",
    "The optimal value function obeys:\n",
    "\\begin{align*}\n",
    "    V^*(s) &= \\max\\limits_{a\\in A} \\left[ r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} V^*(s') \\right]\\\\\n",
    "        &= \\max\\limits_{a\\in A} \\left[ r(s,a) + \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a) V^*(s') \\right]\n",
    "\\end{align*}\n",
    "or in terms of $Q$-functions:\n",
    "\\begin{align*}\n",
    "    Q^*(s,a) &= r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ \\max_{a'\\in A} Q^*(s',a') \\right]\\\\\n",
    "        &= r(s,a) + \\gamma \\sum\\limits_{s'\\in S}p(s'|s,a) \\max\\limits_{a'\\in A} Q^*(s',a')\n",
    "\\end{align*}\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d2d232",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As for the evaluation equation, we have actually written 4 times the same thing in the block above.  \n",
    "We have also defined the **Bellman optimality operator $T^*$** (on $V$ and $Q$ functions) as:\n",
    "<div class=\"alert alert-success\"><b>Bellman optimality operator</b><br>\n",
    "$$\\left(T^*V\\right)(s) = \\max\\limits_{a\\in A} \\left[ r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} V(s') \\right]$$\n",
    "$$\\left(T^*Q\\right)(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ \\max_{a'\\in A} Q(s',a') \\right]$$\n",
    "</div>\n",
    "\n",
    "So finding $V^*$ (resp. $Q^*$) boils down to solving $V= T^* V$ (resp. $Q = T^* Q$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e161502-c125-4496-ae02-6f5d2a353800",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Let's write these equations a few more times. We have written them for a reward model of the form $r(s,a)$. Can you write $T^* V$ and $T^* Q$ for a reward model of the form $r(s,a,s')$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbabec9a-6ccb-484b-9314-175a95099e9c",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "$$\\left(T^*V\\right)(s) = \\max\\limits_{a\\in A} \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ r(s,a,s') + \\gamma V(s') \\right]$$\n",
    "$$\\left(T^*Q\\right)(s,a) = \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[r(s,a,s') + \\gamma \\max_{a'\\in A} Q(s',a') \\right]$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737ad238",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-success\"><b>Properties of $T^*$</b><br>\n",
    "<ol>\n",
    "<li> $T^*$ is non-linear.<br>\n",
    "<li> $T^*$ is a contraction mapping<br>\n",
    "With $\\gamma<1$, $T^*$ is a $\\| \\cdot \\|_\\infty$-contraction mapping over the $\\mathbb{R}^S$ (resp. $\\mathbb{R}^{S A}$) Banach space.<br>\n",
    "$\\Rightarrow$ With $\\gamma<1$, $V^*$ (resp. $Q^*$) is the unique solution to the fixed point equation:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V=T^* V$ (resp. $Q=T^* Q$).\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ca2f91",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Dynamic programming for the optimality equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea2a27b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Repeatedly applying $T^*$ to an initial function $Q_0$ yields the sequence $Q_{n+1} = T^* Q_n$ that converges to $Q^*$.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Value iteration**  \n",
    "The algorithm that computes the sequence $Q_{n+1} = T^* Q_n$ for a finite number of iterations is called **value iteration**.\n",
    "</div>\n",
    "\n",
    "In practice, to compute $Q_{n+1}$ in a finite state and action space MDP, one loops through all states $s$ and actions $a$ and sets $Q_{n+1} = r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} [ \\max_{a'\\in A} Q_n(s',a') ]$ for each $(s,a)$ pair. Note that each of these assignments requires looping through states once again to compute the expectation.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "What is the time complexity of one iteration in value iteration, in terms of $|S|$ and $|A|$?  \n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "$O(|S|^2 |A|)$\n",
    "</details>\n",
    "\n",
    "If you are curious about the complexity of value iteration (and related algorithms), two good reads are:  \n",
    "Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). **[The complexity of Markov decision processes](https://www.jstor.org/stable/3689975?seq=7)**. Mathematics of operations research, 12(3), 441-450.  \n",
    "Littman, M. L., Dean, T. L., & Kaelbling, L. P. (1995). **[On the complexity of solving Markov decision problems](https://dl.acm.org/doi/abs/10.5555/2074158.2074203)**. In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence (pp. 394-402).\n",
    "\n",
    "The pseudo-code of value iteration is quite straightforward and left as a homework exercise.\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Exercise (value iteration)</b><br>\n",
    "Let's compute the sequence $Q_{n+1} = T^* Q_n$ for FrozenLake.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d571fbff",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "# use render_mode=\"human\" to open the game window\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "\n",
    "nb_iter = 20\n",
    "gamma = 0.9\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "Qopt_sequence = [Q]\n",
    "for i in range(nb_iter):\n",
    "    Qnew = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for x in range(env.observation_space.n):\n",
    "        for a in range(env.action_space.n):\n",
    "            outcomes = env.unwrapped.P[x][a]\n",
    "            for o in outcomes:\n",
    "                p = o[0]\n",
    "                y = o[1]\n",
    "                r = o[2]\n",
    "                Qnew[x,a] += p * (r + np.max(Q[y,:]) )\n",
    "    Q = Qnew\n",
    "    Qopt_sequence.append(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbad0ce",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise (convergence of VI)</b><br>\n",
    "Let's plot the sequence of $\\| Q_n - Q_{n-1} \\|_\\infty$ to verify the convergence of the sequence.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74f5890",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "residuals = []\n",
    "for i in range(1, len(Qopt_sequence)):\n",
    "    residuals.append(np.max(np.abs(Qopt_sequence[i]-Qopt_sequence[i-1])))\n",
    "\n",
    "plt.plot(residuals)\n",
    "plt.figure()\n",
    "plt.semilogy(residuals);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0498dec9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are alternatives to Value Iteration with algorithms such as Gauss-Seidel Value Iteration, Asynchronous Value Iteration, Policy Iteration or Modified Policy Iteration for instance. Check the exercises below to go further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f319489a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this section, we have built upon the MDP properties of the environment we wish to control, in order to characterize policies through their value functions.\n",
    "\n",
    "We have learned that:\n",
    "- $Q^\\pi$ is a solution to the Bellman evaluation equation\n",
    "$$Q = T^\\pi Q$$\n",
    "- $Q^*$ is a solution to the Bellman optimality equation\n",
    "$$Q = T^* Q$$\n",
    "- Value Iteration constructs the sequence of $Q_{n+1} = T^* Q_n$ value functions\n",
    "\n",
    "So we have built the second stage of our three-stage rocket:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**What is an optimal strategy?**  \n",
    "An optimal policy is one that yields optimal cumulated rewards. It is a policy that is *greedy* with respect to an optimal value function $Q^*$. Such a value function obeys Bellman's optimality equation and can be computed via dynamic programming.\n",
    "</div>\n",
    "\n",
    "Still, we are dependent on a characterization of $\\pi^*$ that relies on the knowledge of the MDP.\n",
    "\n",
    "But we could imagine that we use a procedure that *learns* $Q_{n+1}$ from samples of $T^* Q_n$. If such samples can be obtained from interaction with the system to control, it might be possible to learn $Q^*$ without knowing the MDP: that's where we really start talking about *reinforcement learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d2749b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Homework\n",
    "\n",
    "The exercises below are here to help you play with the concepts introduced above, to better grasp them. They also introduce additional important notions. They are not optional to reach the class goals. Often, the provided answer reaches out further than the plain question asked and provides comments, additional insights, or external references."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a092f6",
   "metadata": {},
   "source": [
    "## Dynamic programming for the evaluation equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e82b02-7229-4e8b-8f2b-9175aaa3e85a",
   "metadata": {},
   "source": [
    "As we've seen above, one can apply the principle of dynamic programming to both the evaluation and the optimality operators: use a value function $V_0$ (abstracting an $n$-step horizon criterion) to derive a value function $V_1 = T V_0$ (over $n+1$ steps). We will keep the name *value iteration* for the algorithm that does this for the optimality operator $T^*$ only, although some authors might use it for the repeated application of $T^\\pi$ too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6531e08e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Write a function `Q_from_V(env,V)` that computes the $|S|\\times |A|$ matrix $Q^\\pi$, given $V^\\pi$ for FrozenLake. Use $\\gamma=0.9$.<br>\n",
    "Suppose $V^\\pi(s)=0$ in all $s$. Use your function to compute $Q^\\pi(s,a)$ given $V^\\pi$. Comment.\n",
    "</div>\n",
    "\n",
    "Recall that in the previous class, we introduced a few utility functions and accessed the transition probabilities and rewards of FrozenLake using the `env.unwrapped.P`. These are recalled below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0c4163-a812-431d-b544-2fbb11e6dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_actions.py\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "actions = {fl.LEFT: '\\u2190', fl.DOWN: '\\u2193', fl.RIGHT: '\\u2192', fl.UP: '\\u2191'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fdb5f9-9136-48a5-aeb7-9a42613e990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_to_s.py\n",
    "def to_s(env,row,col):\n",
    "    return row*env.unwrapped.ncol+col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cbc7cb-748f-43c3-bfaf-74e01f127773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_to_row_col.py\n",
    "def to_row_col(env,s):\n",
    "    col = s%env.unwrapped.ncol\n",
    "    row = int((s-col)/env.unwrapped.ncol)\n",
    "    return row,col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626cf29f-e264-4610-b09a-49b55c8ab3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the utility functions\n",
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "from solutions.RL1_utility_functions import *\n",
    "from solutions.fl_actions import actions\n",
    "from solutions.fl_to_s import to_s\n",
    "from solutions.fl_to_row_col import to_row_col\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "\n",
    "state = to_s(env,0,1)\n",
    "action = 0\n",
    "outcomes = env.unwrapped.P[state][action]\n",
    "print(outcomes)\n",
    "print()\n",
    "print(\"outcomes for the transition from state \", to_row_col(env,state), \" and action \", actions[action], \":\", sep='')\n",
    "for o in outcomes:\n",
    "    proba      = o[0]\n",
    "    next_state = o[1]\n",
    "    reward     = o[2]\n",
    "    isTerminal = o[3]\n",
    "    print(\" reach state \", to_row_col(env,next_state), \\\n",
    "          \" and get reward \", reward, \\\n",
    "          \" with proba \", proba, \". \", sep='', end=\"\")\n",
    "    if isTerminal:\n",
    "        print(\"Transition is terminal.\")\n",
    "    else:\n",
    "        print(\"Transition is not terminal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5534f5-de38-4f2c-9400-06cd59075256",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0f8eb9-d047-4109-98b4-91fe699145d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_Q_from_V.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe6928b-44d3-43e5-a848-2561e9b18b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "\n",
    "V = np.zeros((env.observation_space.n))\n",
    "Q = Q_from_V(env,V)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e892beb0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "Write a function `greedyQpolicy(env,Q)` that takes a $Q$ function for FrozenLake and returns the greedy policy as an array of action indices.<br>\n",
    "Use the utility function below to print the greedy policy for the $Q$ function of the previous exercise in a human-friendly format.<br>\n",
    "Comment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952226c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_print_policy.py\n",
    "from solutions.fl_actions import actions\n",
    "from solutions.fl_to_s import to_s\n",
    "\n",
    "def print_policy(env,pi):\n",
    "    for row in range(env.unwrapped.nrow):\n",
    "        for col in range(env.unwrapped.ncol):\n",
    "            print(actions[pi[to_s(row,col)]], end='')\n",
    "        print()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb57e2d9-0fb2-4aa3-af29-e108992fc30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c2770-e325-4dcc-bf77-f95d64727c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_greedyQpolicy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba879ca-759d-40c0-b70e-59585b66ca3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from  solutions.fl_print_policy import print_policy\n",
    "from solutions.fl_Q_from_V import Q_from_V\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "\n",
    "V = np.zeros((env.observation_space.n))\n",
    "Q = Q_from_V(env,V)\n",
    "pi = greedyQpolicy(env,Q)\n",
    "print(pi)\n",
    "print_policy(env,pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ba608",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (variations on the theme of the evaluation equation)**  \n",
    "\n",
    "When we introduced the evaluation operator we wrote that finding $V^\\pi$ (resp. $Q^\\pi$) boiled down to solving the evaluation equation $V= T^\\pi V$ (resp. $Q = T^\\pi Q$).  \n",
    "- Suppose the state space is finite. Write $V= T^\\pi V$ as matrix-vector equations. Explain the dimension of all variables.\n",
    "- How does this extend to the case of continuous state spaces?\n",
    "- Write the evaluation equation for a stochastic policy in a discrete state-action space MDP (yes, we have almost done it in class, this is just a reminder).\n",
    "- Repeat the previous questions for $Q = T^\\pi Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db810bef",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "- When the state space is discrete:  \n",
    "$V$ is a vector of size $|S|$,  \n",
    "$P^\\pi$ is a matrix containing the values $P^\\pi_{ij} = p\\left(s_j|s_i,\\pi(s_i)\\right)$   \n",
    "and, similarly, $r^\\pi$ is a vector containing the values $r^\\pi_i = r(s_i,\\pi(s_i))$.  \n",
    "In better words, $P^\\pi$ is the <i>transition kernel</i> of the Markov chain describing the state dynamics under policy $\\pi$ and $r^\\pi$ is the associated reward model.  \n",
    "Then $T^\\pi$ is the linear operator in $\\mathbb{R}^{|S|}$ that maps $V$ to $r^\\pi + \\gamma P^\\pi V$.\n",
    "\n",
    "- This generalizes straightforwardly to the continuous states case:   \n",
    "$V$ is a function in the $\\mathbb{R}^S$ function space (the generalization of the vector in the previous sentence),    \n",
    "$r^\\pi$ becomes the function $s\\mapsto r(s,\\pi(s))$    \n",
    "and  $P^\\pi$ becomes the linear operator over $\\mathbb{R}^S$ that maps function $V$ to function $s\\mapsto \\int\\limits_{s'\\in S} p\\left(s'|s,\\pi\\left(s\\right)\\right) V\\left(s'\\right)ds'$.  \n",
    "Then $T^\\pi$ is the linear operator in $\\mathbb{R}^S$ that maps $V$ to the function $s\\mapsto r^\\pi(s) + \\gamma (P^\\pi V)(s) = r\\left(s,\\pi\\left(s\\right)\\right) + \\gamma \\int\\limits_{s'\\in S} p\\left(s'|s,\\pi\\left(s\\right)\\right) V\\left(s'\\right)ds'$.\n",
    "\n",
    "- For stochastic policies:\n",
    "\\begin{align*}\n",
    "    \\quad V^\\pi(s) &= \\mathbb{E}_{a\\sim\\pi(a|s)} \\left[ r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ V^\\pi\\left(s'\\right) \\right] \\right] \\\\\n",
    "        &= \\sum\\limits_{a\\in A} \\pi(a|s) \\left(r(s,a) + \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a)V^\\pi(s') \\right)\n",
    "\\end{align*}<br>\n",
    "<br>\n",
    "If you prefer using $r(s,a,s')$ instead of $r(s,a)$:<br>\n",
    "\\begin{align*}\n",
    "    V^\\pi\\left(s\\right) &= \\mathbb{E}_{\\substack{a\\sim \\pi(s)\\\\ s'\\sim p(s'|s,a)}} \\left[ r(s,a,s') + \\gamma V^\\pi\\left(s'\\right) \\right]\\\\\n",
    "        &= \\sum\\limits_{s'\\in S} \\sum\\limits_{a\\in A} p\\left(s'|s,a\\right) \\pi\\left(a|s\\right) \\left[ r\\left(s,a,s'\\right) + \\gamma V^\\pi\\left(s'\\right) \\right]\n",
    "    \\end{align*}\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56809266",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "We have seen that $T^\\pi$ is an affine operator. \n",
    "For a discrete state and action space, write $V^\\pi$ the vector of values functions. What is the size of this vector? Write the evaluation equation as a set of operations on such vectors. Deduce a closed-form solution to the evaluation equation on $V$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f5a21",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "$V^\\pi$ is a column vector that lives in $\\mathbb{R}^{|S|}$.  \n",
    "\n",
    "Let $V$ be a column vector of $\\mathbb{R}^{|S|}$. The evaluation operator is written:\n",
    "$$(T^\\pi V) (s) = r\\left(s,\\pi\\left(s\\right)\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,\\pi\\left(s\\right)\\right) V\\left(s'\\right).$$\n",
    "So this is a row of the $V=T^\\pi V$ equation.\n",
    "\n",
    "Let $p^\\pi(s)$ be the row vector composed of:\n",
    "$$p^\\pi(s) = \\left(p(s'|s,\\pi(s))\\right)_{s'\\in S}.$$\n",
    "And let $r^\\pi(s)$ be the scalar $r(s,\\pi(s))$. Then the evaluation operator can be written:\n",
    "$$(T^\\pi V)(s) = r^\\pi(s) + \\gamma p^\\pi(s)\\cdot V(s).$$\n",
    "\n",
    "Let $P^\\pi$ be the matrix obtained by stacking all row vectors $p^\\pi(s)$ on top of each other.  \n",
    "    $P^\\pi$ is the transition kernel of the Markov chain defined by the MDP and the policy $\\pi$.  \n",
    "Let also $r^\\pi$ be the column vector of all $r^\\pi(s)$.  \n",
    "Then the evaluation operator can be written in matrix-vector form as:\n",
    "$$T^\\pi V = r^\\pi + \\gamma P^\\pi V.$$\n",
    "\n",
    "Since $V^\\pi$ is a solution to $V = T^\\pi V$, one has $V = r^\\pi + \\gamma P^\\pi V$.  \n",
    "So $(I-\\gamma P^\\pi) V = r^\\pi$.  \n",
    "Since $P^\\pi$ is a probability matrix, all its eigenvalues are smaller or equal to one.  \n",
    "So the eigenvalues of $\\gamma P^\\pi$ are all strictly smaller than one (because $\\gamma <1$).  \n",
    "Consequently $I-\\gamma P^\\pi$ has only strictly positive eigenvalues and is thus invertible. Finally, the unique solution to $V = T^\\pi V$ is:\n",
    "$$V^\\pi = \\left( I - \\gamma P^\\pi \\right)^{-1} r^\\pi.$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d14132",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "Generalize the code above to a function `policy_eval_lin` that takes any policy as input. We'll suppose in this case that the policy is an array of actions.\n",
    "Use the previous exercise to function `policy_eval_lin(env,pi)` that computes $V^\\pi$ by matrix inversion. The policy `pi` is given as an array of actions. To do this, you'll need to compute $r^\\pi$ and $P^\\pi$. Again, $\\gamma = 0.9$.  \n",
    "Take the policy that always moves right and check if your result for $V^\\pi(s_0)$ is consistent with the Monte Carlo estimate of the previous chapter's exercises.\n",
    "</div>\n",
    "\n",
    "Use the cell below to recall the solution to the Monte Carlo estimation exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e280423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "from solutions.fl_mc_eval import mc_eval\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "pi = fl.RIGHT*np.ones((env.observation_space.n))\n",
    "Vepisode = mc_eval(env,pi,100000)\n",
    "print(\"value estimate:\", np.mean(Vepisode))\n",
    "print(\"return variance:\", np.std(Vepisode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ce3a4c-91b7-4a82-9665-28511d1ff42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964be685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_policy_eval_lin.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562ba679-0c24-4419-a884-d60f3c793d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "pi0 = fl.RIGHT*np.ones((env.observation_space.n))\n",
    "V_pi0 = policy_eval_lin(env,pi0)\n",
    "print(V_pi0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e9bd7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**      \n",
    "Generalize the code provided in the section about the evaluation equation to write two functions that compute the value function of any policy, using the contraction mapping property.  \n",
    "The first function is `policy_eval_iter(env,pi,max_iter)`; it takes a policy `pi` and applies $T^\\pi$ `max_iter` times by updating each state's value individually.  \n",
    "The second function is `policy_eval_iter_mat(env,pi,max_iter)`; it takes a policy `pi` and applies $T^\\pi$ `max_iter` times using matrix-vector operations.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02da8c9-ca43-4048-8a0d-76f55acd1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1837277-8617-4b29-914c-fd7c343241b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_policy_eval_iter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b10f99-c028-4c11-a793-bd635b24b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "pi0 = fl.RIGHT*np.ones((env.observation_space.n))\n",
    "V_pi0 = policy_eval_iter(env,pi0,10000)\n",
    "print(V_pi0)\n",
    "\n",
    "V_pi0 = policy_eval_iter_mat(env,pi0,10000)\n",
    "print(V_pi0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c22786",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise and a word on residuals.**  \n",
    "    \n",
    "Between two applications of $T^\\pi$, the distance between $V_{n+1}$ and $V_n$ decreases as $\\|V_{n+1}-V_n\\| = \\|r^\\pi + \\gamma P^\\pi V_n - V_n\\|$. Since $T^\\pi$ is a contraction mapping, we have $\\|V_{n+1}-V_n\\| < \\|V_{n}-V_{n-1}\\|$. Let's call this distance at time step $n$ the **residual**. Then the successive residuals monotonically tend to zero.  \n",
    "   \n",
    "Use this property on the residuals to modify `policy_eval_iter_mat(env,pi,max_iter)` into a `policy_eval_iter_mat2(env,pi,epsilon,max_iter)` function, by adding a precision parameter `epsilon` that specifies the maximum error allowed on $V^\\pi$. Return both $V^\\pi$ and the sequence of residuals.  \n",
    "Note: we still keep a maximum number of iterations in the functions to stop the computation in case one specifies an `epsilon` that is too small.   \n",
    "Plot the sequence of residuals and display the number of iterations necessary to reach the chose precision `epsilon`. Comment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd9147f-1628-4043-9e66-19b507309f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653a75a7-8b21-4dac-837d-6007910647fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_policy_eval_iter_mat2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee254a-13c0-4d89-a91a-b4502c1f2870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "pi0 = fl.RIGHT*np.ones((env.observation_space.n))\n",
    "V_pi0, residuals = policy_eval_iter_mat2(env,pi0,1e-4,10000)\n",
    "print(V_pi0)\n",
    "plt.plot(residuals)\n",
    "plt.figure()\n",
    "plt.semilogy(residuals)\n",
    "print(\"number of iterations:\", residuals.size)\n",
    "print(\"last residual\", residuals[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0205244-4c18-47a6-9326-6d192ac8ffb5",
   "metadata": {},
   "source": [
    "## Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c224388d-c77d-4f04-8d3e-8c018a44bd83",
   "metadata": {},
   "source": [
    "As we've seen before, value iteration is the algorithm that immediately stems from the contraction mapping property of $T^*$.\n",
    "\n",
    "Let's use this to compute $V^*$. We need to remember that since $T^*$ is a contraction mapping, the sequence $V_{n+1}=T^* V_n$ converges to $T^*$'s fixed point (which happens to be $V^*$ according to the property)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a066ca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "\n",
    "Write a function `value_iteration(env, V, epsilon, max_iter)` that takes `V` as initial value function, and applies $T^*$ repeatedly until the residual is smaller than `epsilon` or at most `max_iter` iterations have been made. Return the value function and the sequence of residuals. Again, $\\gamma = 0.9$.  \n",
    "The solution to this exercise is almost the same code as that we have seen in class.  \n",
    "Plot the residuals and comment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cd1723-75a8-444c-9b8a-aef3bd1ed8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42932307-56a6-470f-a0f4-2f3aab73b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_value_iteration.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76644ea-38ab-4788-9ef0-c2dcdea96d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "Vinit = np.zeros((env.observation_space.n))\n",
    "Vstar, residuals = value_iteration(env, Vinit, 1e-4, 10000)\n",
    "print(Vstar)\n",
    "plt.plot(residuals)\n",
    "plt.figure()\n",
    "plt.semilogy(residuals)\n",
    "print(\"number of iterations:\", residuals.size)\n",
    "print(\"last residual\", residuals[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e24f04",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise</b><br>\n",
    "Write the pseudo-code of Value Iteration for $V$ functions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5221178a",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Input data: $V$, $\\epsilon$<br>\n",
    "Init: $\\Delta = \\epsilon+1$<br>\n",
    "While $\\Delta \\geq \\epsilon$:<br>\n",
    "&nbsp;&nbsp;&nbsp; For $s \\in S$:  <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For $a\\in A$:  <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Q(s,a) = r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$ <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $V_{new}(s) = \\max_a Q(s,a)$  <br>\n",
    "&nbsp;&nbsp;&nbsp; $\\Delta = \\| V_{new}-V \\|_\\infty$  <br>\n",
    "&nbsp;&nbsp;&nbsp; $V = V_{new}$  <br>\n",
    "Return $V$  <br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8cb56d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "Compute and display an optimal policy for the FrozenLake game, using Value Iteration. If you import the results of previous exercises to get Q from V, to extract a greedy policy from Q, and to plot the policy, then you almost don't need to write any new code.  \n",
    "Comment the obtained policy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a473f7b9-e1ad-495f-9e91-40f454649210",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c187f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_display_vi_exercise.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a26bb4",
   "metadata": {},
   "source": [
    "Value Iteration is often viewed as an algorithm that maintains a memory of a $V$ function and iterates until this function is close enough to $V^*$. Still, it is interesting (and will be useful later in the homework exercises) to rephrase the algorithm in alternate terms:  \n",
    "\n",
    "Start with a state-action value function $Q$\n",
    "- Define $\\pi(s) \\in \\arg\\max_a Q(s,a)$ in all states and actions.  \n",
    "  In the algorithm above, this operation is performed when the $\\max_a$ is solved, just the action is not explicitly stored.\n",
    "- $Q \\leftarrow T^\\pi Q$,  \n",
    "  that is, for all $s$ and $a$, $Q(s,a) \\leftarrow r(s,a) + \\gamma \\mathbb{E}_{s'} \\left[ Q(s',\\pi(s')) \\right]$.  \n",
    "- repeat\n",
    "\n",
    "The first operation can be defined through a *greediness operator* $\\mathcal{G}$, which defines $\\pi$ as a policy that picks a *greedy* action with respect to $Q$ in all states $s$: $\\pi \\in \\mathcal{G}Q$.  \n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Greediness operator**  \n",
    "For deterministic policies:\n",
    "$$\\pi \\in \\mathcal{G} Q, \\Leftrightarrow \\pi(s) \\in \\arg\\max_{a\\in A} Q(s,a)$$\n",
    "\n",
    "This can be extended to stochastic policies:\n",
    "$$\\pi \\in \\mathcal{G} Q, \\Leftrightarrow \\pi(s) \\in \\arg\\max_{\\pi \\in \\Delta_A} \\mathbb{E}_{a\\sim\\pi} \\left[Q(s,a)\\right]$$\n",
    "</div>\n",
    "\n",
    "Then, value iteration is the algorithm that defines the sequences $\\pi_n$ and $Q_n$ as:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Value iteration**\n",
    "$$\\pi_n \\in \\mathcal{G} Q_n, \\quad Q_{n+1} = T^{\\pi_n} Q_n.$$\n",
    "</div>\n",
    "\n",
    "So Value Iteration can be seen as picking the greedy action with respect to $Q$ and then updating $Q$ with just one application of $T^\\pi$. This application of $T^\\pi$ alone is not sufficient for $Q$ to reach $Q^\\pi$ but it changes $Q$ and so it changes what the greedy action will be at the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d199ea",
   "metadata": {},
   "source": [
    "## Policy Iteration and Modified Policy Iteration\n",
    "\n",
    "The Policy Iteration algorithm stems from the following remark. Suppose we have a policy $\\pi$ and know its value function $V^\\pi$ and state-action value function $Q^\\pi$. Then, the non-stationary policy $\\pi'$ that acts greedily with respect to $Q^\\pi$ for the first time step and then follows $\\pi$ has a value function $V^{\\pi'}$ that is greater or equal to $V^\\pi$ (equal if $\\pi$ is optimal, strictly greater otherwise). Actually, the contraction property of $T^*$ insures that the stationary policy $\\pi'$ that is greedy with respect to $Q^\\pi$ is at least as good as $\\pi$, that is $V^{\\pi'}\\geq V^\\pi$. Consequently, the sequence of policies defined by $\\pi_{n+1}(s) = \\arg\\max_{a\\in A} Q^{\\pi_n}(s,a)$ has a monotonically improving corresponding sequence of value functions $V^{\\pi_n}$ and converges to $\\pi^*$.  \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Policy improvement theorem**  \n",
    "If $\\pi_{n+1} \\in \\mathcal{G}Q^{\\pi_n}$, then $Q^{\\pi_{n+1}} \\geq Q^{\\pi_n}$.\n",
    "</div>\n",
    "\n",
    "A proof of this statement can be found in the first chapter of the **[Markov Decision Processes in Artifical Intelligence](https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118557426.ch1)** book.  \n",
    "\n",
    "Policy iteration is the algorithm that stems from this theorem and computes the sequence of $\\pi_{n+1} \\in \\mathcal{G}Q^{\\pi_n}$.  \n",
    "Let's make this more simple with a drawing. Policy iteration alternates two phases:\n",
    "1. Evaluate $\\pi_n$ $\\rightarrow Q^{\\pi_n}$\n",
    "2. Compute $\\pi_{n+1}$ as a $Q^{\\pi_n}$-greedy policy\n",
    "\n",
    "<center><img src=\"img/policyiteration.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b015bf7f",
   "metadata": {},
   "source": [
    "The process above defines a sequence of policies **and** value functions. Since, for finite state and action spaces, the number of policies is finite, Policy Iteration is guaranteed to converge in a finite number of iterations.\n",
    "\n",
    "Policy Iteration was introduced in R. A. Howard's book **[Dynamic Programming and Markov Processes](https://psycnet.apa.org/record/1961-01474-000)** (1960).\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "\n",
    "Before we start implementing, let's first note that $\\pi_{n+1} = \\pi_n$ is not a valid termination criterion for Policy Iteration!  \n",
    "Can you explain why? What would be a sound termination criterion?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daaccb6",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Although the optimal value function is unique (since $T^*$ is a contraction mapping), there may be several optimal policies, all having the same value function. This happens for instance when two actions have the same $Q^*(s,a)$ value in a given state $s$. Consequently, there may be several $Q^{\\pi_n}$-greedy policies and policy iteration might keep on hopping from one optimal policy to the other indefinitely.\n",
    "\n",
    "A better termination criterion would be $Q^{\\pi_{n+1}} = Q^{\\pi_{n}}$, or at least $||Q^{\\pi_{n+1}} - Q^{\\pi_{n}}|| \\leq \\epsilon$.\n",
    "</details>\n",
    "\n",
    "As for Value Iteration, those familiar with Dynamic Programming will remark that Policy Iteration is a Dynamic Programming algorithm in policy space, monotonically hopping from policy to policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c158a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "Write the pseudo-code of Policy Iteration using dynamic programming to evalutate $\\pi_n$.  \n",
    "For the sake if simplicity and despite the warning of the previous exercise, use $\\pi_n=\\pi_{n-1}$ as the termination condition.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d0901d",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Input data: $\\pi$, $V$, $\\epsilon$<br>\n",
    "Init: $\\Delta = \\epsilon+1$<br>\n",
    "Do:<br>\n",
    "&nbsp;&nbsp;&nbsp; # Policy evaluation: apply $T^\\pi$ to $V$ until precision $\\epsilon$<br>\n",
    "&nbsp;&nbsp;&nbsp; While $\\Delta \\geq \\epsilon$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $V_{old} = V$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For $s \\in S$:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $V(s) = r(s,\\pi(s)) + \\gamma \\sum_{s'} p(s'|s,\\pi(s)) V_{old}(s')$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\Delta = \\| V-V_{old} \\|_\\infty$<br>\n",
    "&nbsp;&nbsp;&nbsp; # Policy improvement<br>\n",
    "&nbsp;&nbsp;&nbsp; $\\pi_{old} = \\pi$ <br>\n",
    "&nbsp;&nbsp;&nbsp; For $s \\in S$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For $a \\in A$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Q(s,a) = r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$ <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\pi(s) \\in \\arg\\max_a Q(s,a)$<br>\n",
    "While $\\pi_{old} \\neq \\pi$ <br>\n",
    "Return $\\pi$  <br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b41165f-d0f7-474d-8fb1-a128a40d071c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "What is the time complexity of one iteration of Policy Iteration in terms of $|S|$ and $|A|$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b569b82d-75be-43c7-8e79-06ba059e1c33",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "$O(|S|^2 |A|) + O(|S|^3)$\n",
    "\n",
    "The evaluation step has the complexity of inverting an $|S|\\times|S|$ matrix, that's $O(|S|^3)$.  \n",
    "The improvement step iterates over states and actions, and states again (for the expectation computation), that's $O(|S|^2 |A|)$.  \n",
    "In practice, for finite state and action spaces, Policy Iteration converges in a finite number of steps (contrarily to Value Iteration). But each of these steps requires the resolution of $V = T^\\pi V$ which is where the real computational cost is.  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458c95e2",
   "metadata": {},
   "source": [
    "The previous exercise was a little convoluted. We can make things simpler.\n",
    "\n",
    "In all rigor, Policy Iteration is the algorithm that applies the Bellman evaluation operator an infinite number of times to $V$ so that it reaches $V^\\pi$, then defines $\\pi$ as the greedy policy with respect to $V$. With the notations introduced previously, policy iteration is the algorithm which defines the sequences $\\pi_n$ and $Q_n$ as:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Policy iteration**  \n",
    "$$\\pi_n \\in \\mathcal{G} Q_n, \\quad Q_{n+1} = (T^{\\pi_n})^\\infty Q_n.$$\n",
    "</div>\n",
    "\n",
    "Obviously, an infinite number of applications of $T^\\pi$ is not very practical. In the previous exercises, we controlled the error made in the convergence to $V^\\pi$ with a parameter $\\epsilon$, that allowed to perform a finite number of $T^\\pi$ applications.\n",
    "\n",
    "A simpler way is to define a certain number $m$ of applications of $T^\\pi$. This provides the **Modified Policy Iteration** algorithm, that applies the $T^\\pi$ operator $m$ times to update $V$, then defines $\\pi$ as the greedy policy with respect to $V$.\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Modified policy iteration**  \n",
    "$$\\pi_n \\in \\mathcal{G} Q_n, \\quad Q_{n+1} = (T^{\\pi_n})^m Q_n.$$\n",
    "</div>\n",
    "\n",
    "Modified Policy Iteration was introduced by M. L. Puterman and M. C. Shin in **[Modified Policy Iteration Algorithms for Discounted Markov Decision Problems](https://pubsonline.informs.org/doi/abs/10.1287/mnsc.24.11.1127)** (1978).\n",
    "\n",
    "Note that value iteration and policy iteration are two extremes of modified policy iteration (with $m=1$ and $m=\\infty$ respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228bcbb7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "Write the pseudo-code of Modified Policy Iteration.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bccfaee",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Input data: $\\pi$, $m$<br>\n",
    "Init: $\\Delta = \\Delta_\\pi = \\epsilon+1$<br>\n",
    "While $\\Delta_\\pi \\geq \\epsilon$:<br>\n",
    "&nbsp;&nbsp;&nbsp; # Policy evaluation, solve $V=T^\\pi V$<br>\n",
    "&nbsp;&nbsp;&nbsp; $V_{old} = V$<br>\n",
    "&nbsp;&nbsp;&nbsp; For $i \\in [1,m]$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For $s \\in S$:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $V(s) \\leftarrow (T^\\pi V)(s)$<br>\n",
    "&nbsp;&nbsp;&nbsp; # Policy improvement<br>\n",
    "&nbsp;&nbsp;&nbsp; For $s \\in S$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For $a \\in A$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Q(s,a) = r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$ <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\pi(s) \\in \\max_a Q(s,a)$<br>\n",
    "Return $\\pi$  <br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80fcf9",
   "metadata": {},
   "source": [
    "Interestingly, Modified Policy Iteration benefits from the same convergence properties as Policy Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232d9223",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "What is Modified Policy Iteration with $m=1$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55d2662",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "According to the last remark about Value Iteration, that's exactly Value Iteration!  \n",
    "So Modified Policy Iteration is actually a continuum of algorithms between Value Iteration and Policy Iteration.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f521e424",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Write a function `modified_policy_iteration(env,pi0,m,max_iter)` that implements modified policy iteration. `max_iter` is the maximum number of iterations (that is, of computed policies). `m` is the number of applications of the $T^\\pi$ operator.  \n",
    "Since you will use a fixed number `m` of iterations for the resolution of $V=T^\\pi V$, you can reuse function `policy_eval_iter_mat` from the correction of the exercises on the evaluation equation. Recall also that you can use the previously defined `Q_from_V` and `greedyQpolicy` functions.  \n",
    "Compute and display an optimal policy for the FrozenLake game, using Modified Policy Iteration, with $m=500$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604df682-b30e-47be-9d67-bb6836a2bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31ebe92-384c-4478-8f6d-7c840348d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_modified_policy_iteration.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7482672-f1c3-4669-8e90-ba67315516ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "from solutions.RL2_utility_functions import print_policy\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "pi0 = fl.RIGHT*np.ones((env.observation_space.n))\n",
    "print_policy(pi0)\n",
    "m = 500\n",
    "policies = modified_policy_iteration(env,pi0,m,10)\n",
    "print(\"number of iterations:\", policies.shape[0])\n",
    "print_policy(policies[-1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5596384",
   "metadata": {},
   "source": [
    "As for Value Iteration, those familiar with Dynamic Programming will remark that Policy Iteration is a Dynamic Programming algorithm in policy space, monotonically hopping from policy to policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd4865c-caf3-48f0-b650-6131b99007b5",
   "metadata": {},
   "source": [
    "Do you remember how we introduced the $G^\\pi_m(s,a,Q)$ random variable and called it \n",
    "the *bootstrapped return* $G^\\pi_m(s,a,Q)$, for $m\\geq 1$? We had:\n",
    "$$G^\\pi_m(s,a,Q) = \\sum\\limits_{t = 0}^{m-1} \\gamma^t R_t + \\gamma^m Q(S_m, A_m) \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s, A_0=a\\\\ A_t \\sim \\pi(S_t)\\textrm{ for }t>0,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1}).\\end{array}$$\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Prove that $m$ applications of $T^\\pi$ to $Q$ actually compute $\\mathbb{E}[G^\\pi_m(s,a,Q)]$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92fca0b-4aa0-45d8-b6ff-566691770c9d",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Let $Q_m = (T^\\pi)^m Q$. We want to prove that $Q_{m}(s,a) = \\mathbb{E} \\left[ G^\\pi_{m}(s,a,Q) \\right]$\n",
    "\n",
    "We have already seen in class that $(T^\\pi Q)(s,a) = \\mathbb{E} \\left[ G^\\pi_1(s,a,Q) \\right]$. So $Q_1(s,a) = \\mathbb{E} \\left[ G^\\pi_1(s,a,Q) \\right]$: the statement is true for $m=1$.\n",
    "\n",
    "Let's suppose the statement is true for $m-1$: $Q_{m-1}(s,a) = \\mathbb{E} \\left[ G^\\pi_{m-1}(s,a,Q) \\right]$.\n",
    "\n",
    "Let $R_0$ be the random variable of the reward obtained when playing $a$ in $s$.\n",
    "\\begin{align*}\n",
    "Q_m(s,a) &= (T^\\pi (T^\\pi)^{m-1} Q)(s,a)\\\\\n",
    "&= \\mathbb{E} \\left[ R_0 + \\gamma Q_{m-1}(s',a') | s'\\sim p(\\cdot|s,a), a' \\sim \\pi(s') \\right]\\\\\n",
    "&= \\mathbb{E} \\left[ R_0 + \\gamma \\mathbb{E} \\left[ G^\\pi_{m-1}(s',a',Q) \\right] | s'\\sim p(\\cdot|s,a), a' \\sim \\pi(s') \\right]\\\\\n",
    "&= \\mathbb{E} \\left[ R_0 + \\gamma \\left[ \\sum\\limits_{t = 0}^{m-2} \\gamma^t R_{t+1} + \\gamma^{m-1} Q(S_m, A_m) \\right] \\right]\\\\\n",
    "&= \\mathbb{E} \\left[ \\sum\\limits_{t = 0}^{m-1} \\gamma^t R_{t} + \\gamma^m Q(S_m, A_m) \\right]\\\\\n",
    "&= \\mathbb{E} \\left[ G^\\pi_m(s,a,Q) \\right].\n",
    "\\end{align*}\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054594f2",
   "metadata": {},
   "source": [
    "## Solving MDPs with Linear Programming\n",
    "\n",
    "An alternative way of finding $V^*$ for discrete state and action spaces is by casting the optimality equation as a linear optimization problem. This formulation is mainly given for your curiosity and we will not study it any further.<br>\n",
    "<br>\n",
    "Recall the optimality equation:\n",
    "$$\\forall s\\in S, V(s)=\\max\\limits_{a\\in A} \\left[r(s,a) + \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a) V(s')\\right]$$\n",
    "\n",
    "The key remark to transform this into a linear program is to rephrase it as \"$V^*$ is the smallest value that dominates over all policy values\". This can be written as:\n",
    "$$\\left\\{ \\begin{array}{c}\n",
    "\\min \\sum\\limits_{s\\in S} V(s)\\\\\n",
    "s.t. \\ \\forall \\pi, \\ V \\geq T^\\pi V\n",
    "\\end{array} \\right.$$\n",
    "\n",
    "\"For all $\\pi$\" means for all possible association $s\\leftrightarrow a$, so this can be expanded as:\n",
    "$$\\left\\{ \\begin{array}{c}\n",
    "\\min \\sum\\limits_{s\\in S} V(s)\\\\\n",
    "s.t. \\ \\forall (s,a)\\in S\\times A, \\quad V(s) - \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a)V(s') \\geq r(s,a)\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "Which, finally, is a linear program with $|S|$ variables and $|S||A|$ constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4020121-b02d-44e6-b0d2-8995f88dd73e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Find an optimal value function (and deduce an optimal policy) for the FrozenLake environment using linear programming.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990fb6b3-d2b1-453d-ab74-fe628894c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "import numpy as np\n",
    "\n",
    "nS = env.observation_space.n\n",
    "nA = env.action_space.n\n",
    "P = np.zeros((nS,nA,nS))\n",
    "for x in range(env.observation_space.n):\n",
    "    for a in range(env.action_space.n):\n",
    "        outcomes = env.unwrapped.P[x][a]\n",
    "        for o in outcomes:\n",
    "            p = o[0]\n",
    "            y = o[1]\n",
    "            P[x,a,y] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b0beac-743f-413e-9c0f-0d876f8f372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a90a64-f566-4c29-83f4-1dc5a723d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/no_solution_yet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab779f3a",
   "metadata": {},
   "source": [
    "## Asynchronous Dynamic Programming\n",
    "\n",
    "We have seen that Value Iteration and Policy Iteration are Dynamic Programming algorithms. They follow a path, respectively in value function and in policy space that leads to $V^*$ and $\\pi^*$. But we can remark that they both perform *state-wise* operations such as:\n",
    "\n",
    "- $Q(s,a) \\leftarrow r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$\n",
    "- $V(s) \\leftarrow \\max_{a} r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$\n",
    "- $\\pi(s) \\leftarrow \\arg\\max_a Q^{\\pi}(s,a)$\n",
    "- $V(s) \\leftarrow r(s,\\pi(s)) + \\gamma \\sum_{s'} p(s'|s,\\pi(s)) V(s')$\n",
    "\n",
    "These state-wise operations are called Bellman backups.\n",
    "\n",
    "Let's use $V$ functions to describe Value Iteration. We can define the Bellman backup operator:\n",
    "- `BBV(V,s): return` $\\max_{a} r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$  \n",
    "  `BBV` is the operation performed in every state, in one pass of Value Iteration\n",
    "\n",
    "Alternatively, we can operate only on $Q$ functions and define the corresponding Bellman backup operators:\n",
    "- `BBQ(Q,pi,s,a): return` $r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) Q(s',\\pi(s'))$  \n",
    "  `BBQ` serves in all *evaluation* steps,\n",
    "- `BBpi(Q,s): return` $\\max_a Q(s,a)$ and $\\arg\\max_a Q(s,a)$  \n",
    "  `BBpi` serves in all *improvement* steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edb9975",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Suppose we maintain a memory of a function `V`.\n",
    "Using `BBV`, rewrite Value Iteration.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef70ae5",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "    \n",
    "\n",
    "Value Iteration:<br>\n",
    "```\n",
    "V(s) = Vinit(s) for all s\n",
    "while error>epsilon\n",
    "  for s in S\n",
    "    W(s) = BBV(V,s)\n",
    "  error = norm(W-V)\n",
    "  V = W\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8c0845",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Suppose we maintain a memory of a function `Q` and a policy `pi`. \n",
    "Using `BBQ` and `BBpi`, rewrite Value Iteration and Modified Policy Iteration.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01c0cc4",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "    \n",
    "\n",
    "Value Iteration:<br>\n",
    "```\n",
    "V(s) = Vinit(s) for all s\n",
    "while error>epsilon\n",
    "  for s in S\n",
    "    for a in A\n",
    "      Qnew(s,a) = BBQ(Q,pi,s,a)\n",
    "    W(s), pi(s) = BBpi(Qnew,s)\n",
    "  error = norm(W-V)\n",
    "  V = W\n",
    "```\n",
    "<br><br>\n",
    "Modified Policy Iteration:<br>\n",
    "```\n",
    "while(pi not constant)\n",
    "  Q(s,a) = 0 for all s,a\n",
    "  while error>epsilon\n",
    "    for k in [1,m]\n",
    "      for s,a in SxA\n",
    "        Qnew(s,a) = BBQ(s,a)\n",
    "  for s in S\n",
    "    V, pi = BBpi(s)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86170a4e",
   "metadata": {},
   "source": [
    "Let's take the pseudo-code of Value Iteration using a $V$ function from the exercises above. Why don't we perform directly `V(s) = BBV(V,s)`, instead of relying on the intermediate `W` function? In other terms, if we have already performed a backup in $s$, why couldn't we reuse it in the next backup? Doing so is actually called **Gauss-Seidl Value Iteration** and it opens the door to a much wider class of algorithms called **Asynchronous Value Iteration**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254271e0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Using `BBV`, write Gauss-Seidl Value Iteration.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4154d8e",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "\n",
    "Value Iteration:<br>\n",
    "```\n",
    "V(s) = Vinit(s) for all s\n",
    "while error>epsilon\n",
    "  W=V\n",
    "  for s in S\n",
    "    for a in A\n",
    "      V(s) = BBV(V,s)\n",
    "  error = norm(W-V)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a47274",
   "metadata": {},
   "source": [
    "It is crucial to note that in Gauss-Seidl Value Iteration, the order in which the states are considered for backups greatly affects of rewards are propagated through the state space and how the sequence of value functions converges to $V^*$.  \n",
    "\n",
    "But still, in Gauss-Seidl Value Iteration, states are updated once per sweep over the state space.\n",
    "Why wouldn't we update the value of some states more often than others? Would the overall value function still converge to $V^*$? A very powerful theorem actually states what follows.\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Convergence of Asynchronous Value Iteration**\n",
    "    \n",
    "As long as every state is visited infinitely often by the `V(s)` $\\leftarrow$ `BBV(V,s)` operation as time tends to $+\\infty$, the value function $V$ converges to $V^*$\n",
    "</div>\n",
    "\n",
    "Consequently, we could pick states totally randomly in order to perform Bellman backups on $V$, and $V$ would still converge to $V^*$. Although picking states randomly for that purpose seems like a bad idea, identifying a good ordering for the backups can lead to drastic improvements in convergence speed. This is the key idea of **Asynchronous Value Iteration** and has justified (among other things) the popular **[Prioritized Sweeping](https://link.springer.com/article/10.1007/BF00993104)** and **[Real-Time Dynamic Programming](https://www.sciencedirect.com/science/article/pii/000437029400011O)** algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd5c793",
   "metadata": {},
   "source": [
    "Let's now take the pseudo-code of Modified Policy Iteration from the exercises above, using `BBQ` and `BBpi`. The evaluation step and the improvement step are clearly separated. But we know already that if we require the evaluation step to have infinite precision, $m$ needs to tend to $\\infty$. We also know that if we take an arbitrary value for $m$ and the algorithm still converges.\n",
    "\n",
    "Can we introduce the idea of asynchronous Bellman backups in Policy Iteration? As in the value iteration case, can we update the value or policy of a given state in any ordering? Our most general theorem for Asynchronous Dynamic Programming in MDP states the following.\n",
    "<div class=\"alert alert-success\"> \n",
    "    \n",
    "**Convergence of Asynchronous Policy Iteration**\n",
    "    \n",
    "As long as every state is visited infinitely often by the `Q(s,a)` $\\leftarrow$ `BBQ(Q,pi,s,a)`  and the `pi(s)` $\\leftarrow$ `BBpi(Q,s)` operations as time tends to $+\\infty$, the value function $Q$ and the policy $\\pi$ converge respectively to $Q^*$ and $\\pi^*$\n",
    "</div>\n",
    "\n",
    "That is the most general framework one can give for **Asynchronous Dynamic Programming** in MDP resolution. It is often called **Asynchronous Policy Iteration**.<br>\n",
    "\n",
    "Overall, Asynchronous Policy Iteration can be written:  \n",
    "```\n",
    "Do forever:\n",
    "  Pick a set SAset={(s,a)}\n",
    "  For s,a in SAset:\n",
    "    Q(s,a) = BBQ(Q,pi,s,a)\n",
    "  Pick a set Sset={s}:\n",
    "  For s in Sset:\n",
    "    pi(s) = BBpi(Q,s)\n",
    "```\n",
    "\n",
    "So Asynchronous Policy Iteration encompasses all the previous algorithms, both synchronous (VI, PI, MPI) and asynchronous (Asynchronous VI).\n",
    "\n",
    "Of course, just as for Asynchronous Value Iteration, the most important thing with Asynchronous Policy Iteration is the order in which we pick the states and actions for backups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b07349-d6c5-418f-b4dc-4498533e3f36",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "TODO add a series of exercises here to implement Prioritized Sweeping.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
