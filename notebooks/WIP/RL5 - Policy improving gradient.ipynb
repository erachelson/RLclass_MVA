{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc11419b-0ffe-4beb-854c-fd0336c1eeb6",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://erachelson.github.io/RLclass_MVA/\">https://erachelson.github.io/RLclass_MVA/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fbc2eb-57c5-4e65-abbe-87dc0bc787d7",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Chapter 5: Policy improving gradients</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea818474-994d-4898-a8a0-4dff51c899af",
   "metadata": {},
   "source": [
    "So far, we have been mostly concerned with the problem of function approximation, that is \"how do we store $Q(s,a)$ in a convenient way, that is amenable to learning, retrieval and optimization?\". A specific feature of the problems we tackled was that they had few discrete actions, making the dependency on $s$ the key difficulty. In particular, when looking for a $Q$-greedy action, the $\\max_{a\\in A}$ problem had a straightforward solution as we could iterate through actions and retain the best one. We now turn to a more general case where the actions are too numerous to be enumerated (either because the action space is continuous or because it just has too many actions). Too many states motivated the introduction of function approximators for $V$ and $Q$; too many actions similarly lead to function approximation for $\\pi$. In the present chapter, we consider the general policy space (with possible approximation) and ask the question \"how do we find a monotonically improving sequence of policies?\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93c3ea2-ff63-4a30-8a8b-e7a00122f88d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Learning outcomes**   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fdf3f9-2a70-4acd-b0d0-be5dd980fe49",
   "metadata": {},
   "source": [
    "# Policy gradient methods\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Bottomline question:**   \n",
    "The previous chapters have focussed on *action-value methods*; they aimed at estimating $Q^*$ in order to deduce $\\pi^*$, or they jointly optimized $Q$ and $\\pi$. Could we directly optimize $\\pi$?\n",
    "</div>\n",
    "\n",
    "Suppose we have a policy $\\pi_\\theta$ parameterized by a vector $\\theta$. Our goal is to find the parameter $\\theta^*$ corresponding to $\\pi^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc535ba-951e-4c7a-ae5d-ba6ab684b299",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Recall the FrozenLake environment.  \n",
    "How many states and how many actions were there in this environment? \n",
    "What would be a policy parameterization which does not make any approximation (ie. that can represent any policy in the policy space) for stationary, memoryless, stochastic policies?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472994ab-5894-4a98-89bb-5c00bff2b430",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "There are 16 states and 4 actions in the FrozenLake game.\n",
    "\n",
    "A stationary, memoryless, stochastic policy is a mapping from $S$ to $\\Delta_A$. Since $S$ and $A$ are discrete, it can be represented in tabular form as:\n",
    "$$\\pi = \\left[ \\begin{array}{cccc}\n",
    "\\pi(a_0|s_0) & \\pi(a_1|s_0) & \\pi(a_2|s_0) & 1 - \\sum_{i=0}^2 \\pi(a_i|s_0) \\\\\n",
    "\\ldots & & & \\\\\n",
    "\\pi(a_0|s_{|S|}) & \\pi(a_1|s_{|S|}) & \\pi(a_2|s_{|S|}) & 1 - \\sum_{i=0}^2 \\pi(a_i|s_{|S|})\n",
    "\\end{array} \\right].$$\n",
    "\n",
    "This parameterization enables representing any stochastic policy. It involves $|A|-1=3$ parameters per line, and $|S|=16$ lines, so in total $3\\times 16 = 58$ parameters.\n",
    "\n",
    "As previously, parameterization does not necessarily involve approximation! As the action set will become large or continuous, parameterization will enable generalization across actions at the cost of approximation, but a tabular representation is also a policy parameterization.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77337f57-75e3-43c3-b036-1edb73dcde69",
   "metadata": {},
   "source": [
    "Remarks:\n",
    "- $\\pi_\\theta$ might not be able to represent $\\pi^*$. We will take a shortcut and call $\\pi^*$ the best policy among the $\\pi_\\theta$ ones.\n",
    "- For discrete state and action spaces, the tabular policy representation is a special case of policy parameterization.\n",
    "- Policy parameterization is a (possibly useful) way of introducing prior knowledge on the set of the desired policies.\n",
    "- The optimal deterministic policies might not belong to the policy subspace of $\\pi_\\theta$, thus it makes sense to consider stochastic policies for $\\pi_\\theta$.\n",
    "- It makes even more sense to consider stochastic policies that it opens the family of environments that we can tackle, like partially observable MDPs or multi-player games.\n",
    "\n",
    "For stochastic policies, we shall write $\\pi_\\theta(a|s)$.\n",
    "\n",
    "In the remainder of the chapter, we will assume that $\\pi_\\theta(a|s)$ is differentiable with respect to $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0164591c-c6ea-4766-80b5-838c76a93d52",
   "metadata": {},
   "source": [
    "Suppose now we define some performance metric $J(\\pi_\\theta) = J(\\theta)$. If $J$ is differentiable and a stochastic estimate $\\tilde{\\nabla}_\\theta J(\\theta)$ of the gradient is available, then we can define the stochastic gradient ascent update procedure:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\tilde{\\nabla}_\\theta J(\\theta).$$\n",
    "\n",
    "We will call **policy gradient methods** all methods that follow such a procedure (whether or not they also learn a value function or not).\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Policy gradient method**   \n",
    "We call **policy gradient method** any method that performs stochastic gradient ascent on the policy's parameters.  \n",
    "Given a stochastic estimate $\\tilde{\\nabla}_\\theta J(\\theta)$ of a policy's performance criterion with respect to the policy's parameters, such a method implements the update procedure: \n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\tilde{\\nabla}_\\theta J(\\theta).$$\n",
    "</div>\n",
    "\n",
    "Remarks: \n",
    "- Note that $J$ is a generic criterion. For example, $J$ could be defined as the $\\gamma$-discounted value of a starting state (or a distribution of starting states), or as the undiscounted reward over a certain horizon, or as the average reward.\n",
    "- Note that this family of methods can use any gradient estimate for $\\tilde{\\nabla}_\\theta J(\\theta)$: formal calculus, finite differences, automated differentiation, evolution strategies, etc.\n",
    "- Why is it interesting to look at methods which explicitly store a policy function? Because the evaluation of the policy in a given state $s$ does not require the maximization step ($\\max_a Q(s,a)$), which might be computationally costly, especially for continuous actions. Instead, it replaces it with a call to $\\pi_\\theta(s)$ (or a draw from $\\pi_\\theta(a|s)$). This argument makes actor-critic architectures or direct policy search a method of choice for continuous actions domains (especially common in Robotics) and Policy Gradient is one of them.\n",
    "- When do policy gradient approaches outperform value-based ones? It's hard to give a precise criterion; it really depends on the problem. One thing that comes into play is how easy it is to approximate the optimal policy or the optimal value function. If one is simpler than the other (by \"simpler\", we mean \"it is easier to find a parameterization whose spanned function space almost includes the function to approximate\"), then it is a good heuristic to try to approximate it. But this criterion might itself be hard to assess."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a90f48-67fc-44c5-82c5-3fbf56199aaa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "From the class on Markov Decision Processes, can you recall a scalar criterion $J(\\pi)$ whose optimization is provably equivalent to finding a policy that dominates any other one in every state?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f56515-4ab6-4475-a249-fb32a6e11542",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Provided $\\rho_0$ has non-zero probability mass on all states, an optimal policy is a solution to $\\max_\\pi J(\\pi) = \\mathbb{E}_{s_0\\sim \\rho_0}[V^\\pi(s_0)]$.\n",
    "\n",
    "This makes optimizing $J(\\pi)$ a legitimate goal for finding optimal policies: from now on we will work with $J(\\pi)$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d122f745-2e58-42eb-a124-efa9a0d767f9",
   "metadata": {},
   "source": [
    "**Notations**\n",
    "\n",
    "- We consider probability density functions $p(X)$ for all random variables $X$.\n",
    "- For a policy $\\pi_\\theta$ and a random variable $X$ we write indifferently $p(X|\\pi_\\theta) = p(X|\\theta)$.\n",
    "- We will write $\\pi_\\theta(s)$ the policy's distribution over actions in $s$, and $\\pi_\\theta(a|s)$ the probability that this policy picks action $a$ in $s$.\n",
    "- A trajectory is noted $\\tau = (s_t,a_t)_{t\\in \\mathbb{N}}$.\n",
    "- The state random variable at step $t$ is $S_t$ and its law's density is $p_t(s)$.\n",
    "- The action random variable at step $t$ is $A_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa3d439-2015-4152-ac30-0689aaaa950f",
   "metadata": {},
   "source": [
    "# The policy gradient theorem\n",
    "\n",
    "In this section, we derive our first key result in this class: can we obtain a usable expression for $\\nabla_\\theta J(\\theta)$, so that we can take gradient steps which improve the current policy (hence the name of this whole chapter: policy improving gradients)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0b58f2-29d7-4d0f-83c7-57cf779b55da",
   "metadata": {},
   "source": [
    "## A Bellman equation on value gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14639488-de67-4fc9-bbc4-da0a2b097bc3",
   "metadata": {},
   "source": [
    "As indicated in the previous section, we want to optimize the scalar criterion \n",
    "$$J(\\theta) = \\mathbb{E}_{s_0\\sim\\rho_0} [V^{\\pi_\\theta}(s_0)].$$\n",
    "\n",
    "So, quite immediately, \n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s_0\\sim\\rho_0} [\\nabla_\\theta V^{\\pi_\\theta}(s_0)].$$\n",
    "\n",
    "Let us look a little into $\\nabla_\\theta V^{\\pi_\\theta}(s_0)$. But first, let's simplify our notations so that the reasoning appears more clearly. We will drop the $\\theta$ subscripts almost everywhere ($\\nabla$ stands for $\\nabla_\\theta$ and $\\pi$ stands for $\\pi_\\theta$). Thus we have:\n",
    "$$J(\\theta) = \\mathbb{E}_{s_0\\sim\\rho_0} [V^{\\pi_\\theta}(s_0)] = \\rho_0 V^\\pi.$$\n",
    "\n",
    "And hence: \n",
    "$$\\nabla_\\theta J(\\theta) = \\rho_0 \\nabla V^{\\pi}.$$\n",
    "\n",
    "Similarly, we will write $V^\\pi(s) = \\mathbb{E}_{a\\sim \\pi} [Q(s,a)] = \\pi Q^\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87c9026-2541-47f1-9807-fd5b0e230548",
   "metadata": {},
   "source": [
    "As indicated in the previous section, we want to optimize the scalar criterion \n",
    "$$J(\\theta) = \\mathbb{E}_{s_0\\sim\\rho_0} [V^{\\pi_\\theta}(s_0)].$$\n",
    "\n",
    "So, quite immediately, \n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s_0\\sim\\rho_0} [\\nabla_\\theta V^{\\pi_\\theta}(s_0)].$$\n",
    "\n",
    "Let us look a little into $\\nabla_\\theta V^{\\pi_\\theta}(s_0)$. We have \n",
    "$$V^\\pi(s) = \\mathbb{E}_{a\\sim \\pi} [Q^\\pi(s,a)] = \\int_A Q^\\pi(s,a) \\pi(a|s) da.$$\n",
    "\n",
    "So \n",
    "$$\\nabla_\\theta V^\\pi(s) = \\int_A \\Big[Q^\\pi(s,a) \\nabla_\\theta \\pi(a|s) + \\pi(a|s) \\nabla_\\theta Q^\\pi(s,a)\\Big] da.$$\n",
    "\n",
    "Now, using the definition of $Q^\\pi$, \n",
    "$$\\nabla_\\theta V^\\pi(s) = \\int_A \\Big[Q^\\pi(s,a) \\nabla_\\theta \\pi(a|s) + \\pi(a|s) \\nabla_\\theta \\int_S (r(s,a,s') + \\gamma V^\\pi(s')) p(s'|s,a) ds'\\Big] da.$$\n",
    "\n",
    "Quite obviously, $\\nabla_\\theta r(s,a,s') = 0$. So we obtain\n",
    "$$\\nabla_\\theta V^\\pi(s) = \\int_A \\Big[Q^\\pi(s,a) \\nabla_\\theta \\pi(a|s) +  \\gamma \\pi(a|s) \\nabla_\\theta \\int_S V^\\pi(s') p(s'|s,a) ds'\\Big] da.$$\n",
    "\n",
    "We can split this in two parts, and switch the integration order in the second term. This yields\n",
    "$$\\nabla_\\theta V^\\pi(s) = \\int_A Q^\\pi(s,a) \\nabla_\\theta \\pi(a|s) da +  \\gamma  \\int_S \\nabla_\\theta V^\\pi(s') \\Big[\\int_A \\pi(a|s) p(s'|s,a) da \\Big] ds'.$$\n",
    "\n",
    "Let us write $p^\\pi(s'|s) = \\int_A \\pi(a|s) p(s'|s,a) da$. It is the transition kernel of the Markov chain defined by the MDP controled by $\\$\\pi$. We have\n",
    "$$\\nabla_\\theta V^\\pi(s) = \\int_A Q^\\pi(s,a) \\nabla_\\theta \\pi(a|s) da +  \\gamma  \\int_S \\nabla_\\theta V^\\pi(s') p^\\pi(s'|s) ds'.$$\n",
    "\n",
    "Swithcing back to expectations, we have:\n",
    "$$\\nabla_\\theta V^\\pi(s) = \\mathbb{E}_{a\\sim \\pi(a|s)} [Q^\\pi(s,a) \\nabla_\\theta \\pi(a|s)] + \\gamma \\mathbb{E}_{s'\\sim p^\\pi(s'|s)} [\\nabla_\\theta V^\\pi(s')].$$\n",
    "\n",
    "This is a Bellman equation on $\\nabla_\\theta V^\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cbc531-aad1-4ce4-990d-a45fa1384bc0",
   "metadata": {},
   "source": [
    "## The policy gradient theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f8f452-8c79-4ac2-8396-cc661856523a",
   "metadata": {},
   "source": [
    "## The link with $Q$-greedy actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03ac2aa-da8f-4637-9c59-eabe188a975a",
   "metadata": {},
   "source": [
    "## Deterministic policies: a limit case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c557b4-8698-444f-9b3b-fb3b07c6d310",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a202f-a22e-4b2f-838a-7f80fc687fb0",
   "metadata": {},
   "source": [
    "# A roll-out based view on policy gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab538eb-26ea-4d1d-bd88-f5774501fdbd",
   "metadata": {},
   "source": [
    "## A Monte-Carlo policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfac6dd-c3c7-4e72-893b-77ebbda6e771",
   "metadata": {},
   "source": [
    "## REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5098bc-4934-4b4e-a3b7-31fd225a2340",
   "metadata": {},
   "source": [
    "# Actor-critic algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e6a56-f2aa-4fbe-88ad-1df11c701ad0",
   "metadata": {},
   "source": [
    "## Introducing a critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4885dd7-8a76-4600-9a8f-27190d6ea1d3",
   "metadata": {},
   "source": [
    "## Baselines in policy gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee54310-ff34-4610-879c-dcd6fc3d683a",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1acf267-03ce-475a-86e7-2ca46999cf01",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e4ea2-e29b-4763-a58e-642fdb655fb5",
   "metadata": {},
   "source": [
    "- DDPG\n",
    "- TD3\n",
    "- SAC\n",
    "- A2C\n",
    "- Running rollouts in parallel\n",
    "- PG on continuous action domains\n",
    "- PG for the finite horizon criterion\n",
    "- GAE\n",
    "- From off-policy PG to TRPO\n",
    "- PPO\n",
    "- Gradient free policy search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
