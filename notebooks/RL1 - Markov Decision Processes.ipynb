{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b6229bf",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://erachelson.github.io/RLclass_MVA/\">https://erachelson.github.io.github.io/RLclass_MVA/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72b7186",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Chapter 1: Modeling sequential decision problems with Markov Decision Processes</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b1870",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Learning outcomes:**  \n",
    "This chapter is mostly about definitions. By the end of this chapter you should be able to define and discuss what is:\n",
    "- a Markov decision process,\n",
    "- an action policy,\n",
    "- a value function,\n",
    "- an optimal policy / value function.\n",
    "\n",
    "Additionally, after doing the homework, you should be able to define and discuss:\n",
    "- Monte Carlo estimation of values,\n",
    "- the stationary distribution of a Markov decision process,\n",
    "- the state occupancy measure of a policy given a starting distribution,\n",
    "- the horizon under a $\\gamma$-discounted criterion.\n",
    "\n",
    "You should also be familiar with the Gymnasium API.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b29ed",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Definition\n",
    "\n",
    "Let's take a higher view and develop a general theory for describing problems such as writing a prescription for the patient we considered in the previous class.\n",
    "\n",
    "Let us assume we have:\n",
    "- a set of states $S$ describing the system to control,\n",
    "- a set of actions $A$ we can apply.\n",
    "\n",
    "Curing patients is a conceptually difficult task. \n",
    "To keep things grounded, we shall use a toy example called [FrozenLake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) and work our way to more general concepts. It's also the occasion to familiarize with [Gymnasium](https://gymnasium.farama.org/).\n",
    "\n",
    "<center><img src=\"img/frozenlake.jpg\" style=\"height: 300px;\"></img></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0d24023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\x1b[41mS\\x1b[0mFFF\\nFHFH\\nFFFH\\nHFFG\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "# use render_mode=\"human\" to open the game window\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "env.reset()\n",
    "print(env.render())\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adcfbcd",
   "metadata": {},
   "source": [
    "falling into a hole: not penalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63dd3f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this only if you have used render_mode=\"human\" in the cell above\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373e826a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The game's goal is to navigate across this lake, from position S to position G, while avoiding falling into the holes H. Frozen positions are slippery so you don't always move in the intended direction. Reaching the goal provides a reward of 1, and zero otherwise. Falling into a hole or reaching the goal ends an episode.\n",
    "\n",
    "A more complete description of the environment is provided in [Gymnasium's documentation](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) (or through `help(fl.FrozenLakeEnv)`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490f722a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "How many states are there in this game?  \n",
    "How many actions?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6009d0a",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "States set: the 16 positions on the map.  \n",
    "Actions set: the 4 actions $\\{$N,S,E,W$\\}$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5008a9cd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's confirm that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28c3b5e9",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e508cc4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At every time step, the system state is $S_t$ and we decide to apply action $A_t$. This results in observing a new state $S_{t+1}$ and receiving a scalar reward signal $R_t$ for this transition.\n",
    "\n",
    "$R_t$ tells us how happy we are with the last transition.\n",
    "\n",
    "For example, in FrozenLake, all transitions have reward 0 except for the one that reaches the goal, which yields reward 1. Let's verify this in the code and introduce a few utility functions on the way.\n",
    "\n",
    "Note that $S_t$, $A_t$, $S_{t+1}$ and $R_t$ are random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d182e7-3a95-4252-b43d-2b873b93ee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL1_utility_functions.py\n",
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "\n",
    "actions = {fl.LEFT: '\\u2190', fl.DOWN: '\\u2193', fl.RIGHT: '\\u2192', fl.UP: '\\u2191'}\n",
    "\n",
    "def to_s(row,col):\n",
    "    env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "    return row*env.unwrapped.ncol+col\n",
    "\n",
    "def to_row_col(s):\n",
    "    env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "    col = s%env.unwrapped.ncol\n",
    "    row = int((s-col)/env.unwrapped.ncol)\n",
    "    return row,col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8baaf6bd",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '←', 1: '↓', 2: '→', 3: '↑'}\n",
      "Apply → from (3, 2):\n",
      "  Reach ((3, 2)) and get reward 0.0 with proba 0.3333333333333333.\n",
      "  Reach ((3, 3)) and get reward 1.0 with proba 0.3333333333333333.\n",
      "  Reach ((2, 2)) and get reward 0.0 with proba 0.3333333333333333.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "\n",
    "print(actions)\n",
    "row=3\n",
    "col=2\n",
    "a=2\n",
    "print(\"Apply \", actions[2], \" from (\", row, \", \", col, \"):\", sep='')\n",
    "for tr in env.unwrapped.P[to_s(row,col)][a]:\n",
    "    print(\"  Reach (\", to_row_col(tr[1]), \") and get reward \", tr[2], \" with proba \", tr[0], \".\", sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "214f45a9-5845-46ca-b417-cb88951df080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 14, 0.0, False),\n",
       " (0.3333333333333333, 15, 1.0, True),\n",
       " (0.3333333333333333, 10, 0.0, False)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.P[to_s(row,col)][a]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd02440",
   "metadata": {},
   "source": [
    "Chess: 2 player games: control problem, one player game: not deterministic, model your opponents\n",
    "two players stochastic games: break many hypotheses\n",
    "go two players games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "578a818a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 10, 0.0, False),\n",
       "  (0.3333333333333333, 13, 0.0, False),\n",
       "  (0.3333333333333333, 14, 0.0, False)],\n",
       " 1: [(0.3333333333333333, 13, 0.0, False),\n",
       "  (0.3333333333333333, 14, 0.0, False),\n",
       "  (0.3333333333333333, 15, 1.0, True)],\n",
       " 2: [(0.3333333333333333, 14, 0.0, False),\n",
       "  (0.3333333333333333, 15, 1.0, True),\n",
       "  (0.3333333333333333, 10, 0.0, False)],\n",
       " 3: [(0.3333333333333333, 15, 1.0, True),\n",
       "  (0.3333333333333333, 10, 0.0, False),\n",
       "  (0.3333333333333333, 13, 0.0, False)]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.P[to_s(row,col)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d63792",
   "metadata": {},
   "source": [
    "first order dynamic system: markov property\n",
    "discrete time of a stochastic first order differential equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e3c21",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Policies\n",
    "\n",
    "Formally, how does one write the behavior of an agent?\n",
    "This behavior specifies how to choose actions at each time step:\n",
    "$$A_t \\sim \\pi_t.$$\n",
    "\n",
    "Let $\\Delta_A$ be the set of probability measures on the action space $A$. Then the law $\\pi_t$ of $A_t$ belongs to $\\Delta_A$. \n",
    "$\\pi_t$ is called the **decision rule** at step $t$, it is a distribution over the action space $A$.  \n",
    "The collection $\\pi = \\left(\\pi_t \\right)_{t\\in T}$ is the oracle's **policy**.\n",
    "\n",
    "<div class=\"alert alert-success\"><b>Policy $\\pi$</b><br>\n",
    "A policy $\\pi$ is a sequence of decision rules $\\pi_t$: $\\pi = \\{\\pi_t\\}_{t\\in T}$, with $\\pi_t \\in \\Delta_A$.\n",
    "</div>\n",
    "\n",
    "One policy implies one specific distribution over trajectories over the frozen lake. More generally, the policy and $S_0$ condition the sequence $S_0, A_0, R_0, S_1, A_1, R_1, \\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bece33be",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Value of a trajectory / of a policy\n",
    "\n",
    "In FrozenLake as in the patient's example, some trajectories are better than others. We need a criterion to compare trajectories. Intuitively, this criterion should reflect the idea that a good policy accumulates as much reward as possible along a trajectory.\n",
    "\n",
    "Let's compare the policy that always moves to the right and the policy that always moves left by summing the rewards obtained along trajectories and then averaging these rewards across trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "418f3558",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est. value of 'right' policy: 0.03108 std dev: 0.17353395517880643\n",
      "est. value of 'left'  policy: 0.0 std dev: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "nb_episodes = 50000\n",
    "horizon = 200\n",
    "\n",
    "Vright = np.zeros(nb_episodes)\n",
    "for i in range(nb_episodes):\n",
    "    env.reset()\n",
    "    for t in range(horizon):\n",
    "        next_state, r, done, trunc, _ = env.step(fl.RIGHT)\n",
    "        Vright[i] += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "Vleft  = np.zeros(nb_episodes)\n",
    "for i in range(nb_episodes):\n",
    "    env.reset()\n",
    "    for t in range(horizon):\n",
    "        next_state, r, done, trunc, _ = env.step(fl.LEFT)\n",
    "        Vleft[i] += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print(\"est. value of 'right' policy:\", np.mean(Vright), \"std dev:\", np.std(Vright))\n",
    "print(\"est. value of 'left'  policy:\", np.mean(Vleft),  \"std dev:\", np.std(Vleft))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b7a8b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the general case, this sum of rewards on an infinite horizon might be unbounded. So let us introduce the **$\\gamma$-discounted sum of rewards** (from a starting state $s$, under policy $\\pi$) random variable:\n",
    "\n",
    "$$G^\\pi(s) = \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s,\\\\ A_t \\sim \\pi_t,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1}).\\end{array}$$\n",
    "\n",
    "$G^\\pi(s)$ represents what we can gain in the long-term by applying the actions from $\\pi$.\n",
    "\n",
    "Then, given a starting state $s$, we can define the value of $s$ under policy $\\pi$:\n",
    "$$V^\\pi(s) = \\mathbb{E} \\left[ G^\\pi(s) \\right]$$\n",
    "\n",
    "This defines the value function $V^\\pi$ of policy $\\pi$:\n",
    "\n",
    "<div class=\"alert alert-success\"><b>\n",
    "\n",
    "Value function $V^\\pi$ of a policy $\\pi$ under a $\\gamma$-discounted criterion</b><br>\n",
    "$$V^\\pi : \\left\\{\\begin{array}{ccl}\n",
    "S & \\rightarrow & \\mathbb{R}\\\\\n",
    "s & \\mapsto & V^\\pi(s)=\\mathbb{E}\\left( \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\bigg| S_0 = s, \\pi \\right)\\end{array}\\right. $$\n",
    "\n",
    "</div>\n",
    "\n",
    "Finite horizon:\n",
    "   $V(s) = \\mathbb{E}\\left( \\sum\\limits_{t = 0}^H R_t \\bigg| s_0 = s \\right)$   with $H \\in \\mathbb{N}$\n",
    "\n",
    "Average reward:\n",
    "   $V(s) = \\mathbb{E}\\left( \\lim\\limits_{H\\rightarrow\\infty}  \\frac{1}{H} \\sum\\limits_{t = 0}^H R_t \\bigg| s_0 = s \\right)$\n",
    "\n",
    "Total reward:\n",
    "   $V(s) = \\mathbb{E}\\left( \\lim\\limits_{H\\rightarrow\\infty} \\sum\\limits_{t = 0}^H R_t \\bigg| s_0 = s \\right)$\n",
    "\n",
    "Discounted reward:\n",
    "   $V(s) = \\mathbb{E}\\left( \\lim\\limits_{H\\rightarrow\\infty} \\sum\\limits_{t = 0}^H \\gamma^t R_t \\bigg| s_0 = s \\right)$   with $0 \\leq \\gamma < 1$\n",
    "\n",
    "\n",
    "And, given a distribution $\\rho_0$ on starting states, we can map $\\pi$ to the scalar value:\n",
    "$$J(\\pi) = \\mathbb{E}_{s \\sim \\rho_0} \\left[ V^\\pi(s) \\right]$$\n",
    "\n",
    "Note that this definition is quite arbitrary: instead of the expected (discounted) sum of rewards, we could have taken the average reward over all time steps, or some other (more or less exotic) comparison criterion between policies. For example:\n",
    "\n",
    "\n",
    "- The average reward criterion characterizes the average reward per time step the agent gets. This can be useful in some control applications. However, in the case of FrozenLake, we don't want to average our rewards, we want to get to the goal as soon as possible.\n",
    "- The total reward criterion seems more adapted: it maximizes the cumulated rewards obtained during an episode. But it does not discriminate whether they were obtained at the beginning or late in the episode. Additionally, it suffers from a major flaw: for infinite horizon problems, even if the reward model is bounded, this sum might diverge. So we need a better formulation for the general case of infinite horizon problems.\n",
    "- The discounted reward criterion suits our needs. The discount factor ($0\\leq \\gamma<1$) guarantees that with bounded reward models $r$, the sum always converges. Also it has the properties we desire: a reward of 1 obtained at the first time step weights 1 in the final criterion, while a reward of 1 obtained after $t$ time steps only weights $\\gamma^t$; it is *discounted* by $\\gamma^t$ (hence the criterion's name).\n",
    "\n",
    "Most of the RL literature uses this discounted criterion (in some cases with $\\gamma=1$), some works use a finite horizon criterion, some use the average reward criterion, and few works venture into more exotic criteria. For now, we will limit ourselves to the discounted criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ab534",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Optimal policies\n",
    "\n",
    "The fog clears up a bit: we can now compare policies given an initial state (or initial state distribution).  \n",
    "\n",
    "Thus, an **optimal** policy is one that is better than any other.\n",
    "\n",
    "<div class=\"alert alert-success\"><b>Optimal policy $\\pi^*$</b><br>\n",
    "\n",
    "$\\pi^*$ is said to be optimal iff $\\pi^* \\in \\arg\\max\\limits_{\\pi} V^\\pi$.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "    \n",
    "A policy is optimal if it **dominates** over any other policy in every state:\n",
    "$$\\pi^* \\textrm{ is optimal}\\Leftrightarrow \\forall s\\in S, \\ \\forall \\pi, \\ V^{\\pi^*}(s) \\geq V^\\pi(s)$$\n",
    "</div>\n",
    "\n",
    "Note that although there may be several optimal policies, they all share the same value function $V^* = V^{\\pi^*}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8637d7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now get to our first fundamental result. Fortunately for us...  \n",
    "\n",
    "<div class=\"alert alert-success\"><b>Theorem: family of optimal policies</b><br>\n",
    "\n",
    "For $\\left\\{\\begin{array}{l}\n",
    "\\gamma\\textrm{-discounted criterion}\\\\\n",
    "\\textrm{infinite horizon}\n",
    "\\end{array}\\right.$, \n",
    "there always exists at least one optimal stationary, deterministic, memoryless policy.\n",
    "</div>\n",
    "\n",
    "Let's explain a little:\n",
    "- Memoryless (also called Markovian): all decision rules are only conditioned by the last seen state. Mathematically: \n",
    "$\\left.\\begin{array}{l}\n",
    "\\forall \\left(s_i,a_i\\right)_{i\\leq t-1}\\in \\left(S\\times A\\right)^{t-1}\\\\\n",
    "\\forall \\left(s'_i,a'_i\\right)_{i\\leq t-1}\\in \\left(S\\times A\\right)^{t-1}\\\\\n",
    "\\forall s \\in S\n",
    "\\end{array}\\right\\}, \\pi_t\\left(A_t|S_0=s_0, A_0=a_0, \\ldots, S_t=s\\right) = \\pi_t\\left(A_t|S'_0=s'_0, A'_0=a'_0, \\ldots, S_t=s\\right)$.  \n",
    "One writes $\\pi_t(A_t|S_t=s)$, or more simply $\\pi_t(\\cdot | s)$ or $\\pi_t(s) \\in \\Delta_A$.\n",
    "- Stationary (and memoryless): all decision rules are the same throughout time. Mathematically:  \n",
    "$\\forall (t,t')\\in \\mathbb{N}^2, \\pi_t(A_t|S_t=s) = \\pi_t(A_{t'}|S_{t'}=s)$.  \n",
    "This unique distribution is written $\\pi(\\cdot | s) = \\pi_t( \\cdot | s)$ or $\\pi(s) \\in \\Delta_A$.\n",
    "- Deterministic: all decision rules put all probability mass on a single item of the action space $A$.  \n",
    "$\\pi_t(A_t|history) = \\left\\{\\begin{array}{l}\n",
    "1\\textrm{ for a single }a\\\\\n",
    "0\\textrm{ otherwise}\n",
    "\\end{array}\\right.$.\n",
    "\n",
    "So in simpler words, we know that among all possible optimal ways of picking $A_t$, at least one is a function $\\pi:S\\rightarrow A$.\n",
    "\n",
    "That helps a lot: we don't have to search for optimal policies in a complex family of history-dependent, stochastic, non-stationary policies; instead we can simply search for a function $\\pi(s)=a$ that maps states to actions.\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Proof sketch (click to expand)</b></summary>\n",
    "\n",
    "The proof (very simple but a little long) is in chapter 6 of the <b>Markov Decision Processes</b> book by Martin L. Puterman.<br>\n",
    "To give you the general flavour:\n",
    "<ul>\n",
    "<li> The infinite horizon leads to the existence of an optimal <b>stationary</b> policy: if the horizon is infinitely far, the optimal decision rule $n$ steps before the end if the same as the one $n+1$ steps before the end (watch out, this intuition can be very false in other contexts).\n",
    "<li> The <b>Markovian</b> property of $p(s'|s,a)$ allows to get optimal memoryless policies.\n",
    "<li> The <b>deterministic</b> part is somehow more tricky but just note that this result only holds for single-player MDPs. For a two-agents competitive game for example (like poker for instance), there is no deterministic optimal policy.\n",
    "</ul>\n",
    "</details>\n",
    "\n",
    "Remark: quite often, we will retain the memoryless and stationary properties of optimal policies but still search for stochastic ones. Why we do this is still a bit beyond the current notebook (it involves notions of exploration, sample distribution, and smoothness of the optimization landscape). For now, let us just remember we shall search for optimal policies under the form of functions from $S$ to either $A$ (deterministic policies, $\\pi(s)\\in A$) or $\\Delta_A$ (stochastic policies, $\\pi(s) \\in \\Delta_A$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c003f",
   "metadata": {},
   "source": [
    "# A weaker notion of optimality \n",
    "\n",
    "If all states have a non-zero probability of being visited by any policy, then an optimal policy should pick optimal actions along any trajectory starting in (any) $s_0$, so it should pick optimal actions in all states. Then finding a policy which maximize $V^\\pi$ in all states is actually the same as finding a policy which maximizes the expected value $V^\\pi(s_0)$ of a fixed initial state $s_0$, or the expected value over any distribution on initial states $\\mathbb{E}_{s_0\\sim \\rho_0}[V^\\pi(s_0)]$.\n",
    "\n",
    "To fix ideas, let's write $J(\\pi) = \\mathbb{E}_{s_0\\sim \\rho_0}[V^\\pi(s_0)]$. Then:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**The policy optimization problem:**  \n",
    "Provided all states are reachable from any other state, an optimal policy is a solution to $\\max_\\pi J(\\pi) = \\mathbb{E}_{s_0\\sim \\rho_0}[V^\\pi(s_0)]$.\n",
    "</div>\n",
    "\n",
    "So we turned a problem of value maximization in *every state* into one of value maximization *on average* across a distribution $\\rho_0$. Note that this distribution need not be a specific initial state distribution.\n",
    "\n",
    "The assumption that all states are reachable from any other one is very strong. When it is not verified (which will happen in many real-life cases), the optimization problem $\\max_\\pi J(\\pi) = \\mathbb{E}_{s_0\\sim \\rho_0}[V^\\pi(s_0)]$ might not yield a fully optimal policy. It will provide a policy which maximizes its expected outcome on average across states, on a specific set of starting states distributed according to $\\rho_0$.\n",
    "\n",
    "This provides us with a weaker definition of optimality, stating that we search for a policy which maximizes its expected gain, *on average across $\\rho_0$*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30cc496",
   "metadata": {},
   "source": [
    "# A cartography of optimization methods: important for the rest of the class\n",
    "\n",
    "The two notions of optimality introduced above pave the way to two different families of methods in RL. Either one searches for the optimal value function (strong optimality), and then derives an optimal policy as a by-product, or one directly optimizes for good average-value policies (weak optimality). In practice, no approach is more justified than the other and both deserve studying. This enables separating families of methods and drawing a map of future classes.\n",
    "\n",
    "Policy optimization, solve the $\\max_\\pi$ problem directly:\n",
    "- by derivative-free methods (evolutionary RL)\n",
    "- by derivative-based methods (policy gradient methods and their modern extensions)\n",
    "\n",
    "Value optimization, solve for $V^*$:\n",
    "- by (approximate) dynamic programming (and the vast span of most recent approximate value iteration algorithms)\n",
    "- by linear programming or alternate formulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3a2eee",
   "metadata": {},
   "source": [
    "# Limits of the MDP model\n",
    "\n",
    "What if the system is an MDP but its state is not fully observable?  \n",
    "$\\rightarrow$ This is the (exciting) field of Partially Observable MDPs. Our key result of having a Markovian optimal policy does not hold anymore. There are ways to still obtain optimal policies (but it is often very computationaly costly) or approximate them with Markovian policies.\n",
    "\n",
    "What happens if there are multiple actions taken at the same time by different agents?  \n",
    "$\\rightarrow$ This falls into the category of multi-player stochastic games. Such games can be adversarial, cooperative, or a mix of the two. Of course they can also have partial observability.\n",
    "\n",
    "What if the transition model is not Markovian?  \n",
    "$\\rightarrow$ Beware, here be dragons! All the beautiful framework above crumbles down if its hypothesis are violated. So great care should be taken when choosing the state variables for a given problem. In a sense, an MDP is a discrete time version of a first-order differential equation. Writing a system as $\\dot{X} = f(X,U, noise)$ as is common in control theory is a good practice to ensure the Markov property."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a9d286",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Summary\n",
    "\n",
    "Let's wrap this section up. Our goal was to formally define the search for the best strategy for our game of FrozenLake and the medical prescription problem. This has led us to formalizing the general **discrete-time stochastic optimal control problem**:\n",
    "- Environment (discrete time, non-deterministic, non-linear, Markov) $\\leftrightarrow$ MDP.\n",
    "- Behaviour $\\leftrightarrow$ control policy $\\pi : S\\rightarrow A$ or $\\Delta_A$.\n",
    "- Policy evaluation criterion $\\leftrightarrow$ $\\gamma$-discounted criterion.\n",
    "- Goal $\\leftrightarrow$ Maximize value function $V^\\pi(s)$.\n",
    "\n",
    "So we have built the first stage of our three-stage rocket:  \n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**What is the system to control?**  \n",
    "The system to control is a Markov Decision Process $\\langle S, A, p, r \\rangle$ and we will control it with a policy $\\pi:s\\mapsto a$ in order to optimize $\\mathbb{E} \\left( \\sum_t \\gamma^t R_t\\right)$\n",
    "</div>\n",
    "\n",
    "We can now move on to the next question: how does one find an optimal strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d6c24c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise** The limits of MDP modeling  \n",
    "Can these systems be modeled as MDPs?   \n",
    "- Playing a tennis video game based on a single video frame\n",
    "- Playing a tennis video game based on a full physical description of the ball and the players\n",
    "- The game of Poker\n",
    "- The collaborative game of [Hanabi](https://en.wikipedia.org/wiki/Hanabi_(card_game))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832095eb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "A single video frame does not contain enough information to accurately represent the current state of the game. The velocities are absent for instance. Hence the dynamics might not be Markovian.\n",
    "    \n",
    "A full physical description, however, may contain enough information so that $\\mathbb{P}(S_{t+1})$ is only conditioned by $S_t$ and $A_t$.\n",
    "    \n",
    "Poker is a two-player, adversarial, stochastic game. MDPs only model one-player games.\n",
    "\n",
    "Beyond the fact that it is a multi-player game. Hanabi is a game based mainly on epistemic reasoning. That is, reasoning on beliefs about the state of the world (specifically, the state of the other players' hand). This type of state description is difficult to encode within a Markovian dynamics model.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398d248f-4364-4640-b673-38b41cee2cf3",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3a500a",
   "metadata": {},
   "source": [
    "The exercises below are here to help you play with the concepts introduced above, to better grasp them. They also introduce additional important notions. They are not optional to reach the class goals. Often, the provided answer reaches out further than the plain question asked and provides comments, additional insights, or external references."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e5e95",
   "metadata": {},
   "source": [
    "## Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f0e01",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "In the text above, we wrote that $\\pi_t$ is the distribution over the action space $A$ for the action $A_t$ taken at time step $t$.  \n",
    "- Write this probability $\\mathbb{P}(A_t)$ as a conditional probability $\\pi_t(A_t|\\ldots)$ (the real question is: what are the \"$\\ldots$\"?).\n",
    "- Rephrase, with your own words, what this $\\pi_t(A_t|\\ldots)$ indicates.  \n",
    "Then we defined a policy $\\pi$ as the collection of decision rules $\\left( \\pi_t \\right)_{t\\in\\mathbb{N}}$.\n",
    "- Using the answer to the previous questions, write the definition of a Markovian policy, then a stationary Markovian policy (the answer is actually in the text just after the Optimal policy theorem, the exercise is about being able to recall and explain the definitions and what they imply). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9048755",
   "metadata": {},
   "source": [
    "- $\\pi_t(A_t|S_0, ..., S_t)$\n",
    "\n",
    "- it is the conditional probability of taking the action $A_t$ given the states/observations $(S_t)$\n",
    "\n",
    "- Markovian policy: only depends on $S_t$, stationary: only depends on "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee6eae",
   "metadata": {},
   "source": [
    "$pi_t$ also depends on previous ACTIONS!!\n",
    "\n",
    "Stationarity: same $pi$ at two different $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9190a8",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "- $\\pi_t$ describes the distribution over actions at time step $t$. Because of causality (future events don't affect current events), it can only depend on the realization of the state and actions random variables in previous time steps:\n",
    "$$\\mathbb{P}(A_t) = \\pi_t(A_t | S_0, A_0, \\ldots, S_{t-1}, A_{t-1}, S_t)$$\n",
    "We define the *history* $H_t = S_0, A_0, \\ldots, S_{t-1}, A_{t-1}, S_t$ at time step $t$ as this random sequence. So:\n",
    "$$\\mathbb{P}(A_t) = \\pi_t(A_t | H_t)$$\n",
    "\n",
    "- In plain words, for an action $a$ and a history $h$ at step $t$, $\\pi_t(a|h)$ indicates  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; the probability to pick action $a$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; at time $t$,  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; given the history of states/actions $h$.  \n",
    "This is called a *history-dependent, non-stationary, stochastic* policy and is the most generic class of policies.\n",
    "\n",
    "- In a Markovian policy, all decision rules are only conditioned by the last encountered state.\n",
    "$$\\pi_t(A_t|H_t) = \\pi_t(A_t | S_t)$$\n",
    "In other words, given two (possibly different) sequences of state-action random variables realizations up to time $t-1$ and a single realization of S_t the distribution of $A_t$ is the same.\n",
    "Mathematically: \n",
    "$\\left\\{\\begin{array}{l}\n",
    "\\forall \\left(s_i,a_i\\right)_{i\\leq t-1}\\in \\left(S\\times A\\right)^{t-1}\\\\\n",
    "\\forall \\left(s'_i,a'_i\\right)_{i\\leq t-1}\\in \\left(S\\times A\\right)^{t-1}\\\\\n",
    "\\forall s \\in S\n",
    "\\end{array}\\right.,$\n",
    "\\begin{align*}\n",
    "    \\pi_t(A_t|H_t) &= \\pi_t\\left(A_t|S_0=s_0, A_0=a_0, \\ldots, S_t=s\\right)\\\\\n",
    "    &= \\pi_t\\left(A_t|S'_0=s'_0, A'_0=a'_0, \\ldots, S_t=s\\right)\\\\\n",
    "    &= \\pi_t(A_t | S_t)\n",
    "\\end{align*}\n",
    "One writes $\\pi_t(A_t|S_t=s)$, or more simply $\\pi_t(\\cdot | s)$.  \n",
    "In a stationary Markovian policy, all decision rules are the same throughout time. Mathematically:  \n",
    "$\\forall (t,t')\\in \\mathbb{N}^2, \\pi_t(A_t|S_t=s) = \\pi_t(A_{t'}|S_{t'}=s)$.  \n",
    "This unique distribution is written $\\pi(\\cdot | s) = \\pi_t( \\cdot | s)$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c169b751",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "In the patient example, suppose the physician tells the patient to take drug A every day for 5 days, then drug B every two days for 9 days, then come back for a check-up. The physician adds to take drug C once a day if the patient feels pain over two consecutive days. Can you write the sequence of corresponding decision rules?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ff278a",
   "metadata": {},
   "source": [
    "$\\forall t \\in [0, 5], B_t= a$, $\\forall t \\in [6, 10], B_t= b$\n",
    "\n",
    "$A_t = B_t + \\delta_{pain}= B_t + \\delta_{S_t=pain, S_{t-1}=pain}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd563b3",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "This prescription is made over a finite horizon $H=14$ days. The actions are the combinations of drugs $A=\\left\\{ \\emptyset, (A), (B), (C), (A,B), (A,C), (B,C), (A,B,C) \\right\\}$.   \n",
    "    \n",
    "The prescription is deterministic: the distribution over actions is a Dirac. We will write it $a_t = \\pi_t(h)$.\n",
    "    \n",
    "The prescription depends on the two last states of the patient. So it's not Markovian, it is history-dependent. Precisely, it depends on the boolean state variable \"is there pain?\". So we can write $\\pi_t(h) = \\pi_t(s_t,s_{t-1})$.  \n",
    "  \n",
    "It also is not stationary, since the prescription changes after day 5.  \n",
    "    \n",
    "Consequently, the policy is:  \n",
    "For $t \\in [1, 5]$:   \n",
    "if $pain(s_t,s_{t-1})=True$, $\\pi_t(s_t,s_{t-1}) = (A,C)$,  \n",
    "if $pain(s_t,s_{t-1})=False$, $\\pi_t(s_t,s_{t-1}) = (A)$.  \n",
    "For $t \\in [6, 14]$:   \n",
    "if $t$ is even and $pain(s_t,s_{t-1})=True$, $\\pi_t(s_t,s_{t-1}) = (B,C)$,  \n",
    "if $t$ is even and $pain(s_t,s_{t-1})=False$,  $\\pi_t(s_t,s_{t-1}) = (B)$,  \n",
    "if $t$ is odd and $pain(s_t,s_{t-1})=True$, $\\pi_t(s_t,s_{t-1}) = (C)$,  \n",
    "if $t$ is odd and $pain(s_t,s_{t-1})=False$,  $\\pi_t(s_t,s_{t-1}) = \\emptyset$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dd2c36",
   "metadata": {},
   "source": [
    "## Monte Carlo value estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b0b20c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Write a `mc_eval(pi,nb_trials)` function that uses the FrozenLake environment we've introduced earlier to obtain a vector of length `nb_trials` containing the return realizations for `nb_trials` Monte-Carlo rollouts of policy `pi` (given as an array of action indices) starting in the initial state $s_0$. Take $\\gamma = 0.9$. Yes, the code is almost the same as the example provided earlier.  \n",
    "Run this function with the policy that always goes right and for 100000 trials.  \n",
    "Note that $\\gamma^{200} \\approx 10^{-9}$ so any reward obtained after 200 time steps will have a negligible contribution to $V^\\pi(s_0)$, thus rolling an episode out for 200 time steps should be sufficient.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4396e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code).\n",
    "\n",
    "import numpy as np\n",
    "def mc_eval(pi, nb_trials):\n",
    "    trials = np.zeros(nb_trials)\n",
    "    horizon = 200\n",
    "    gamma_cst = 0.9\n",
    "    for i in range(nb_trials):        \n",
    "        state, _ = env.reset()\n",
    "        reward = 0\n",
    "        first_state = env\n",
    "        for t in range(horizon):\n",
    "            state, r, done, trunc, _ = env.step(pi[state])\n",
    "            reward += gamma_cst**i * r            \n",
    "            if done:\n",
    "                break\n",
    "        trials[i] = reward\n",
    "    return trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38674483-2f41-430b-8a8d-fb8a11a8446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL1_exercise1.py\n",
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "\n",
    "def mc_eval(pi,nb_trials):\n",
    "    env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "    horizon = 200\n",
    "    gamma = 0.9\n",
    "    Vepisode = np.zeros(nb_trials)\n",
    "    for i in range(nb_trials):\n",
    "        env.reset()\n",
    "        for t in range(horizon):\n",
    "            next_state, r, done, _, _ = env.step(fl.RIGHT)\n",
    "            Vepisode[i] += gamma**t * r\n",
    "            if done:\n",
    "                break\n",
    "    return Vepisode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e0eb76",
   "metadata": {},
   "source": [
    "going to the rigth for every state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61b5f954-b940-418a-9e7d-a320124eb062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "value estimate: 0.013016378529937271\n",
      "return variance: 0.07540441788730998\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "pi = fl.RIGHT*np.ones((env.observation_space.n))\n",
    "print(pi)\n",
    "Vepisode = mc_eval(pi,100000)\n",
    "print(\"value estimate:\", np.mean(Vepisode))\n",
    "print(\"return variance:\", np.std(Vepisode))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d236b26d",
   "metadata": {},
   "source": [
    "## Markov strikes back: transition kernels and stationary distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f43625",
   "metadata": {},
   "source": [
    "In this section, we get back to basics on Markov chains and play a bit with core notions to acquire a better understanding of MDP properties.\n",
    "\n",
    "Let's consider a given finite state space MDP and a fixed policy. To set ideas, take the FrozenLake environment and the deterministic policy that always moves right. Let's initialize the MDP to a starting state $s_0$ drawn from a distribution $\\rho_0(s)$ and let's look at how the state evolves across time steps. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Use the MDP's transition model and the policy to define the transition probability $p^\\pi(s'|s)$ of reaching $s'$ from $s$.  \n",
    "What is $\\mathbb{P}(s_{t+1})$ given $p^\\pi$ and $\\mathbb{P}(s_t)$?  \n",
    "What is the state probability $\\mathbb{P}(s_{t+k})$ after $k$ transitions, given $p^\\pi$ and $\\mathbb{P}(s_t)$?\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "For a deterministic policy, \n",
    "$$p^\\pi(s'|s) = p(s'|s,\\pi(s)).$$ \n",
    "\n",
    "For a stochastic policy, \n",
    "$$p^\\pi(s'|s) = \\mathbb{E}_{a\\sim \\pi(\\cdot | s)} [ p(s'|s,a) ] = \\sum_{a\\in A} p(s'|s,a) \\pi(a|s).$$\n",
    "    \n",
    "$\\mathbb{P}(s_{t+1}) = \\mathbb{P}(s_t) \\cdot p^\\pi$ (because the MDP equipped with the policy is a Markov chain)\n",
    "\n",
    "$\\mathbb{P}(s_{t+k}) = \\mathbb{P}(s_t) \\cdot (p^\\pi)^k$ ($k$-step transition matrix in a Markov chain).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4481778d",
   "metadata": {},
   "source": [
    "if s' is to the right, up or bottom of, s then 1/3, otherwise 0\n",
    "\n",
    "the product $p^\\pi(s_{t+1}|s_t) \\mathbb{P}(s_t)$\n",
    "\n",
    "induction: $\\prod_{j=t}^{t+k-1} p^\\pi(s_{j+1}|s_j) \\mathbb{P}(s_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e83f10f",
   "metadata": {},
   "source": [
    "The stochastic process of $S_t$ is a Markov chain (as we saw in the previous exercise, since $\\pi$ is fixed, the probability of reaching $S_{t+1}$ is only conditionned by $S_t$). \n",
    "Let's adopt the convention that $\\mathbb{P}(S)$ is a row vector (for a certain ordering of states in $S$), and let's note $p^\\pi$ the $|S|\\times |S|$ stochastic matrix where $p^\\pi_{ss'} = p^\\pi(s'|s)$. \n",
    "Note that the first line of $p^\\pi$ is the distribution over $s_{t+1}$ provided that $s_t$ is the first state. \n",
    "Let's use our notations and recall a few results about Markov chains:\n",
    "- State $s'$ is said to be *reachable* from state $s$ is there exists a sequence of transitions originating in $s$ and ending in $s'$ with positive probability.\n",
    "- The Markov chain is *irreducible* if all states are reachable from any other state.\n",
    "- The *period* of a state $s$ is the greatest common divisor of the number of steps required to reach $s$ from itself.\n",
    "- Two states are said to *communicate* with each other if both are reachable from one another. \n",
    "- Two communicating states have the same period.\n",
    "- A *class* of states is a set of communicating states.\n",
    "- An irreducible Markov chain has a single class.\n",
    "- An irreducible Markov chain is *aperiodic* if all states have period one.\n",
    "- A class $C'$ is *accessible* from another class $C$ if there exists $(s,s')\\in C\\times C'$ such that $s'$ is accessible from $s$.\n",
    "- A class is *closed (or terminal, or absorbing)* if the probability of leaving the class is zero. Otherwise it is called *transient*.\n",
    "- A state is *positive recurrent* if one is guaranteed to come back to it in finite time.\n",
    "- A state that is positive recurrent and aperiodic is called *ergodic*.\n",
    "- If all states in an irreducible Markov chain are ergodic, then the chain is said to be ergodic.\n",
    "- Equivalently, a Markov chain is ergodic if any state can be reached from any other state in a bounded number of steps.\n",
    "- A *stationary distribution* is a distribution $\\rho$ such that $\\rho = \\rho \\cdot p^\\pi$.\n",
    "- If there is at least one positive recurrent state $s$ in the chain, then there exists at least one stationary distribution $\\rho$ and $\\rho(s)>0$.\n",
    "- Conversely, if there exists a stationary distribution $\\rho$ (such that $\\rho = \\rho \\cdot p^\\pi$) and $\\rho(s)>0$, then $s$ is positive recurrent.\n",
    "- If there is a *single* final class, then there is at most one stationary distribution.\n",
    "- If the Markov chain is irreductible and aperiodic (furthermore if it is ergodic), when $k\\rightarrow\\infty$, $(p^\\pi)^k$ tends to a matrix whose lines are all equal to the stationary distribution of the Markov chain.\n",
    "- Consequently, if the Markov chain is irreducible and aperiodic, in the long run, the distribution of states follows a stationary distribution $\\rho^\\pi(s|s_0) = \\lim_{k\\rightarrow \\infty} \\rho_0 (p^\\pi)^k$, for all possible initial state distribution $\\rho_0$. \n",
    "\n",
    "A few examples to make this concrete:\n",
    "- A discretized inverted pendulum with a random action policy is an irreducible and aperiodic Markov chain (but not necessarily ergodic as some states might be asymptotically long to reach).  \n",
    "- The FrozenLake MDP, under any policy, is never an irreducible Markov chain because of the absorbing \"hole in the ice\" states.\n",
    "- The holes in the ice of the FrozenLake MDP, under any policy, are (each) a set of final classes.  \n",
    "- Suppose we abstract the holes in the ice as a single absorbing state, then under any policy, this state forms a unique final class and the stationary distribution concentrates all probability mass on it.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (open discussion):**  \n",
    "Depending on the policy used, the Markov chain will have different properties. \n",
    "In particular, the stationary distribution is not necessarily unique: it depends on $s_0$. \n",
    "Here are a few informal examples for you to practice. Can you list a few more?  \n",
    "What about an Atari 2600 video game like Pong, under an optimal policy?  \n",
    "A self-driving car under a \"reasonably safe\" policy?  \n",
    "The patient example with a chronic disease, under a policy that fights off the disease?   \n",
    "The patient with a deadly disease under a policy that doesn't cure them?  \n",
    "The Mad Hatter's casino (from a previous class) under a fixed random policy?  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb68c29a",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "The patient with a chronic disease under a policy that fights off the disease will most likely live a rather long life (let's say infinite, for the sake of this example) and will explore states that are linked to the evolution of the disease. The states corresponding to non-recoverable situations however will not be visited. The Markov chain is not irreducible: depending on the initial state (recoverable situation or not) there are at least two subgraphs describing it. The same goes for the car driven by a cautious driver: many states are visited but not the ones that cause an accident.\n",
    "    \n",
    "The patient with a deadly disease and a bad treatment policy will likely die, sadly. On an infinite horizon, the stationary distribution only has probability mass on the states corresponding to death, which is the single absorbing class.\n",
    "    \n",
    "Similarly, the FrozenLake example (or the Pong game) has several terminal states, either by reaching the goal or by falling into a hole. It should be noted however that for such episodic environments, it is possible to define an alternate distribution $\\rho^\\pi(s|s_0)$ that describes the distribution of states before termination.\n",
    "    \n",
    "Finally, the Mad Hatter's casino under a fixed random policy is a very nice ergodic Markov chain: from any starting state there is a non-zero probability of reaching any state in a finite number of steps. No terminal states in wonderland!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5eb4a0-d9ae-45cd-90e3-187dddbbb7c4",
   "metadata": {},
   "source": [
    "## Summing rewards over states rather than time steps: the state occupancy measure\n",
    "\n",
    "Recall that we introduced $J(\\pi) = \\mathbb{E}_{s_0\\sim \\rho_0}[V^\\pi(s_0)]$. This is the value of policy $\\pi$, evaluated on average across starting states distributed according to $\\rho_0$. So:\n",
    "$$J(\\pi) = \\mathbb{E}_{(R_t)_{t\\in\\mathbb{N}}} \\left[ \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\bigg| \\rho_0,\\pi \\right]$$\n",
    "\n",
    "For the sake of simplicity, let's write:\n",
    "$$r^\\pi(s) = \\mathbb{E}_{\\substack{a\\sim \\pi \\\\ s'\\sim p^\\pi}} [r(s,a,s')].$$\n",
    "\n",
    "So we can write:\n",
    "$$J(\\pi) = \\mathbb{E}_{(S_t)_{t\\in\\mathbb{N}}} \\left( \\sum\\limits_{t = 0}^\\infty \\gamma^t r^\\pi(S_t) \\bigg| \\rho_0, \\pi \\right).$$\n",
    "\n",
    "We can remark each of the terms under the sum is concerned with a single time step, and its value depends only on $S_t$. \n",
    "We can swap the expectation of the sum (over time steps) by the sum (over times steps) of expectations:\n",
    "$$J(\\pi) = \\sum\\limits_{t = 0}^\\infty \\gamma^t \\mathbb{E}_{S_t} \\left( r^\\pi(S_t) | \\rho_0, \\pi \\right). $$\n",
    "\n",
    "Let's write $p(S_t=s|\\rho_0,\\pi)$ the probability density of encountering each state $s$ at time step $t$, provided that the starting state was drawn according to $\\rho_0$ and that the MDP evolves under policy $\\pi$. Then:\n",
    "$$J(\\pi) = \\sum\\limits_{t = 0}^\\infty \\gamma^t \\int_S r^\\pi(s) p(S_t=s|\\rho_0,\\pi) ds.$$\n",
    "\n",
    "Intead of summing over time steps first, let us sum over states. This yields:\n",
    "$$J(\\pi) = \\int_S \\sum\\limits_{t = 0}^\\infty \\gamma^t r^\\pi(s) p(S_t=s|\\rho_0,\\pi) ds.$$\n",
    "\n",
    "Let us write $\\rho^\\pi_{\\rho_0}(s) = \\sum\\limits_{t = 0}^\\infty \\gamma^t p(S_t=s|\\rho_0,\\pi)$ for all $s \\in S$. This quantity is called the *discounted state occupancy measure*. Although it is not a proper probability density (it does not sum to one), it is a proxy of how much time policy $\\pi$ spends in state $s$ over the course of a trajectory, given that it started in $S_0$ drawn according to $\\rho_0$.\n",
    "\n",
    "So we have $J(\\pi) = \\langle \\rho^\\pi_{\\rho_0}, r^\\pi \\rangle$. We can abuse the $\\mathbb{E}$ notation and write:\n",
    "$$J(\\pi) = \\mathbb{E}_{s\\sim \\rho^\\pi_{\\rho_0}}[r^\\pi(s)] = \\langle \\rho^\\pi_{\\rho_0}, r^\\pi \\rangle.$$\n",
    "\n",
    "So the value of policy $\\pi$ is both a sum of rewards over time steps $\\mathbb{E}[ \\sum_{t = 0}^\\infty \\gamma^t R_t]$ and a sum of rewards across states $\\langle \\rho^\\pi_{\\rho_0}, r^\\pi \\rangle$, weighted by their occupancy measure under $(\\pi,\\rho_0)$.\n",
    "\n",
    "Interestingly, searching for a policy which maximizes $J(\\pi)$ boils down to searching for a policy's whose state occupancy measure is maximally aligned with the reward model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928fb0c1-c2b4-44bc-a309-12fe6bd0a167",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "What does $\\rho^\\pi_{\\rho_0}$ sum to? Normalize this measure to turn it into a probability distribution and correct the last expression of $J(\\pi)$ above.\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "    \n",
    "The integral of $\\rho^\\pi_{\\rho_0}$ across $S$ is:\n",
    "\\begin{align*}\n",
    "\\int_S \\rho^\\pi_{\\rho_0}(s)ds &= \\int_S \\sum\\limits_{t = 0}^\\infty \\gamma^t p(S_t=s|\\rho_0,\\pi) ds \\\\\n",
    " &= \\sum\\limits_{t = 0}^\\infty \\gamma^t \\int_S  p(S_t=s|\\rho_0,\\pi) ds \\\\\n",
    " &= \\sum\\limits_{t = 0}^\\infty \\gamma^t \\\\\n",
    " &= \\frac{1}{1-\\gamma}\n",
    "\\end{align*}\n",
    "\n",
    "So $\\rho^\\pi_{\\rho_0}$ always sums to $\\frac{1}{1-\\gamma}$. A proper probability distribution would then be $(1-\\gamma)\\rho^\\pi_{\\rho_0}$ and a correct writing for $J(\\pi)$ is thus:\n",
    "$$J(\\pi) = \\mathbb{E}_{s\\sim (1-\\gamma)\\rho^\\pi_{\\rho_0}}[r^\\pi(s)]$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5b8a2d",
   "metadata": {},
   "source": [
    "Finite sum: constant Z, divide by Z\n",
    "\n",
    "$\\sum \\gamma_t \\int_s p(S_t=s|\\rho_0, \\pi)$ = $\\sum \\gamma_t$ = $\\frac{1}{1-\\gamma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65112987-8a14-4c63-bd94-a3b56924f955",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Can you derive the same result with a state-action occupancy measure, instead of a state-only occupancy measure?\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "    \n",
    "$$J(\\pi) = \\mathbb{E}_{(S_t,A_t)_{t\\in\\mathbb{N}}}\\left[ \\sum\\limits_{t = 0}^\\infty \\gamma^t r(S_t,A_t) \\right]$$\n",
    "\n",
    "Let's define $\\rho^\\pi_{\\rho_0}(s,a) = \\sum\\limits_{t = 0}^\\infty \\gamma^t p(S_t=s, A_t=a|\\rho_0,\\pi)$. Then, given a reward model $r(s,a))$:\n",
    "$$J(\\pi) = \\langle \\rho^\\pi_{\\rho_0}, r \\rangle = \\mathbb{E}_{s\\sim (1-\\gamma)\\rho^\\pi_{\\rho_0}}[r(s,a)].$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809805fb-7584-4e5a-a699-abec9932e43a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Consider a finite state space MDP. So $\\rho_0$ is a line vector and $p^\\pi$ is a square matrix. In a previous exercise, we derived the expression of $p(S_t=s|\\rho_0,\\pi)$. Use this expression in $\\rho^\\pi_{\\rho_0}(s)$.\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "    \n",
    "We had $p(S_t=s|\\rho_0,\\pi) = \\rho_0 (p^\\pi)^t$.\n",
    "\n",
    "So $\\rho^\\pi_{\\rho_0}(s) = \\sum\\limits_{t = 0}^\\infty \\rho_0 (\\gamma p^\\pi)^t$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a39493c",
   "metadata": {},
   "source": [
    "$=\\lim_k \\rho_0 (p^\\pi)^k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6413e969-7697-4f02-9408-323e62c351de",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Write a function that estimates $\\rho^\\pi_{\\rho_0}$ to compute the value of the initial state in FrozenLake. Compare to the Monte Carlo estimate built in a previous exercise.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b73dbf1d-e814-4359-bc92-e37bccfdb9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code).\n",
    "# https://marcinbogdanski.github.io/rl-sketchpad/UCL_Course_on_RL/Lecture03_DP/DynamicProgramming.html\n",
    "def estimate():\n",
    "    gamma = 0.9\n",
    "\n",
    "    rho_0 = np.zeros((16))\n",
    "    rho_0[0] = 1 # start from first state\n",
    "\n",
    "    p_pi = np.zeros((16, 16))\n",
    "    for state in env.env.P.keys():\n",
    "        p_state = env.env.P[state][fl.RIGHT]\n",
    "        for proba, new_state, _, _ in p_state: # new_states can be duplicated\n",
    "            p_pi[state, new_state] += proba\n",
    "        assert np.isclose(np.sum(p_pi[state, :]), 1)\n",
    "\n",
    "    state_occupancy = np.zeros(16)\n",
    "\n",
    "    horizon = 200\n",
    "    p_pi_sum_t = np.eye(16)\n",
    "    for t in range(horizon):\n",
    "        state_occupancy += gamma**t*(rho_0 @ p_pi_sum_t)\n",
    "        p_pi_sum_t = p_pi @ p_pi_sum_t\n",
    "    return state_occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "655d5837",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p_pi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# env.env.P\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# absorbing state, compute the value: stay at 1\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# build P\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mp_pi\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'p_pi' is not defined"
     ]
    }
   ],
   "source": [
    "# env.env.P\n",
    "# absorbing state, compute the value: stay at 1\n",
    "# build P\n",
    "\n",
    "print(p_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2899b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.66361974 0.71297989 0.36057067 0.270428   0.54844607 3.95420625\n",
      " 0.12835168 1.19633904 0.16453382 0.05664279 0.06726825 0.20180474\n",
      " 0.49360146 0.02427548 0.03923303 0.11769908]\n",
      "0.11769908102276781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\flowp\\miniconda3\\envs\\rl\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "state_occupancy = estimate()\n",
    "print(state_occupancy)\n",
    "rewards = np.zeros(16)\n",
    "rewards[-1] = 1\n",
    "value = np.dot(rewards, state_occupancy)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c01d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def display_function_of_state(f):\n",
    "    print(np.reshape(f, (4,4)))\n",
    "    return\n",
    "\n",
    "def state_occupancy_measure(P_pi,rho,gamma,horizon):\n",
    "    state_proba_at_t = rho\n",
    "    rho_pi = rho\n",
    "    for t in range(1,horizon+1):\n",
    "        state_proba_at_t = state_proba_at_t @ P_pi\n",
    "        rho_pi += gamma**t * state_proba_at_t\n",
    "    return rho_pi\n",
    "\n",
    "def fl_P_and_r(env,pi):\n",
    "    r_pi = np.zeros((env.observation_space.n))\n",
    "    P_pi = np.zeros((env.observation_space.n, env.observation_space.n))\n",
    "    for x in range(env.observation_space.n):\n",
    "        outcomes = env.unwrapped.P[x][pi[x]]\n",
    "        for o in outcomes:\n",
    "            p = o[0]\n",
    "            y = o[1]\n",
    "            r = o[2]\n",
    "            P_pi[x,y] += p\n",
    "            r_pi[x] += r*p\n",
    "    return P_pi,r_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad8db9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "\n",
    "def mc_eval(env,pi,nb_trials):\n",
    "    horizon = 200\n",
    "    gamma = 0.9\n",
    "    Vepisode = np.zeros(nb_trials)\n",
    "    for i in range(nb_trials):\n",
    "        state,_ = env.reset()\n",
    "        for t in range(horizon):\n",
    "            next_state, r, done, _, _ = env.step(pi[state])\n",
    "            Vepisode[i] += gamma**t * r\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "    return Vepisode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb842182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.33333333 0.        ]\n",
      "[[1.66361974 0.71297989 0.36057067 0.270428  ]\n",
      " [0.54844607 3.95420625 0.12835168 1.19633904]\n",
      " [0.16453382 0.05664279 0.06726825 0.20180474]\n",
      " [0.49360146 0.02427548 0.03923303 0.11769908]]\n",
      "estimation of V(s0) through rho_pi: 0.01307767569389063\n",
      "Monte Carlo estimate: 0.012790500957490921 std dev: 0.07477379732900774\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "# from solutions.fl_mc_eval import mc_eval\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "pi = fl.RIGHT*np.ones((env.observation_space.n))\n",
    "gamma = 0.9\n",
    "horizon = 200\n",
    "\n",
    "# rho0\n",
    "rho0 = np.zeros((env.observation_space.n))\n",
    "rho0[0] = 1\n",
    "\n",
    "P_pi,r_pi = fl_P_and_r(env,pi)\n",
    "print(r_pi)\n",
    "rho_pi = state_occupancy_measure(P_pi,rho0,gamma,horizon)\n",
    "display_function_of_state(rho_pi)\n",
    "\n",
    "print(\"estimation of V(s0) through rho_pi:\", np.dot(rho_pi,r_pi))\n",
    "mc_rollouts = mc_eval(env,pi,100000)\n",
    "print(\"Monte Carlo estimate:\", np.mean(mc_rollouts), \"std dev:\", np.std(mc_rollouts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cba8e8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 7, 0.0, True),\n",
       " (0.3333333333333333, 3, 0.0, False),\n",
       " (0.3333333333333333, 3, 0.0, False)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.P[3][fl.RIGHT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3b080d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16636197, 0.07129799, 0.03605707, 0.0270428 , 0.05484461,\n",
       "       0.39542062, 0.01283517, 0.1196339 , 0.01645338, 0.00566428,\n",
       "       0.00672682, 0.02018047, 0.04936015, 0.00242755, 0.0039233 ,\n",
       "       0.01176991])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_occupancy*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57b76b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999999999294492"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(state_occupancy*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "542152e6-3714-44a4-8717-c9b66db96803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/no_solution_yet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73abef0d",
   "metadata": {},
   "source": [
    "## Homework: Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf881583",
   "metadata": {},
   "source": [
    "In this notebook, we have been using a suite of environments called [Gymnasium](https://gymnasium.farama.org/), which provided us with the FrozenLake environment. Gymnasium is a maintained fork of OpenAI’s Gym library which has become the *de facto* standard in terms of Python API for reinforcement learning environments. As Gym had a history of low maintenance and poor documentation, all development of Gym has been moved to Gymnasium.\n",
    "\n",
    "The Gymnasium API has been used without introduction of explanations: it is time to explore a little more how it works.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Read three specific pages in Gymnasium's documentation:  \n",
    "- The [front page](https://gymnasium.farama.org/)\n",
    "- The introduction to Gymnasium's [basic usage](https://gymnasium.farama.org/content/basic_usage/)\n",
    "- The general API to all Gymnasium's [environments](https://gymnasium.farama.org/api/env/)\n",
    "\n",
    "Also take a minute to browse through the different families of environments provided.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0910e3cc-03e6-49d2-9ef5-7adbfc6080d9",
   "metadata": {},
   "source": [
    "## Interpreting gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a3293f-4e7e-43b8-8438-90e5476dc937",
   "metadata": {},
   "source": [
    "We motivated the introduction of $\\gamma$ with the argument that $\\sum\\limits_{t = 0}^\\infty R_t$ might grow unbounded. The solution was to *discount* future rewards by $\\gamma^t$.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Suppose rewards are drawn from the $[0,1]$ interval. What is the upper bound on $\\sum\\limits_{t = 0}^\\infty \\gamma^t R_t$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f173a3",
   "metadata": {},
   "source": [
    "$R_t=1 \\forall t$ so sum of $\\gamma_t = \\frac{1}{1-\\gamma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ad055-a33f-47bc-8d2a-65d1c8e7283e",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "The upper bound is $\\frac{1}{1-\\gamma}$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f905749-e619-474f-bc27-1fbeb65d3d6a",
   "metadata": {},
   "source": [
    "Consider an MDP $\\langle S, A, p, r\\rangle$ and a policy $\\pi$. \n",
    "Let's now append one state to $S$, which we will write $s_{null}$. This defines a new state space $S'$. We will consider $S$ is ordered and $s_{null}$ is the last element of $S'$. \n",
    "The action space remains the same.  \n",
    "$s_{null}$ will play the role of a zero-reward, absorbing state, reachable from any other state.\n",
    "\n",
    "When in any state $s \\in S$, we first check for a transition to $s_{null}$ with probability $1-\\gamma$. Otherwise, we transition to another state $s'$ with probability $p^\\pi(s'|s)$. \n",
    "So $s_{null}$ is an absorbing state, reachable in one step from any other state with probability $1-\\gamma$. \n",
    "All transitions from $s_{null}$ loop back to itself with probability one, whichever the action taken. Consequently, the policy $\\pi$ can be trivially extended to $S'$ as a policy $\\pi'$ which takes the same action as $\\pi$ in all $s\\in S$ and takes any action in $s_{null}$.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Let $p^\\pi$ be the transition matrix of the first MDP under $\\pi$. Write the transition matrix $p^{\\pi'}$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26da3b1",
   "metadata": {},
   "source": [
    "$p^{\\pi'}$ has one row and one column more than $p^{\\pi}$. The new column is made of $1-\\gamma$, and the last row has only 1 in the last column.\n",
    "For the first rows and columns, it is the previous value multiplied by $\\gamma$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67576e8e-9d00-4d48-b4c8-84d67ba5d2b5",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "    \n",
    "Let's write $\\mathbf{(1-\\gamma)_{S}}$ the column vector composed of $|S|$ elements, all equal to $1-\\gamma$.  \n",
    "Let's write $\\mathbf{0_S}$ the column vector composed of $|S|$ elements, all equal to $0$.  \n",
    "Then:\n",
    "$$p^{\\pi'} = \\left[ \\begin{array}{cc} \\gamma p^\\pi & \\mathbf{(1-\\gamma)_{S}} \\\\ \\mathbf{0_S}^T & 1 \\end{array} \\right]$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58f834c",
   "metadata": {},
   "source": [
    "So, in plain words, $s_{null}$ is a *terminal* state, and $\\gamma$ is a probability of *non-termination* in the second MDP.\n",
    "\n",
    "An iteresting aspect is that $s_{null}$ is accessible from *any* state of $S$ with the same probability.\n",
    "\n",
    "Let's call *length* of a trajectory the first time step at which $s_{null}$ is encountered. So it is the number of transitions leading to $s_{null}$ (note that this includes the last transition to $s_{null}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d28477",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "What is the probability of a trajectory of length greater or equal to $h$ time steps?  \n",
    "What is the probability of a trajectory of length exactly equal to $h$ time steps?  \n",
    "How is this probability distribution called?  \n",
    "Deduce the average trajectory length.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd63f53b",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "    \n",
    "Reaching $s_{null}$ after at least $h$ time steps, means not having reached it before, which has probability $(1-\\gamma)^h$.\n",
    "    \n",
    "$$\\mathbb{P}(\\textrm{length}\\geq h) = (1-\\gamma)^h$$\n",
    "\n",
    "Hence the probability of a trajectory of length $h$ is:\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(\\textrm{length}= h) &= \\mathbb{P}(h+1 > \\textrm{length}\\geq h)\\\\\n",
    "&= \\mathbb{P}(\\textrm{length}\\geq h) - \\mathbb{P}(\\textrm{length}\\geq h+1)\\\\\n",
    "&= (1-\\gamma)^h - (1-\\gamma)^{h+1}\\\\\n",
    "&= \\gamma (1-\\gamma)^h\n",
    "\\end{align*}\n",
    "    \n",
    "So the length of a trajectory follows a geometric distribution of parameter $\\gamma$. The average trajectory length is hence $\\frac{1}{1-\\gamma}$. We will sometimes call this quantity (a bit abusively) the **horizon of MDP $\\langle S,A,p,r \\rangle$ under the $\\gamma$-discounted criterion**.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39295ced-5fa9-44e0-8999-9897a6eff8d3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Can you relate the value of $\\pi'$ in the second MDP under the total reward criterion to that of $\\pi$ in the first MDP under the $\\gamma$ discounted criterion?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb36e0e-d4c0-4c35-9f67-fb34284cb257",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "    \n",
    "We shall write $V^{\\pi'}$ the value of $\\pi'$ under the total reward criterion, and $V^\\pi_\\gamma$ that of $\\pi$ under the $\\gamma$-discounted criterion, to avoid ambiguities.\n",
    "\n",
    "Let's consider the first transition of a trajectory starting in $s$. Either this transition moves to $s_{null}$ with probability $1-\\gamma$, or it moves to a state $s'\\in S$, with probability $\\gamma p^\\pi(s'|s)$. The transition to $s_{null}$ provides zero reward, while the transition to $s'$ provides $r^\\pi(s,s')$. From $s'$, the expected gain is $V^\\pi(s')$. So:\n",
    "$$V^{\\pi'}(s) = (1-\\gamma)\\cdot 0 + \\mathbb{E}_{s' \\sim \\gamma p^\\pi} [r^\\pi(s,s') + V^\\pi(s')].$$\n",
    "    \n",
    "But $\\mathbb{E}_{s' \\sim \\gamma p^\\pi}[\\cdot] = \\gamma \\mathbb{E}_{s' \\sim p^\\pi}[\\cdot]$, so:\n",
    "$$V^{\\pi'}(s) = \\gamma \\mathbb{E}_{s' \\sim p^\\pi} \\left[r^\\pi(s,s') + V^\\pi(s')\\right].$$\n",
    "    \n",
    "We can unfold this for the following time step, ie. states reached from $s'$:\n",
    "$$V^{\\pi'}(s) = \\gamma \\mathbb{E}_{s' \\sim \\gamma p^\\pi} \\left[r^\\pi(s,s') + \\gamma \\mathbb{E}_{s'' \\sim (p^\\pi)^2} \\left[r^\\pi(s',s'') + V^\\pi(s'') \\right] \\right].$$\n",
    "\n",
    "Which, in the limit, yields:\n",
    "$$V^{\\pi'}(s) = \\gamma \\mathbb{E}_{s_t \\sim (p^\\pi)^t} \\left[ \\sum_{t=0}^\\infty \\gamma^t r^\\pi(s_t,s_{t+1}) \\right].$$\n",
    "\n",
    "But we find there the definition of $V^\\pi_\\gamma$:\n",
    "$$V^{\\pi'}(s) = \\gamma V^\\pi_\\gamma(s).$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218a597f",
   "metadata": {},
   "source": [
    "Consequently, to a constant multiplicative factor, the value of a trajectory under a $\\gamma$-discounted criterion, is actually the value of playing the same game under a total reward criterion, but with a probability of termination of $1-\\gamma$ at each time step. In other words, using the $\\gamma$-discounted criterion boils down to taking into account a probability $\\gamma$ of \"staying alive\" at each time step in the current game."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
