{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b6229bf",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://erachelson.github.io/RLclass_MVA/\">https://erachelson.github.io/RLclass_MVA/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72b7186",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Chapter 1: Modeling sequential decision problems with Markov Decision Processes</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b1870",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Learning outcomes:**  \n",
    "This chapter is mostly about definitions. By the end of this chapter you should be able to define and discuss what is:\n",
    "- a Markov decision process,\n",
    "- an action policy,\n",
    "- a value function,\n",
    "- an optimal policy / value function.\n",
    "\n",
    "Additionally, after doing the homework, you should be able to define and discuss:\n",
    "- Monte Carlo estimation of values,\n",
    "- the stationary distribution of a Markov decision process,\n",
    "- the state occupancy measure of a policy given a starting distribution,\n",
    "- the horizon under a $\\gamma$-discounted criterion.\n",
    "\n",
    "You should also be familiar with the Gymnasium API.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b29ed",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Definition\n",
    "\n",
    "Let's take a higher view and develop a general theory for describing problems such as writing a prescription for the patient we considered in the previous class.\n",
    "\n",
    "Let us assume we have:\n",
    "- a set of states $S$ describing the system to control,\n",
    "- a set of actions $A$ we can apply.\n",
    "\n",
    "Curing patients is a conceptually difficult task. \n",
    "To keep things grounded, we shall use a toy example called [FrozenLake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) and work our way to more general concepts. It's also the occasion to familiarize with [Gymnasium](https://gymnasium.farama.org/).\n",
    "\n",
    "<center><img src=\"img/frozenlake.jpg\" style=\"height: 300px;\"></img></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d24023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "# use render_mode=\"human\" to open the game window\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "env.reset()\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dd3f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this only if you have used render_mode=\"human\" in the cell above\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373e826a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The game's goal is to navigate across this lake, from position S to position G, while avoiding falling into the holes H. Frozen positions are slippery so you don't always move in the intended direction. Reaching the goal provides a reward of 1, and zero otherwise. Falling into a hole or reaching the goal ends an episode.\n",
    "\n",
    "A more complete description of the environment is provided in [Gymnasium's documentation](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) (or through `help(fl.FrozenLakeEnv)`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490f722a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "How many states are there in this game?  \n",
    "How many actions?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6009d0a",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "States set: the 16 positions on the map.  \n",
    "Actions set: the 4 actions $\\{$N,S,E,W$\\}$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5008a9cd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's confirm that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c3b5e9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e508cc4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At every time step, the system state is $S_t$ and we decide to apply action $A_t$. This results in observing a new state $S_{t+1}$ and receiving a scalar reward signal $R_t$ for this transition.\n",
    "\n",
    "$R_t$ tells us how happy we are with the last transition.\n",
    "\n",
    "For example, in FrozenLake, all transitions have reward 0 except for the one that reaches the goal, which yields reward 1. Let's verify this in the code and introduce a few utility functions on the way.\n",
    "\n",
    "Note that $S_t$, $A_t$, $S_{t+1}$ and $R_t$ are random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd3b19d-5121-483b-810a-e9dffc08ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_actions.py\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "actions = {fl.LEFT: '\\u2190', fl.DOWN: '\\u2193', fl.RIGHT: '\\u2192', fl.UP: '\\u2191'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb24167-d16f-4589-a8ed-7416faf1d8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_to_s.py\n",
    "def to_s(env,row,col):\n",
    "    return row*env.unwrapped.ncol+col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8290d58-223d-4771-81a9-75c901c2924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_to_row_col.py\n",
    "def to_row_col(env,s):\n",
    "    col = s%env.unwrapped.ncol\n",
    "    row = int((s-col)/env.unwrapped.ncol)\n",
    "    return row,col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baaf6bd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "from solutions.fl_actions import actions\n",
    "from solutions.fl_to_s import to_s\n",
    "from solutions.fl_to_row_col import to_row_col\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "\n",
    "print(actions)\n",
    "row=3\n",
    "col=2\n",
    "a=2\n",
    "s=to_s(env,row,col)\n",
    "print(env.unwrapped.P[s][a])\n",
    "print(\"Apply \", actions[a], \" from (\", row, \", \", col, \"):\", sep='')\n",
    "for tr in env.unwrapped.P[to_s(env,row,col)][a]:\n",
    "    print(\"  Reach (\", to_row_col(env,tr[1]), \") and get reward \", tr[2], \" with proba \", tr[0], \".\", sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69869e23",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will now make our main assumption about the systems we want to control.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Fundamental assumption (Markov property)**\n",
    "$$\\mathbb{P}(S_{t+1},R_t|S_t, A_t, S_{t-1}, A_{t-1}, \\ldots, S_0, A_0) = \\mathbb{P}(S_{t+1},R_t|S_t, A_t)$$\n",
    "</div>\n",
    "    \n",
    "Such a system will be called a Markov Decision Process (MDP).\n",
    "\n",
    "One generally separates the state dynamics and the rewards by:\n",
    "$$\\mathbb{P}(S_{t+1},R_t|S_t, A_t) = \\mathbb{P}(S_{t+1}|S_t, A_t)\\cdot \\mathbb{P}(R_t|S_t, A_t, S_{t+1})$$\n",
    "\n",
    "Which leads in turn to the general definition of an MDP:\n",
    "<div class=\"alert alert-success\"><b>Markov Decision Process (MDP)</b><br>\n",
    "A Markov Decision Process is given by:\n",
    "<ul>\n",
    "<li> A set of states $\\mathcal{S}$\n",
    "<li> A set of actions $\\mathcal{A}$\n",
    "<li> A (Markovian) transition model $\\mathbb{P}\\left(S_{t+1} | S_t, A_t \\right)$, noted $p(s'|s,a)$\n",
    "<li> A reward model $\\mathbb{P}\\left( R_t | S_t, A_t, S_{t+1} \\right)$, noted $r(s,a)$ or $r(s,a,s')$\n",
    "<li> A set of discrete decision epochs $\\mathcal{T}=\\{0,1,\\ldots,h\\}$\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Most of the results presented here can be found in M. L. Puterman's classic book, **[Markov Decision Processes: Discrete Stochastic Dynamic Programming](https://www.wiley.com/en-us/Markov+Decision+Processes%3A+Discrete+Stochastic+Dynamic+Programming-p-9781118625873)**.\n",
    "\n",
    "If $h\\rightarrow\\infty$ we have an infinite horizon control problem.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Since we will work most of the time with infinite horizon problems, we shall identify the MDP with the 4-tuple $(\\mathcal{S},\\mathcal{A},p,r)$.    \n",
    "</div>\n",
    "\n",
    "\n",
    "In the general case, $\\mathcal{S}$ and $\\mathcal{A}$ may each be either:\n",
    "  - arbitrary finite sets,\n",
    "  - arbitrary countable infinite sets,\n",
    "  - compact subsets of a finite dimensional Euclidean space, or\n",
    "  - non-empty Borel subsets of complete, separable metric spaces.\n",
    "\n",
    "The reward model can be written interchangeably $r(s,a)$ or $r(s,a,s')$, depending on authors, with $r(s,a) = \\mathbb{E}_{s'\\sim p(\\cdot|s,a)} [r(s,a,s')]$. We will use both notations in the rest of the class.\n",
    "\n",
    "So, in RL, we wish to control the trajectory of a system that, we suppose, behaves as a Markov Decision Process.\n",
    "\n",
    "<center><img src=\"img/dynamic.png\" style=\"height: 240px;\"></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e3c21",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Policies\n",
    "\n",
    "Formally, how does one write the behavior of an agent?\n",
    "This behavior specifies how to choose actions at each time step:\n",
    "$$A_t \\sim \\pi_t.$$\n",
    "\n",
    "Let $\\Delta_\\mathcal{A}$ be the set of probability measures on the action space $\\mathcal{A}$. Then the law $\\pi_t$ of $A_t$ belongs to $\\Delta_\\mathcal{A}$.  \n",
    "$\\pi_t$ is called the **decision rule** at step $t$, it is a distribution over the action space $\\mathcal{A}$.  \n",
    "The collection $\\pi = \\left(\\pi_t \\right)_{t\\in T}$ is called a **policy**.\n",
    "\n",
    "<div class=\"alert alert-success\"><b>Policy $\\pi$</b><br>\n",
    "A policy $\\pi$ is a sequence of decision rules $\\pi_t$: $\\pi = \\{\\pi_t\\}_{t\\in T}$, with $\\pi_t \\in \\Delta_\\mathcal{A}$.\n",
    "</div>\n",
    "\n",
    "One policy implies one specific distribution over trajectories over the frozen lake. More generally, the policy and $S_0$ condition the sequence $S_0, A_0, R_0, S_1, A_1, R_1, \\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bece33be",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Value of a trajectory / of a policy\n",
    "\n",
    "In FrozenLake as in the patient's example, some trajectories are better than others. We need a criterion to compare trajectories. Intuitively, this criterion should reflect the idea that a good policy accumulates as much reward as possible along a trajectory.\n",
    "\n",
    "Let's compare the policy that always moves to the right and the policy that always moves left by summing the rewards obtained along trajectories and then averaging these rewards across trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f3558",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "nb_episodes = 50000\n",
    "horizon = 200\n",
    "\n",
    "Vright = np.zeros(nb_episodes)\n",
    "for i in range(nb_episodes):\n",
    "    env.reset()\n",
    "    for t in range(horizon):\n",
    "        next_state, r, done, trunc, _ = env.step(fl.RIGHT)\n",
    "        Vright[i] += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "Vleft  = np.zeros(nb_episodes)\n",
    "for i in range(nb_episodes):\n",
    "    env.reset()\n",
    "    for t in range(horizon):\n",
    "        next_state, r, done, trunc, _ = env.step(fl.LEFT)\n",
    "        Vleft[i] += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print(\"est. value of 'right' policy:\", np.mean(Vright), \"std dev:\", np.std(Vright))\n",
    "print(\"est. value of 'left'  policy:\", np.mean(Vleft),  \"std dev:\", np.std(Vleft))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b7a8b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the general case, this sum of rewards on an infinite horizon might be unbounded. So let us introduce the **$\\gamma$-discounted sum of rewards** (from a starting state $s$, under policy $\\pi$) random variable:\n",
    "$$G^\\pi(s) = \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s,\\\\ A_t \\sim \\pi_t,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1}).\\end{array}$$\n",
    "\n",
    "$G^\\pi(s)$ represents what we can gain in the long-term by applying the actions from $\\pi$.\n",
    "\n",
    "Then, given a starting state $s$, we can define the value of $s$ under policy $\\pi$:\n",
    "$$v^\\pi(s) = \\mathbb{E} \\left[ G^\\pi(s) \\right]$$\n",
    "\n",
    "This defines the value function $v^\\pi$ of policy $\\pi$:\n",
    "<div class=\"alert alert-success\"><b>Value function $v^\\pi$ of a policy $\\pi$ under a $\\gamma$-discounted criterion</b><br>\n",
    "$$v^\\pi : \\left\\{\\begin{array}{ccl}\n",
    "\\mathcal{S} & \\rightarrow & \\mathbb{R}\\\\\n",
    "s & \\mapsto & v^\\pi(s)=\\mathbb{E}\\left( \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\bigg| S_0 = s, \\pi \\right)\\end{array}\\right. $$\n",
    "</div>\n",
    "\n",
    "And, given a distribution $\\rho_0$ on starting states, we can map $\\pi$ to the scalar value:\n",
    "$$J(\\pi) = \\mathbb{E}_{s \\sim \\rho_0} \\left[ v^\\pi(s) \\right]$$\n",
    "\n",
    "Note that this definition is quite arbitrary: instead of the expected (discounted) sum of rewards, we could have taken the average reward over all time steps, or some other (more or less exotic) comparison criterion between policies. For example:\n",
    "<table>\n",
    "<tr>\n",
    "    <td>Finite horizon</td>\n",
    "    <td>$v(s) = \\mathbb{E}\\left( \\sum\\limits_{t = 0}^h R_t \\bigg| s_0 = s \\right)$ </td>\n",
    "    <td width=\"150px\">with $h \\in \\mathbb{N}$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Average reward </td>\n",
    "    <td width=\"300px\">$v(s) = \\mathbb{E}\\left( \\lim\\limits_{h\\rightarrow\\infty}  \\frac{1}{h} \\sum\\limits_{t = 0}^h R_t \\bigg| s_0 = s \\right)$ </td>\n",
    "    <td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Total reward </td>\n",
    "    <td>$v(s) = \\mathbb{E}\\left( \\lim\\limits_{h\\rightarrow\\infty} \\sum\\limits_{t = 0}^h R_t \\bigg| s_0 = s \\right)$ </td>\n",
    "    <td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Discounted reward </td>\n",
    "    <td>$v(s) = \\mathbb{E}\\left( \\lim\\limits_{h\\rightarrow\\infty} \\sum\\limits_{t = 0}^h \\gamma^t R_t \\bigg| s_0 = s \\right)$ </td>\n",
    "    <td width=\"150px\">with $0\\leq \\gamma<1$</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "- The average reward criterion characterizes the average reward per time step the agent gets. This can be useful in some control applications. However, in the case of FrozenLake, we don't want to average our rewards, we want to get to the goal as soon as possible.\n",
    "- The total reward criterion seems more adapted: it maximizes the cumulated rewards obtained during an episode. But it does not discriminate whether they were obtained at the beginning or late in the episode. Additionally, it suffers from a major flaw: for infinite horizon problems, even if the reward model is bounded, this sum might diverge. So we need a better formulation for the general case of infinite horizon problems.\n",
    "- The discounted reward criterion suits our needs. The discount factor ($0\\leq \\gamma<1$) guarantees that with bounded reward models $r$, the sum always converges. Also it has the properties we desire: a reward of 1 obtained at the first time step weights 1 in the final criterion, while a reward of 1 obtained after $t$ time steps only weights $\\gamma^t$; it is *discounted* by $\\gamma^t$ (hence the criterion's name).\n",
    "\n",
    "Most of the RL literature uses this discounted criterion (in some cases with $\\gamma=1$), some works use a finite horizon criterion, some use the average reward criterion, and few works venture into more exotic criteria. For now, we will limit ourselves to the discounted criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ab534",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Optimal policies\n",
    "\n",
    "The fog clears up a bit: we can now compare policies given an initial state (or initial state distribution).  \n",
    "\n",
    "Thus, an **optimal** policy is one that is better than any other.\n",
    "\n",
    "<div class=\"alert alert-success\"><b>Optimal policy $\\pi^*$</b><br>\n",
    "$\\pi^*$ is said to be optimal iff $\\pi^* \\in \\arg\\max\\limits_{\\pi} v^\\pi$.<br>\n",
    "<br>\n",
    "    \n",
    "A policy is optimal if it **dominates** over any other policy in every state:\n",
    "$$\\pi^* \\textrm{ is optimal}\\Leftrightarrow \\forall s\\in S, \\ \\forall \\pi, \\ v^{\\pi^*}(s) \\geq v^\\pi(s)$$\n",
    "</div>\n",
    "\n",
    "Note that although there may be several optimal policies, they all share the same value function $v^* = v^{\\pi^*}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8637d7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now get to our first fundamental result. Fortunately for us...  \n",
    "\n",
    "<div class=\"alert alert-success\"><b>Theorem: family of optimal policies</b><br>\n",
    "For $\\left\\{\\begin{array}{l}\n",
    "\\gamma\\textrm{-discounted criterion}\\\\\n",
    "\\textrm{infinite horizon}\n",
    "\\end{array}\\right.$, \n",
    "there always exists at least one optimal stationary, deterministic, memoryless policy.\n",
    "</div>\n",
    "\n",
    "Let's explain a little:\n",
    "- Memoryless (also called Markovian): all decision rules are only conditioned by the last seen state. Mathematically: \n",
    "$\\left.\\begin{array}{l}\n",
    "\\forall \\left(s_i,a_i\\right)_{i\\leq t-1}\\in \\left(\\mathcal{S}\\times \\mathcal{A}\\right)^{t-1}\\\\\n",
    "\\forall \\left(s'_i,a'_i\\right)_{i\\leq t-1}\\in \\left(\\mathcal{S}\\times \\mathcal{A}\\right)^{t-1}\\\\\n",
    "\\forall s \\in \\mathcal{S}\n",
    "\\end{array}\\right\\}, \\pi_t\\left(A_t|S_0=s_0, A_0=a_0, \\ldots, S_t=s\\right) = \\pi_t\\left(A_t|S'_0=s'_0, A'_0=a'_0, \\ldots, S_t=s\\right)$.  \n",
    "One writes $\\pi_t(A_t|S_t=s)$, or more simply $\\pi_t(\\cdot | s)$ or $\\pi_t(s) \\in \\Delta_\\mathcal{A}$.\n",
    "- Stationary (and memoryless): all decision rules are the same throughout time. Mathematically:  \n",
    "$\\forall (t,t')\\in \\mathbb{N}^2, \\pi_t(A_t|S_t=s) = \\pi_{t'}(A_{t'}|S_{t'}=s)$.  \n",
    "This unique distribution is written $\\pi(\\cdot | s) = \\pi_t( \\cdot | s)$ or $\\pi(s) \\in \\Delta_\\mathcal{A}$.\n",
    "- Deterministic: all decision rules put all probability mass on a single item of the action space $\\mathcal{A}$.  \n",
    "$\\pi_t(A_t|history) = \\left\\{\\begin{array}{l}\n",
    "1\\textrm{ for a single }a\\\\\n",
    "0\\textrm{ otherwise}\n",
    "\\end{array}\\right.$.\n",
    "\n",
    "So in simpler words, we know that among all possible optimal ways of picking $A_t$, at least one is a function $\\pi:\\mathcal{S}\\rightarrow \\mathcal{A}$.\n",
    "\n",
    "That helps a lot: we don't have to search for optimal policies in a complex family of history-dependent, stochastic, non-stationary policies; instead we can simply search for a function $\\pi(s)=a$ that maps states to actions.\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Proof sketch (click to expand)</b></summary>\n",
    "\n",
    "The proof (very simple but a little long) is in chapter 6 of the <b>Markov Decision Processes</b> book by Martin L. Puterman.<br>\n",
    "To give you the general flavour:\n",
    "<ul>\n",
    "<li> The infinite horizon leads to the existence of an optimal <b>stationary</b> policy: if the horizon is infinitely far, the optimal decision rule $n$ steps before the end if the same as the one $n+1$ steps before the end (watch out, this intuition can be very false in other contexts).\n",
    "<li> The <b>Markovian</b> property of $p(s'|s,a)$ allows to get optimal memoryless policies.\n",
    "<li> The <b>deterministic</b> part is somehow more tricky but just note that this result only holds for single-player MDPs. For a two-agents competitive game for example (like poker for instance), there is no deterministic optimal policy.\n",
    "</ul>\n",
    "</details>\n",
    "\n",
    "Remark: quite often, we will retain the memoryless and stationary properties of optimal policies but still search for stochastic ones. Why we do this is still a bit beyond the current notebook (it involves notions of exploration, sample distribution, and smoothness of the optimization landscape). For now, let us just remember we shall search for optimal policies under the form of functions from $\\mathcal{S}$ to either $\\mathcal{A}$ (deterministic policies, $\\pi(s)\\in \\mathcal{A}$) or $\\Delta_\\mathcal{A}$ (stochastic policies, $\\pi(s) \\in \\Delta_\\mathcal{A}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c003f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Optimality on average across states\n",
    "\n",
    "If all states have a non-zero probability of being visited by any policy, then an optimal policy should pick optimal actions along any trajectory starting in (any) $s_0$, so it should pick optimal actions in all states. Then finding a policy which maximizes $v^\\pi$ in all states is actually the same as finding a policy which maximizes the expected value $v^\\pi(s_0)$ of a fixed initial state $s_0$, or the expected value over any distribution on initial states $\\mathbb{E}_{s_0\\sim \\rho_0}[v^\\pi(s_0)]$.\n",
    "\n",
    "To fix ideas, let's write $J(\\pi) = \\mathbb{E}_{s_0\\sim \\rho_0}[v^\\pi(s_0)]$. Then:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**The policy optimization problem:**  \n",
    "Provided all states are reachable from any other state, an optimal policy is a solution to $\\max_\\pi J(\\pi) = \\mathbb{E}_{s_0\\sim \\rho_0}[v^\\pi(s_0)]$.\n",
    "</div>\n",
    "\n",
    "So we turned a problem of value maximization in *every state* (point-wise optimality) into one of value maximization *on average* across a distribution $\\rho_0$. Note that this distribution need not be a specific initial state distribution.\n",
    "\n",
    "The assumption that all states are reachable from any other one is very strong. When it is not verified (which will happen in many real-life cases), the optimization problem $\\max_\\pi J(\\pi) = \\mathbb{E}_{s_0\\sim \\rho_0}[v^\\pi(s_0)]$ might not yield a fully optimal policy. It will provide a policy which maximizes its expected outcome on average across states, on a specific set of starting states distributed according to $\\rho_0$.\n",
    "\n",
    "Interestingly, $\\rho_0$ needs not be interpreted as a starting state distribution (even though it is mentally convenient to see it this way). This leads to an alternate formulation of the policy optimization problem as the search for a policy which has the highest value function on average across states.\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**The policy optimization problem:**  \n",
    "Provided $\\rho_0$ has non-zero probability mass on all states, an optimal policy is a solution to $\\max_\\pi J(\\pi) = \\mathbb{E}_{s_0\\sim \\rho_0}[v^\\pi(s_0)]$.\n",
    "</div>\n",
    "\n",
    "The exercises will provide a proof of this result (equivalence between the state-wise definition of optimality and the policy optimization problem).\n",
    "\n",
    "This provides us with a scalar criterion for optimality, stating that we search for a policy which maximizes its expected gain, *on average across $\\rho_0$*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30cc496",
   "metadata": {},
   "source": [
    "# A cartography of optimization methods\n",
    "\n",
    "The two notions of optimality introduced above pave the way to two different families of methods in RL. Either one searches for the optimal value function (point-wise optimality), and then derives an optimal policy as a by-product, or one directly optimizes for good average-value policies (optimality on average). In practice, no approach is more justified than the other and both deserve studying. The latter will be particularly usefull when we attempt to perform gradient-like updates on policies, which would otherwise suffer from the absence of a scalar optimality criterion. This enables separating families of methods and drawing a map of future classes.\n",
    "\n",
    "Policy optimization, solve the $\\max_\\pi$ problem directly:\n",
    "- by derivative-free methods (evolutionary RL)\n",
    "- by derivative-based methods (policy gradient methods and their modern extensions)\n",
    "\n",
    "Value optimization, solve for $v^*$:\n",
    "- by (approximate) dynamic programming (and the vast span of most recent approximate value iteration algorithms)\n",
    "- by linear programming or alternate formulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3a2eee",
   "metadata": {},
   "source": [
    "# Limits of the MDP model\n",
    "\n",
    "What if the system is an MDP but its state is not fully observable?  \n",
    "$\\rightarrow$ This is the (exciting) field of Partially Observable MDPs. Our key result of having a Markovian optimal policy does not hold anymore. There are ways to still obtain optimal policies (but it is often very computationaly costly) or approximate them with Markovian policies.\n",
    "\n",
    "What happens if there are multiple actions taken at the same time by different agents?  \n",
    "$\\rightarrow$ This falls into the category of multi-player stochastic games. Such games can be adversarial, cooperative, or a mix of the two. Of course they can also have partial observability.\n",
    "\n",
    "What if the transition model is not Markovian?  \n",
    "$\\rightarrow$ Beware, here be dragons! All the beautiful framework above crumbles down if its hypothesis are violated. So great care should be taken when choosing the state variables for a given problem. In a sense, an MDP is a discrete time version of a first-order differential equation. Writing a system as $\\dot{x} = f(x,u, noise)$, as is common in control theory, is a good practice to ensure the Markov property."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a9d286",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Summary\n",
    "\n",
    "Let's wrap this section up. Our goal was to formally define the search for the best strategy for our game of FrozenLake and the medical prescription problem. This has led us to formalizing the general **discrete-time stochastic optimal control problem**:\n",
    "- Environment (discrete time, non-deterministic, non-linear, Markov) $\\leftrightarrow$ MDP.\n",
    "- Behaviour $\\leftrightarrow$ control policy $\\pi : \\mathcal{S}\\rightarrow \\mathcal{A}$ or $\\Delta_\\mathcal{A}$.\n",
    "- Policy evaluation criterion $\\leftrightarrow$ $\\gamma$-discounted criterion.\n",
    "- Goal $\\leftrightarrow$ Maximize value function $v^\\pi(s)$.\n",
    "\n",
    "So we have built the first stage of our three-stage rocket:  \n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**What is the system to control?**  \n",
    "The system to control is a Markov Decision Process $(\\mathcal{S}, \\mathcal{A}, p, r)$, and we will control it with a policy $\\pi:s\\mapsto a$, in order to optimize $\\mathbb{E} \\left( \\sum_t \\gamma^t R_t\\right)$\n",
    "</div>\n",
    "\n",
    "We can now move on to the next question: how does one find an optimal strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d6c24c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise** The limits of MDP modeling  \n",
    "Can these systems be modeled as MDPs?   \n",
    "- Playing a tennis video game based on a single video frame\n",
    "- Playing a tennis video game based on a full physical description of the ball and the players\n",
    "- The game of Poker\n",
    "- The collaborative game of [Hanabi](https://en.wikipedia.org/wiki/Hanabi_(card_game))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832095eb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "A single video frame does not contain enough information to accurately represent the current state of the game. The velocities are absent for instance. Hence the dynamics might not be Markovian.\n",
    "    \n",
    "A full physical description, however, may contain enough information so that $\\mathbb{P}(S_{t+1})$ is only conditioned by $S_t$ and $A_t$.\n",
    "    \n",
    "Poker is a two-player, adversarial, partially observable, stochastic game. MDPs only model one-player games (curious about games? Check the [wikipedia page](https://en.wikipedia.org/wiki/Game_theory)).\n",
    "\n",
    "Beyond the fact that it is a multi-player game. Hanabi is a game based mainly on epistemic reasoning. That is, reasoning on beliefs about the state of the world (specifically, the state of the other players' hand). This type of state description is difficult to encode within a Markovian dynamics model.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398d248f-4364-4640-b673-38b41cee2cf3",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3a500a",
   "metadata": {},
   "source": [
    "The exercises below are here to help you play with the concepts introduced above, to better grasp them. They also introduce additional important notions. They are not optional to reach the class goals. Often, the provided answer reaches out further than the plain question asked and provides comments, additional insights, or external references."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e5e95",
   "metadata": {},
   "source": [
    "## Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f0e01",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "In the text above, we wrote that $\\pi_t$ is the distribution over the action space $\\mathcal{A}$ for the action $A_t$ taken at time step $t$.  \n",
    "- Write this probability $\\mathbb{P}(A_t)$ as a conditional probability $\\pi_t(A_t|\\ldots)$ (the real question is: what are the \"$\\ldots$\"?).\n",
    "- Rephrase, with your own words, what this $\\pi_t(A_t|\\ldots)$ indicates.  \n",
    "Then we defined a policy $\\pi$ as the collection of decision rules $\\left( \\pi_t \\right)_{t\\in\\mathbb{N}}$.\n",
    "- Using the answer to the previous questions, write the definition of a Markovian policy, then a stationary Markovian policy (the answer is actually in the text just after the Optimal policy theorem, the exercise is about being able to recall and explain the definitions and what they imply). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9190a8",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "- $\\pi_t$ describes the distribution over actions at time step $t$. Because of causality (future events don't affect current events), it can only depend on the realization of the state and actions random variables in previous time steps:\n",
    "$$\\mathbb{P}(A_t) = \\pi_t(A_t | S_0, A_0, \\ldots, S_{t-1}, A_{t-1}, S_t)$$\n",
    "We define the *history* $H_t = S_0, A_0, \\ldots, S_{t-1}, A_{t-1}, S_t$ at time step $t$ as this random sequence. So:\n",
    "$$\\mathbb{P}(A_t) = \\pi_t(A_t | H_t)$$\n",
    "\n",
    "- In plain words, for an action $a$ and a history $h$ at step $t$, $\\pi_t(a|h)$ indicates  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; the probability to pick action $a$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; at time $t$,  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; given the history of states/actions $h$.  \n",
    "This is called a *history-dependent, non-stationary, stochastic* policy and is the most generic class of policies.\n",
    "\n",
    "- In a Markovian policy, all decision rules are only conditioned by the last encountered state.\n",
    "$$\\pi_t(A_t|H_t) = \\pi_t(A_t | S_t)$$\n",
    "In other words, given two (possibly different) sequences of state-action random variables realizations up to time $t-1$ and a single realization of S_t the distribution of $A_t$ is the same.\n",
    "Mathematically: \n",
    "$\\left\\{\\begin{array}{l}\n",
    "\\forall \\left(s_i,a_i\\right)_{i\\leq t-1}\\in \\left(\\mathcal{S}\\times \\mathcal{A}\\right)^{t-1}\\\\\n",
    "\\forall \\left(s'_i,a'_i\\right)_{i\\leq t-1}\\in \\left(\\mathcal{S}\\times \\mathcal{A}\\right)^{t-1}\\\\\n",
    "\\forall s \\in \\mathcal{S}\n",
    "\\end{array}\\right.,$\n",
    "\\begin{align*}\n",
    "    \\pi_t(A_t|H_t) &= \\pi_t\\left(A_t|S_0=s_0, A_0=a_0, \\ldots, S_t=s\\right)\\\\\n",
    "    &= \\pi_t\\left(A_t|S'_0=s'_0, A'_0=a'_0, \\ldots, S_t=s\\right)\\\\\n",
    "    &= \\pi_t(A_t | S_t)\n",
    "\\end{align*}\n",
    "One writes $\\pi_t(A_t|S_t=s)$, or more simply $\\pi_t(\\cdot | s)$.  \n",
    "In a stationary Markovian policy, all decision rules are the same throughout time. Mathematically:  \n",
    "$\\forall (t,t')\\in \\mathbb{N}^2, \\pi_t(A_t|S_t=s) = \\pi_t'(A_{t'}|S_{t'}=s)$.  \n",
    "This unique distribution is written $\\pi(\\cdot | s) = \\pi_t( \\cdot | s)$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c169b751",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "In the patient example, suppose the physician tells the patient to take drug A every day for 5 days, then drug B every two days for 9 days, then come back for a check-up. The physician adds to take drug C once a day if the patient feels pain over two consecutive days. Can you write the sequence of corresponding decision rules?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd563b3",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "This prescription is made over a finite horizon $h=14$ days. The actions are the combinations of drugs $\\mathcal{A}=\\left\\{ \\emptyset, (A), (B), (C), (A,B), (A,C), (B,C), (A,B,C) \\right\\}$.   \n",
    "    \n",
    "The prescription is deterministic: the distribution over actions is a Dirac. Given a history $\\eta$, we will write it $a_t = \\pi_t(\\eta)$.\n",
    "    \n",
    "The prescription depends on the two last states of the patient. So it's not Markovian, it is history-dependent. Precisely, it depends on the boolean state variable \"is there pain?\". So we can write $\\pi_t(\\eta) = \\pi_t(s_t,s_{t-1})$.  \n",
    "  \n",
    "It also is not stationary, since the prescription changes after day 5.  \n",
    "    \n",
    "Consequently, the policy is:  \n",
    "If $t \\in [1, 5]$:   \n",
    "if $pain(s_t,s_{t-1})=True$, $\\pi_t(s_t,s_{t-1}) = (A,C)$,  \n",
    "if $pain(s_t,s_{t-1})=False$, $\\pi_t(s_t,s_{t-1}) = (A)$.  \n",
    "If $t \\in [6, 14]$:   \n",
    "if $t$ is even and $pain(s_t,s_{t-1})=True$, $\\pi_t(s_t,s_{t-1}) = (B,C)$,  \n",
    "if $t$ is even and $pain(s_t,s_{t-1})=False$,  $\\pi_t(s_t,s_{t-1}) = (B)$,  \n",
    "if $t$ is odd and $pain(s_t,s_{t-1})=True$, $\\pi_t(s_t,s_{t-1}) = (C)$,  \n",
    "if $t$ is odd and $pain(s_t,s_{t-1})=False$,  $\\pi_t(s_t,s_{t-1}) = \\emptyset$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dd2c36",
   "metadata": {},
   "source": [
    "## Monte Carlo value estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b0b20c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Write a `mc_eval(env,pi,nb_trials)` function that uses the FrozenLake environment we've introduced earlier to obtain a vector of length `nb_trials` containing the return realizations for `nb_trials` Monte-Carlo rollouts of policy `pi` (given as an array of action indices) starting in the initial state $s_0$. Take $\\gamma = 0.9$. Yes, the code is almost the same as the example provided earlier.  \n",
    "Run this function with the policy that always goes right and for 100000 trials.  \n",
    "Note that $\\gamma^{200} \\approx 10^{-9}$ so any reward obtained after 200 time steps will have a negligible contribution to $V^\\pi(s_0)$, thus rolling an episode out for 200 time steps should be sufficient.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4396e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3734e-26b4-411b-830a-b79b3ffbceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_mc_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5f954-b940-418a-9e7d-a320124eb062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "pi = fl.RIGHT*np.ones((env.observation_space.n))\n",
    "Vepisode = mc_eval(env,pi,100000)\n",
    "print(\"value estimate:\", np.mean(Vepisode))\n",
    "print(\"return std dev:\", np.std(Vepisode))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d236b26d",
   "metadata": {},
   "source": [
    "## Markov strikes back: transition kernels and stationary distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f43625",
   "metadata": {},
   "source": [
    "In this section, we get back to basics on Markov chains and play a bit with core notions to acquire a better understanding of MDP properties.\n",
    "\n",
    "Let's consider a given finite state space MDP and a fixed policy. To set ideas, take the FrozenLake environment and the deterministic policy that always moves right. Let's initialize the MDP to a starting state $s_0$ drawn from a distribution $\\rho_0(s)$ and let's look at how the state evolves across time steps. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Use the MDP's transition model and the policy to define the transition probability $p^\\pi(s'|s)$ of reaching $s'$ from $s$.  \n",
    "What is $\\mathbb{P}(s_{t+1})$ given $p^\\pi$ and $\\mathbb{P}(s_t)$?  \n",
    "What is the state probability $\\mathbb{P}(s_{t+k})$ after $k$ transitions, given $p^\\pi$ and $\\mathbb{P}(s_t)$?\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "For a deterministic policy, \n",
    "$$p^\\pi(s'|s) = p(s'|s,\\pi(s)).$$ \n",
    "\n",
    "For a stochastic policy, \n",
    "$$p^\\pi(s'|s) = \\mathbb{E}_{a\\sim \\pi(\\cdot | s)} [ p(s'|s,a) ] = \\sum_{a\\in \\mathcal{A}} p(s'|s,a) \\pi(a|s).$$\n",
    "    \n",
    "$\\mathbb{P}(s_{t+1}) = \\mathbb{P}(s_t) \\cdot p^\\pi$ (because the MDP equipped with the policy is a Markov chain)\n",
    "\n",
    "$\\mathbb{P}(s_{t+k}) = \\mathbb{P}(s_t) \\cdot (p^\\pi)^k$ ($k$-step transition matrix in a Markov chain).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e83f10f",
   "metadata": {},
   "source": [
    "The stochastic process of $S_t$ is a Markov chain (as we saw in the previous exercise, since $\\pi$ is fixed, the probability of reaching $S_{t+1}$ is only conditionned by $S_t$). \n",
    "Let's adopt the convention that $\\mathbb{P}(S)$ is a row vector (for a certain ordering of states in $S$), and let's note $p^\\pi$ the $|S|\\times |S|$ stochastic matrix where $p^\\pi_{ss'} = p^\\pi(s'|s)$. \n",
    "Note that the first line of $p^\\pi$ is the distribution over $s_{t+1}$ provided that $s_t$ is the first state. \n",
    "Let's use our notations and recall a few results about Markov chains:\n",
    "- State $s'$ is said to be *reachable* from state $s$ is there exists a sequence of transitions originating in $s$ and ending in $s'$ with positive probability.\n",
    "- The Markov chain is *irreducible* if all states are reachable from any other state.\n",
    "- The *period* of a state $s$ is the greatest common divisor of the number of steps required to reach $s$ from itself.\n",
    "- Two states are said to *communicate* with each other if both are reachable from one another. \n",
    "- Two communicating states have the same period.\n",
    "- A *class* of states is a set of communicating states.\n",
    "- An irreducible Markov chain has a single class.\n",
    "- An irreducible Markov chain is *aperiodic* if all states have period one.\n",
    "- A class $C'$ is *accessible* from another class $C$ if there exists $(s,s')\\in C\\times C'$ such that $s'$ is accessible from $s$.\n",
    "- A class is *closed (or terminal, or absorbing)* if the probability of leaving the class is zero. Otherwise it is called *transient*.\n",
    "- A state is *positive recurrent* if one is guaranteed to come back to it in finite time.\n",
    "- A state that is positive recurrent and aperiodic is called *ergodic*.\n",
    "- If all states in an irreducible Markov chain are ergodic, then the chain is said to be ergodic.\n",
    "- Equivalently, a Markov chain is ergodic if any state can be reached from any other state in a bounded number of steps.\n",
    "- A *stationary distribution* is a distribution $\\rho$ such that $\\rho = \\rho \\cdot p^\\pi$.\n",
    "- If there is at least one positive recurrent state $s$ in the chain, then there exists at least one stationary distribution $\\rho$ and $\\rho(s)>0$.\n",
    "- Conversely, if there exists a stationary distribution $\\rho$ (such that $\\rho = \\rho \\cdot p^\\pi$) and $\\rho(s)>0$, then $s$ is positive recurrent.\n",
    "- If there is a *single* final class, then there is at most one stationary distribution.\n",
    "- If the Markov chain is irreductible and aperiodic (furthermore if it is ergodic), when $k\\rightarrow\\infty$, $(p^\\pi)^k$ tends to a matrix whose lines are all equal to the stationary distribution of the Markov chain.\n",
    "- Consequently, if the Markov chain is irreducible and aperiodic, in the long run, the distribution of states follows a stationary distribution $\\rho^\\pi(s|s_0) = \\lim_{k\\rightarrow \\infty} \\rho_0 (p^\\pi)^k$, for all possible initial state distribution $\\rho_0$. \n",
    "\n",
    "A few examples to make this concrete:\n",
    "- A discretized inverted pendulum with a random action policy is an irreducible and aperiodic Markov chain (but not necessarily ergodic as some states might be asymptotically long to reach).  \n",
    "- The FrozenLake MDP, under any policy, is never an irreducible Markov chain because of the absorbing \"hole in the ice\" states.\n",
    "- The holes in the ice of the FrozenLake MDP, under any policy, are (each) a set of final classes.  \n",
    "- Suppose we abstract the holes in the ice as a single absorbing state, then under any policy, this state forms a unique final class and the stationary distribution concentrates all probability mass on it.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (open discussion):**  \n",
    "Depending on the policy used, the Markov chain will have different properties. \n",
    "In particular, the stationary distribution is not necessarily unique: it depends on $s_0$. \n",
    "Here are a few informal examples for you to practice. Can you list a few more?  \n",
    "What about an Atari 2600 video game like Pong, under an optimal policy?  \n",
    "A self-driving car under a \"reasonably safe\" policy?  \n",
    "The patient example with a chronic disease, under a policy that fights off the disease?   \n",
    "The patient with a deadly disease under a policy that doesn't cure them?  \n",
    "The Mad Hatter's casino (from a previous class) under a fixed random policy?  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb68c29a",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "The patient with a chronic disease under a policy that fights off the disease will most likely live a rather long life (let's say infinite, for the sake of this example) and will explore states that are linked to the evolution of the disease. The states corresponding to non-recoverable situations however will not be visited. The Markov chain is not irreducible: depending on the initial state (recoverable situation or not) there are at least two subgraphs describing it. The same goes for the car driven by a cautious driver: many states are visited but not the ones that cause an accident.\n",
    "    \n",
    "The patient with a deadly disease and a bad treatment policy will likely die, sadly. On an infinite horizon, the stationary distribution only has probability mass on the states corresponding to death, which is the single absorbing class.\n",
    "    \n",
    "Similarly, the FrozenLake example (or the Pong game) has several terminal states, either by reaching the goal or by falling into a hole. It should be noted however that for such episodic environments, it is possible to define an alternate distribution $\\rho^\\pi(s|s_0)$ that describes the distribution of states before termination.\n",
    "    \n",
    "Finally, the Mad Hatter's casino under a fixed random policy is a very nice ergodic Markov chain: from any starting state there is a non-zero probability of reaching any state in a finite number of steps. No terminal states in wonderland!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5eb4a0-d9ae-45cd-90e3-187dddbbb7c4",
   "metadata": {},
   "source": [
    "## Summing rewards over states rather than time steps: the state occupancy measure\n",
    "\n",
    "Recall that we introduced $J(\\pi) = \\mathbb{E}_{s_0\\sim \\rho_0}[v^\\pi(s_0)]$. This is the value of policy $\\pi$, evaluated on average across starting states distributed according to $\\rho_0$. So:\n",
    "$$J(\\pi) = \\mathbb{E}_{(R_t)_{t\\in\\mathbb{N}}} \\left[ \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\bigg| \\rho_0,\\pi \\right]$$\n",
    "\n",
    "For the sake of simplicity, let's write:\n",
    "$$r^\\pi(s) = \\mathbb{E}_{\\substack{a\\sim \\pi \\\\ s'\\sim p^\\pi}} [r(s,a,s')].$$\n",
    "\n",
    "So we can write:\n",
    "$$J(\\pi) = \\mathbb{E}_{(S_t)_{t\\in\\mathbb{N}}} \\left( \\sum\\limits_{t = 0}^\\infty \\gamma^t r^\\pi(S_t) \\bigg| \\rho_0, \\pi \\right).$$\n",
    "\n",
    "We can remark each of the terms under the sum is concerned with a single time step, and its value depends only on $S_t$. \n",
    "We can swap the expectation of the sum (over time steps) by the sum (over times steps) of expectations:\n",
    "$$J(\\pi) = \\sum\\limits_{t = 0}^\\infty \\gamma^t \\mathbb{E}_{S_t} \\left( r^\\pi(S_t) | \\rho_0, \\pi \\right). $$\n",
    "\n",
    "Let's write $p(S_t=s|\\rho_0,\\pi)$ the probability density of encountering each state $s$ at time step $t$, provided that the starting state was drawn according to $\\rho_0$ and that the MDP evolves under policy $\\pi$. Then:\n",
    "$$J(\\pi) = \\sum\\limits_{t = 0}^\\infty \\gamma^t \\int_S r^\\pi(s) p(S_t=s|\\rho_0,\\pi) ds.$$\n",
    "\n",
    "Intead of summing over time steps first, let us sum over states. This yields:\n",
    "$$J(\\pi) = \\int_S \\sum\\limits_{t = 0}^\\infty \\gamma^t r^\\pi(s) p(S_t=s|\\rho_0,\\pi) ds.$$\n",
    "\n",
    "Let us write $\\rho^\\pi_{\\rho_0}(s) = \\sum\\limits_{t = 0}^\\infty \\gamma^t p(S_t=s|\\rho_0,\\pi)$ for all $s \\in S$. This quantity is called the *discounted state occupancy measure*. Although it is not a proper probability density (it does not sum to one), it is a proxy of how much time policy $\\pi$ spends in state $s$ over the course of a trajectory, given that it started in $S_0$ drawn according to $\\rho_0$.\n",
    "\n",
    "So we have $J(\\pi) = \\langle \\rho^\\pi_{\\rho_0}, r^\\pi \\rangle$. We can abuse the $\\mathbb{E}$ notation and write:\n",
    "$$J(\\pi) = \\mathbb{E}_{s\\sim \\rho^\\pi_{\\rho_0}}[r^\\pi(s)] = \\langle \\rho^\\pi_{\\rho_0}, r^\\pi \\rangle.$$\n",
    "\n",
    "So the value of policy $\\pi$ is both a sum of rewards over time steps $\\mathbb{E}[ \\sum_{t = 0}^\\infty \\gamma^t R_t]$ and a sum of rewards across states $\\langle \\rho^\\pi_{\\rho_0}, r^\\pi \\rangle$, weighted by their occupancy measure under $(\\pi,\\rho_0)$.\n",
    "\n",
    "Interestingly, searching for a policy which maximizes $J(\\pi)$ boils down to searching for a policy whose state occupancy measure is maximally aligned with the reward model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928fb0c1-c2b4-44bc-a309-12fe6bd0a167",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "What does $\\rho^\\pi_{\\rho_0}$ sum to? Normalize this measure to turn it into a probability distribution and correct the last expression of $J(\\pi)$ above.\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "    \n",
    "The integral of $\\rho^\\pi_{\\rho_0}$ across $\\mathcal{S}$ is:\n",
    "\\begin{align*}\n",
    "\\int_\\mathcal{S} \\rho^\\pi_{\\rho_0}(s)ds &= \\int_\\mathcal{S} \\sum\\limits_{t = 0}^\\infty \\gamma^t p(S_t=s|\\rho_0,\\pi) ds \\\\\n",
    " &= \\sum\\limits_{t = 0}^\\infty \\gamma^t \\int_\\mathcal{S}  p(S_t=s|\\rho_0,\\pi) ds \\\\\n",
    " &= \\sum\\limits_{t = 0}^\\infty \\gamma^t \\\\\n",
    " &= \\frac{1}{1-\\gamma}\n",
    "\\end{align*}\n",
    "\n",
    "So $\\rho^\\pi_{\\rho_0}$ always sums to $\\frac{1}{1-\\gamma}$. A proper probability distribution would then be $(1-\\gamma)\\rho^\\pi_{\\rho_0}$ and a correct writing for $J(\\pi)$ is thus:\n",
    "$$J(\\pi) = \\mathbb{E}_{s\\sim (1-\\gamma)\\rho^\\pi_{\\rho_0}} \\left[ \\frac{r^\\pi(s)}{1-\\gamma} \\right]$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65112987-8a14-4c63-bd94-a3b56924f955",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Can you derive the same result with a state-action occupancy measure, instead of a state-only occupancy measure?\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "    \n",
    "$$J(\\pi) = \\mathbb{E}_{(S_t,A_t)_{t\\in\\mathbb{N}}}\\left[ \\sum\\limits_{t = 0}^\\infty \\gamma^t r(S_t,A_t) \\right]$$\n",
    "\n",
    "Let's define $\\rho^\\pi_{\\rho_0}(s,a) = \\sum\\limits_{t = 0}^\\infty \\gamma^t p(S_t=s, A_t=a|\\rho_0,\\pi)$. Then, given a reward model $r(s,a))$:\n",
    "$$J(\\pi) = \\langle \\rho^\\pi_{\\rho_0}, r \\rangle = \\mathbb{E}_{s\\sim (1-\\gamma)\\rho^\\pi_{\\rho_0}}\\left[ \\frac{r(s,a)}{1-\\gamma} \\right].$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809805fb-7584-4e5a-a699-abec9932e43a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Consider a finite state space MDP. So $\\rho_0$ is a line vector and $p^\\pi$ is a square matrix. In a previous exercise, we derived the expression of $p(S_t=s|\\rho_0,\\pi)$. Use this expression in $\\rho^\\pi_{\\rho_0}(s)$.\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "    \n",
    "We had $p(S_t=s|\\rho_0,\\pi) = \\rho_0 (p^\\pi)^t$.\n",
    "\n",
    "So $\\rho^\\pi_{\\rho_0}(s) = \\sum\\limits_{t = 0}^\\infty \\rho_0 (\\gamma p^\\pi)^t$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6413e969-7697-4f02-9408-323e62c351de",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Write a function `state_occupancy_measure(P_pi,rho,gamma,horizon)` that estimates $\\rho^\\pi_{\\rho_0}$. Then use this function and the two utility functions provided below to compute the value of the initial state in FrozenLake. Compare to the Monte Carlo estimate built in a previous exercise.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda51cc3-a87a-4b0a-a67b-89561dc77dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_display_function_of_state.py\n",
    "import numpy as np\n",
    "\n",
    "def display_function_of_state(f):\n",
    "    print(np.reshape(f, (4,4)))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8c06a2-9506-4714-9450-7556e2302ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_P_and_r.py\n",
    "import numpy as np\n",
    "\n",
    "def fl_P_and_r(env,pi):\n",
    "    r_pi = np.zeros((env.observation_space.n))\n",
    "    P_pi = np.zeros((env.observation_space.n, env.observation_space.n))\n",
    "    for x in range(env.observation_space.n):\n",
    "        outcomes = env.unwrapped.P[x][pi[x]]\n",
    "        for o in outcomes:\n",
    "            p = o[0]\n",
    "            y = o[1]\n",
    "            r = o[2]\n",
    "            P_pi[x,y] += p\n",
    "            r_pi[x] += r*p\n",
    "    return P_pi,r_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73dbf1d-e814-4359-bc92-e37bccfdb9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542152e6-3714-44a4-8717-c9b66db96803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_state_occupancy_measure.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05b0c45-63d3-4e65-8f80-12085f665b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "from solutions.fl_mc_eval import mc_eval\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "pi = fl.RIGHT*np.ones((env.observation_space.n))\n",
    "gamma = 0.9\n",
    "horizon = 200\n",
    "\n",
    "# rho0\n",
    "rho0 = np.zeros((env.observation_space.n))\n",
    "rho0[0] = 1\n",
    "\n",
    "P_pi,r_pi = fl_P_and_r(env,pi)\n",
    "rho_pi = state_occupancy_measure(P_pi,rho0,gamma,horizon)\n",
    "display_function_of_state(rho_pi)\n",
    "\n",
    "print(\"estimation of V(s0) through rho_pi:\", np.dot(rho_pi,r_pi))\n",
    "mc_rollouts = mc_eval(env,pi,100000)\n",
    "print(\"Monte Carlo estimate:\", np.mean(mc_rollouts), \"std dev:\", np.std(mc_rollouts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006303ab-24f3-43cb-8ab7-d1b6eb3cf4a1",
   "metadata": {},
   "source": [
    "## Equivalence of state-wise optimality and optimality on average across states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e2e02-6ee6-41f0-a191-3e8f8e5bb946",
   "metadata": {},
   "source": [
    "Recall the definition of an **optimal** policy as one that is better than any other.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Optimal policy $\\pi^*$**  \n",
    "$\\pi^*$ is said to be optimal iff $\\pi^* \\in \\arg\\max\\limits_{\\pi} v^\\pi$.  \n",
    "\n",
    "A policy is optimal if it **dominates** over any other policy in every state:\n",
    "$$\\pi^* \\textrm{ is optimal}\\Leftrightarrow \\forall s\\in \\mathcal{S}, \\ \\forall \\pi, \\ v^{\\pi^*}(s) \\geq v^\\pi(s)$$\n",
    "</div>\n",
    "\n",
    "This was a straightforward definition, but does not lend itself easily to optimization as it is a point-wise definition: an optimal policy dominates any other policy *in every state*. $v^{\\pi^*}$ takes as many values as there are states and we want *all* of them to dominate over other policies's state values respectively.\n",
    "\n",
    "To introduce a single scalar criterion, we introduced a distribution on states $\\rho_0$, and wrote $J(\\pi) = \\mathbb{E}_{s_0\\sim \\rho_0}[v^\\pi(s_0)]$.\n",
    "\n",
    "This enabled defining optimality on average across states.\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**The policy optimization problem:**  \n",
    "Provided $\\rho_0$ has non-zero probability mass on all states, an optimal policy is a solution to $\\max_\\pi J(\\pi) = \\mathbb{E}_{s_0\\sim \\rho_0}[v^\\pi(s_0)]$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936769da-7437-4048-a0f0-4d6327451a1d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Prove that, if $\\rho_0$ has non-zero probability mass on all states, the two definitions of an optimal $\\pi$ below are equivalent.  \n",
    "(note that we drop the obvious set notations for the sake of readability: $s\\in \\mathcal{S}, \\pi \\in \\mathcal{A}^\\mathcal{S}$)\n",
    "1. $\\forall s, \\pi' , v^\\pi(s) \\geq v^{\\pi'}(s)$\n",
    "2. $\\forall \\pi' , \\mathbb{E}_{s\\sim \\rho_0} [v^\\pi(s)] \\geq \\mathbb{E}_{s\\sim \\rho_0} [v^{\\pi'}(s)]$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ccba74-6223-417d-ae53-ee32e8ca1a15",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "The implication $1 \\Rightarrow 2$ is trivial. If $v^\\pi(s) \\geq v^{\\pi'}(s)$, then any linear combination with positive weights preserves the inequality and $\\mathbb{E}_{s\\sim \\rho_0} [v^\\pi(s)] \\geq \\mathbb{E}_{s\\sim \\rho_0} [v^{\\pi'}(s)]$.\n",
    "\n",
    "Let us now prove that $\\neg 1 \\Rightarrow \\neg 2$ (which is equivalent to proving that $2 \\Rightarrow 1$).\n",
    "\n",
    "$\\neg 1$ states that $\\exists \\bar{s}, \\bar{\\pi}$ such that $v^{\\bar{\\pi}}(\\bar{s}) > v^\\pi(\\bar{s})$. In plain words: there exists a policy $\\bar{\\pi}$ which dominates $\\pi$ in at least one state $\\bar{s}$.\n",
    "\n",
    "This implies that the optimal value function $v^*$ verifies $v^*(s) \\geq \\max \\{ v^{\\bar{\\pi}}(s), v^\\pi(s) \\}$ in all possible states. And in particular, that $v^*(\\bar{s}) > v^\\pi(\\bar{s})$ (note the strict inequality).\n",
    "\n",
    "Since there always exists an optimal policy which is stationary, deterministic, and memoryless, there exists $\\pi'$ such that $v^{\\pi'} = v^*$.\n",
    "\n",
    "Consequently: $\\exists \\pi'$ such that $v^{\\pi'}(s) \\geq v^{\\pi}(s), \\forall s$, and in particular, $v^{\\pi'}(\\bar{s}) > v^\\pi(\\bar{s})$ (note the strict inequality again).\n",
    "\n",
    "So, since $\\rho_0(\\bar{s}) > 0$, it follows that $\\exists \\pi'$ such that $\\mathbb{E}_{s\\sim \\rho_0} [v^{\\pi'}(s)] > \\mathbb{E}_{s\\sim \\rho_0} [v^{\\pi}(s)]$, which is precisely $\\neg 2$, which concludes the proof.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10403279-6a5e-419d-a1fd-32a5a740ae8f",
   "metadata": {},
   "source": [
    "In practice, it is often sufficient to use the starting state distribution as $\\rho_0$. But beware, this does not apply in the general case. The following example illustrates why.\n",
    "\n",
    "Consider an $n$ states MDP with two actions. The transition kernel is $p(s|s,a)=1, \\forall s,a$. In plain words: all transitions leave the process in the same state, whichever action is taken. This can be seen as a metaphor for a set of $n$ rooms in a building where the architect has forgotten to build doors. Action 0 yields reward 0 (regardless of the state), action 1 yields reward 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a482dac-4dc0-4b54-89b5-69e0b93d3d5b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "What is an optimal policy and its value under a $\\gamma$-discounted criterion?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43cc0cb-32a5-4fe1-9f98-95ece1057dac",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "There is only one optimal policy and it is $\\pi(s)=1, \\forall s$.\n",
    "\n",
    "$v^*(s) = \\sum_{t=0}^\\infty \\gamma^t\\cdot 1 = \\frac{1}{1-\\gamma}$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1dfb81-cca0-43dd-b6f2-03af6a1dd2fd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Take $\\rho_0$ to be the probability distribution that assigns probability 1 to state 0 (and hence probability 0 to all other states). Can you exhibit a non-optimal policy which maximizes $\\mathbb{E}_{s\\sim \\rho_0} [v^\\pi(s)]$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3706ba87-12da-42ac-b931-18e49b53f42e",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "The optimal policy, under the scalar criterion, has value $\\frac{1}{1-\\gamma}$.\n",
    "\n",
    "The policy defined as $\\pi(0) = 1$ and $\\pi(s)=0, \\forall s\\neq 0$, has the same, highest possible value, while being non-optimal.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeaaf22-7285-404f-926e-62fbc734e910",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Let us generalize the previous example to a deterministic, discrete state space MDP representing a grid-world of two unconnected rooms (several states per room) and a single exit (with action 0), providing reward 1 (all other rewards are zero all the time) and transitioning to an absorbing state where all rewards are 0. The actions are the intuitive $\\{N,S,E,W\\}$ navigation actions.\n",
    "\n",
    "In plain words, can you explain why optimizing for $\\mathbb{E}_{s\\sim \\rho_0} [v^\\pi(s)]$ given a $\\rho_0$ that puts all probability mass on state $0$ might not yield an optimal policy?\n",
    "\n",
    "Can you answer the same question in a similar grid-world which is not deterministic anymore but rather has a non-zero probability of reaching any neighboring position after any navigation action?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8809c69e-71a7-49f3-94f3-062e5ba5bf65",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "The grid-world MDP described is a metaphor of a $(2n+1)$-states, $m$ actions, deterministic MDP with two classes and an extra absorbing state (which is a third class by itself), where the first $n$ states are communicating (in the sense that there exists a trajectory linking any two states in this set), the following $n$ states are communicating too, but the two classes do not communicate.\n",
    "\n",
    "The MDP is deterministic but the optimal path from state $0$ to the exit might not be unique. \n",
    "A policy that draws an optimal path from state $0$ to the exit will traverse a subset of states in the room where state $0$ is. \n",
    "The policy can take any value in any other state, this will not affect $\\mathbb{E}_{s\\sim \\rho_0} [v^\\pi(s)]$. \n",
    "This defines a set of policies which maximize $\\mathbb{E}_{s\\sim \\rho_0} [v^\\pi(s)]$. \n",
    "Within this set, on states outside this optimal path from state $0$, some policies will dominate others.\n",
    "Hence optimizing for $\\mathbb{E}_{s\\sim \\rho_0} [v^\\pi(s)]$ might not yield an optimal policy overall.\n",
    "\n",
    "Similarly, if there is a non-zero probability to reach all states in the room of state $0$, then optimizing for $\\mathbb{E}_{s\\sim \\rho_0} [v^\\pi(s)]$ will require the policy to provide an optimal action in any reachable state, and will eventually provide a policy which is optimal in all states of this room. But there is still no guarantee that the obtained policy will be optimal in any state of the second room. Again, as long as $\\rho_0$'s support does not span all $\\mathcal{S}$, in the general case, there is no guarantee that optimizing $\\mathbb{E}_{s\\sim \\rho_0} [v^\\pi(s)]$ will yield an optimal policy.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5d4e1e-34b2-452c-a76e-3b2df7fbfc4a",
   "metadata": {},
   "source": [
    "The take-away message of this short series of exercises, is that, although optimality on average across states is a very convenient notion (as it provides a single scalar optimization criterion) with great practical utility, one should keep in mind the subtleties of when this objective function actually characterizes optimal policies (or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73abef0d",
   "metadata": {},
   "source": [
    "## Homework: Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf881583",
   "metadata": {},
   "source": [
    "In this notebook, we have been using a suite of environments called [Gymnasium](https://gymnasium.farama.org/), which provided us with the FrozenLake environment. Gymnasium is a maintained fork of OpenAI’s Gym library which has become the *de facto* standard in terms of Python API for reinforcement learning environments. As Gym had a history of low maintenance and poor documentation, all development of Gym has been moved to Gymnasium.\n",
    "\n",
    "The Gymnasium API has been used without introduction of explanations: it is time to explore a little more how it works.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Read three specific pages in Gymnasium's documentation:  \n",
    "- The [front page](https://gymnasium.farama.org/)\n",
    "- The introduction to Gymnasium's [basic usage](https://gymnasium.farama.org/introduction/basic_usage)\n",
    "- The general API to all Gymnasium's [environments](https://gymnasium.farama.org/api/env/)\n",
    "\n",
    "Also take a minute to browse through the different families of environments provided.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0910e3cc-03e6-49d2-9ef5-7adbfc6080d9",
   "metadata": {},
   "source": [
    "## Interpreting gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a3293f-4e7e-43b8-8438-90e5476dc937",
   "metadata": {},
   "source": [
    "We motivated the introduction of $\\gamma$ with the argument that $\\sum\\limits_{t = 0}^\\infty R_t$ might grow unbounded. The solution was to *discount* future rewards by $\\gamma^t$.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Suppose rewards are drawn from the $[0,1]$ interval. What is the upper bound on $\\sum\\limits_{t = 0}^\\infty \\gamma^t R_t$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ad055-a33f-47bc-8d2a-65d1c8e7283e",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "The upper bound is $\\frac{1}{1-\\gamma}$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f905749-e619-474f-bc27-1fbeb65d3d6a",
   "metadata": {},
   "source": [
    "Consider an MDP $(\\mathcal{S}, \\mathcal{A}, p, r)$ and a policy $\\pi$. \n",
    "Let's now append one state to $\\mathcal{S}$, which we will write $s_{null}$. This defines a new state space $\\mathcal{S}'$. We will consider $\\mathcal{S}$ is ordered and $s_{null}$ is the last element of $\\mathcal{S}'$. \n",
    "The action space remains the same.  \n",
    "$s_{null}$ will play the role of a zero-reward, absorbing state, reachable from any other state.\n",
    "\n",
    "When in any state $s \\in \\mathcal{S}$, we first check for a transition to $s_{null}$ with probability $1-\\gamma$. Otherwise, we transition to another state $s'$ with probability $p^\\pi(s'|s)$. \n",
    "So $s_{null}$ is an absorbing state, reachable in one step from any other state with probability $1-\\gamma$. \n",
    "All transitions from $s_{null}$ loop back to itself with probability one, whichever the action taken. Consequently, the policy $\\pi$ can be trivially extended to $S'$ as a policy $\\pi'$ which takes the same action as $\\pi$ in all $s\\in S$ and takes any action in $s_{null}$.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Let $p^\\pi$ be the transition matrix of the first MDP under $\\pi$. Write the transition matrix $p^{\\pi'}$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67576e8e-9d00-4d48-b4c8-84d67ba5d2b5",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "    \n",
    "Let's write $\\mathbf{(1-\\gamma)_\\mathcal{S}}$ the column vector composed of $|\\mathcal{S}|$ elements, all equal to $1-\\gamma$.  \n",
    "Let's write $\\mathbf{0_\\mathcal{S}}$ the column vector composed of $|\\mathcal{S}|$ elements, all equal to $0$.  \n",
    "Then:\n",
    "$$p^{\\pi'} = \\left[ \\begin{array}{cc} \\gamma p^\\pi & \\mathbf{(1-\\gamma)_{\\mathcal{S}}} \\\\ \\mathbf{0_\\mathcal{S}}^T & 1 \\end{array} \\right]$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58f834c",
   "metadata": {},
   "source": [
    "So, in plain words, $s_{null}$ is a *terminal* state, and $\\gamma$ is a probability of *non-termination* in the second MDP.\n",
    "\n",
    "An interesting aspect is that $s_{null}$ is accessible from *any* state of $\\mathcal{S}$ with the same probability.\n",
    "\n",
    "Let's call *length* of a trajectory the first time step at which $s_{null}$ is encountered. So it is the number of transitions leading to $s_{null}$ (note that this includes the last transition to $s_{null}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d28477",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "What is the probability of a trajectory of length greater or equal to $h$ time steps?  \n",
    "What is the probability of a trajectory of length exactly equal to $h$ time steps?  \n",
    "How is this probability distribution called?  \n",
    "Deduce the average trajectory length.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd63f53b",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "    \n",
    "Reaching $s_{null}$ after at least $h$ time steps, means not having reached it before, which has probability $\\gamma^h$.\n",
    "    \n",
    "$$\\mathbb{P}(\\textrm{length}\\geq h) = \\gamma^h$$\n",
    "\n",
    "Hence the probability of a trajectory of length $h$ is:\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(\\textrm{length}= h) &= \\mathbb{P}(h+1 > \\textrm{length}\\geq h)\\\\\n",
    "&= \\mathbb{P}(\\textrm{length}\\geq h) - \\mathbb{P}(\\textrm{length}\\geq h+1)\\\\\n",
    "&= \\gamma^h - \\gamma^{h+1}\\\\\n",
    "&= (1-\\gamma) \\gamma^h\n",
    "\\end{align*}\n",
    "    \n",
    "So the length of a trajectory follows a geometric distribution of parameter $1-\\gamma$. The average trajectory length is hence $\\frac{1}{1-\\gamma}$. We will sometimes call this quantity (a bit abusively) the **horizon of MDP $(\\mathcal{S},\\mathcal{A},p,r)$ under the $\\gamma$-discounted criterion**.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39295ced-5fa9-44e0-8999-9897a6eff8d3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Can you relate the value of $\\pi'$ in the second MDP under the total reward criterion to that of $\\pi$ in the first MDP under the $\\gamma$ discounted criterion?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb36e0e-d4c0-4c35-9f67-fb34284cb257",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "    \n",
    "We shall write $v^{\\pi'}$ the value of $\\pi'$ under the total reward criterion, and $v^\\pi_\\gamma$ that of $\\pi$ under the $\\gamma$-discounted criterion, to avoid ambiguities.\n",
    "\n",
    "Let's consider the first transition of a trajectory starting in $s$. Either this transition moves to $s_{null}$ with probability $1-\\gamma$, or it moves to a state $s'\\in \\mathcal{S}$, with probability $\\gamma p^\\pi(s'|s)$. The transition to $s_{null}$ provides zero reward, while the transition to $s'$ provides $r^\\pi(s,s')$. From $s'$, the expected gain is $v^\\pi(s')$. So:\n",
    "$$v^{\\pi'}(s) = (1-\\gamma)\\cdot 0 + \\mathbb{E}_{s' \\sim \\gamma p^\\pi} [r^\\pi(s,s') + v^{\\pi'}(s')].$$\n",
    "    \n",
    "But $\\mathbb{E}_{s' \\sim \\gamma p^\\pi}[\\cdot] = \\gamma \\mathbb{E}_{s' \\sim p^\\pi}[\\cdot]$, so:\n",
    "$$v^{\\pi'}(s) = \\gamma \\mathbb{E}_{s' \\sim p^\\pi} \\left[r^\\pi(s,s') + v^{\\pi'}(s')\\right].$$\n",
    "    \n",
    "We can unfold this for the following time step, ie. states reached from $s'$:\n",
    "$$v^{\\pi'}(s) = \\gamma \\mathbb{E}_{s' \\sim \\gamma p^\\pi} \\left[r^\\pi(s,s') + \\gamma \\mathbb{E}_{s'' \\sim (p^\\pi)^2} \\left[r^\\pi(s',s'') + v^{\\pi'}(s'') \\right] \\right].$$\n",
    "\n",
    "Which, in the limit, yields:\n",
    "$$v^{\\pi'}(s) = \\gamma \\mathbb{E}_{s_t \\sim (p^\\pi)^t} \\left[ \\sum_{t=0}^\\infty \\gamma^t r^\\pi(s_t,s_{t+1}) \\right].$$\n",
    "\n",
    "And we find there the definition of $V^\\pi_\\gamma$:\n",
    "$$v^{\\pi'}(s) = \\gamma v^\\pi_\\gamma(s).$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218a597f",
   "metadata": {},
   "source": [
    "Consequently, to a constant multiplicative factor, the value of a trajectory under a $\\gamma$-discounted criterion, is actually the value of playing the same game under a total reward criterion, but with a probability of termination of $1-\\gamma$ at each time step. In other words, using the $\\gamma$-discounted criterion boils down to taking into account a probability $\\gamma$ of \"staying alive\" at each time step in the current game."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
